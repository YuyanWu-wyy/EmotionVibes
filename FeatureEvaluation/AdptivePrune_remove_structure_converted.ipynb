{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5e4cd1e-14c8-4f76-8133-e8617bb62b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The Baseline Method only use simple gait parameter feature, it only includes 2 dense layers\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "## We delete people's data with low feedback score and remove people who kicked off the sensors during walking'\n",
    "## So 20 people in total.\n",
    "person_nums = [1,2,4,5,6,8,9,10,11,12,13,17,19,21,22,25,26,27,28,29]\n",
    "\n",
    "from feature_emotion import feature_extract, split_data, label_unique_tuples\n",
    "gts, sensor_nums, walk_nums, trace_nums, people_nums, spe_centr, delta_spe_centr, spe_crest, delta_spe_crest, spe_decrease, delta_spe_decrease, spe_entropy, delta_spe_entropy, spe_flatness, delta_spe_flatness, spe_flux, delta_spe_flux, spe_kurtosis, delta_spe_kurtosis, spe_skewness, delta_spe_skewness, spe_rfp, delta_spe_rfp, spe_slope, delta_spe_slope, spe_spread, delta_spe_spread, wlk_fre, wlk_fres_trace, cwt_figs_all, cwt_sum_all_0, cwt_sum_all_1, cwt_sum_all_2, cwt_sum_all_3, high_fre_compos, pitchs, low_fre_compos, auto_corrs, real_hils, imag_hils, dur_time_1_alls, dur_time_2_alls, jitters, shimmers, jitter_rap, hrs, feature, slope, zcrs, fft_features, energy_alls, log_energy_alls, smoothe_energy_alls, legendres, double_support_time, pdps_new, lpcs, ceps_features = feature_extract(person_nums)\n",
    "\n",
    "walk_nums_all = np.squeeze(walk_nums)\n",
    "trace_nums_all = np.squeeze(trace_nums)\n",
    "people_nums_all = np.squeeze(people_nums)\n",
    "\n",
    "## 0: train, 1: validation 2: test\n",
    "flag_tr_val_te = split_data(walk_nums_all, trace_nums_all, people_nums_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "170353c1-fcb9-4e2b-9909-c3a263bf8bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 23:48:37.624783: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/lib/python3.11/site-packages/h5py/__init__.py:36: UserWarning: h5py is running against HDF5 1.14.2 when it was built against 1.14.1, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python3.11/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "## Data Normalization before training ans testing\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_v2_behavior()\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, concatenate, LSTM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scalers = []\n",
    "X_train_normalized = []\n",
    "X_val_normalized = []\n",
    "X_test_normalized = []\n",
    "train_idx = np.where(flag_tr_val_te ==0)[0]\n",
    "np.random.shuffle(train_idx)\n",
    "val_idx = np.where(flag_tr_val_te ==1)[0]\n",
    "test_idx = np.where(flag_tr_val_te ==2)[0]\n",
    "\n",
    "for i, feature in enumerate([wlk_fre, wlk_fres_trace, double_support_time, pdps_new, energy_alls, log_energy_alls, smoothe_energy_alls, dur_time_1_alls, dur_time_2_alls, spe_centr, delta_spe_centr, spe_crest, delta_spe_crest, spe_decrease, delta_spe_decrease, spe_entropy, delta_spe_entropy, spe_flatness, delta_spe_flatness, spe_flux, delta_spe_flux, spe_kurtosis, delta_spe_kurtosis, spe_skewness, delta_spe_skewness, spe_rfp, delta_spe_rfp, spe_slope, delta_spe_slope, spe_spread, delta_spe_spread, slope, lpcs, legendres,  ceps_features, real_hils, imag_hils, jitters, shimmers, jitter_rap, auto_corrs, hrs, zcrs]):\n",
    "    scaler = StandardScaler()\n",
    "    if len(feature.shape)==2:\n",
    "        X_train_i = feature[train_idx,:]\n",
    "        X_val_i = feature[val_idx,:]\n",
    "        X_test_i = feature[test_idx,:]\n",
    "        X_train_normalized_i = scaler.fit_transform(X_train_i)\n",
    "        X_val_normalized_i = scaler.transform(X_val_i)\n",
    "        X_test_normalized_i = scaler.transform(X_test_i)\n",
    "        scalers.append(scaler)\n",
    "    else:\n",
    "        X_train_i = feature[train_idx,:,:]\n",
    "        X_val_i = feature[val_idx,:,:]\n",
    "        X_test_i = feature[test_idx,:,:]\n",
    "        X_train_normalized_i = scaler.fit_transform(X_train_i.reshape(X_train_i.shape[0], -1)).reshape(X_train_i.shape)\n",
    "        X_val_normalized_i = scaler.transform(X_val_i.reshape(X_val_i.shape[0], -1)).reshape(X_val_i.shape)\n",
    "        X_test_normalized_i = scaler.transform(X_test_i.reshape(X_test_i.shape[0], -1)).reshape(X_test_i.shape)\n",
    "        scalers.append(scaler)\n",
    "    X_train_normalized.append(X_train_normalized_i)\n",
    "    X_val_normalized.append(X_val_normalized_i)\n",
    "    X_test_normalized.append(X_test_normalized_i)\n",
    "y_train = gts[train_idx,:]\n",
    "y_val = gts[val_idx,:]\n",
    "y_test = gts[test_idx,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79e10b0d-fa00-4007-ad02-296357d1871d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_normalized_new = []\n",
    "combined_feature = np.empty((len(X_train_normalized[0]),0))\n",
    "for feature in X_train_normalized:\n",
    "    if len(feature.shape) == 3:\n",
    "        X_train_normalized_new.append(feature)\n",
    "    elif feature.shape[1] <20:\n",
    "        combined_feature = np.hstack((combined_feature, feature))\n",
    "    else:\n",
    "        X_train_normalized_new.append(feature)\n",
    "X_train_normalized_new.append(combined_feature)\n",
    "\n",
    "X_val_normalized_new = []\n",
    "combined_feature = np.empty((len(X_val_normalized[0]),0))\n",
    "for feature in X_val_normalized:\n",
    "    if len(feature.shape) == 3:\n",
    "        X_val_normalized_new.append(feature)\n",
    "    elif feature.shape[1] <20:\n",
    "        combined_feature = np.hstack((combined_feature, feature))\n",
    "    else:\n",
    "        X_val_normalized_new.append(feature)\n",
    "X_val_normalized_new.append(combined_feature)\n",
    "\n",
    "X_test_normalized_new = []\n",
    "combined_feature = np.empty((len(X_test_normalized[0]),0))\n",
    "for feature in X_test_normalized:\n",
    "    if len(feature.shape) == 3:\n",
    "        X_test_normalized_new.append(feature)\n",
    "    elif feature.shape[1] <20:\n",
    "        combined_feature = np.hstack((combined_feature, feature))\n",
    "    else:\n",
    "        X_test_normalized_new.append(feature)\n",
    "X_test_normalized_new.append(combined_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25c8fb4a-5f62-4ff5-9d55-a5fd0197b624",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build the baseline model for emotion recognition with dropout layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, concatenate, SimpleRNN, LSTM, Conv2D, Flatten, MaxPooling2D, GRU, AveragePooling2D, Dropout, Lambda\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "def individual_model(features_list):\n",
    "    input_layers = []\n",
    "    hidden_layers = []\n",
    "    combined_feature = np.empty((len(features_list[0]),0))\n",
    "    for i, feature in enumerate(features_list):\n",
    "        \n",
    "        if len(feature.shape) == 3:\n",
    "            input_i = Input(shape=(feature.shape[1], feature.shape[2]))\n",
    "            input_layers.append(input_i)\n",
    "\n",
    "            hidden_i = input_i[:,:,:,None]\n",
    "            hidden_i = Conv2D(32, 3, activation='relu')(hidden_i)\n",
    "            hidden_i = AveragePooling2D((3, 3))(hidden_i)\n",
    "            hidden_i = Dropout(0.5)(hidden_i)\n",
    "            hidden_i = Conv2D(16, 3, activation='relu')(hidden_i)\n",
    "            hidden_i = AveragePooling2D((3, 3))(hidden_i)\n",
    "            hidden_i = Dropout(0.5)(hidden_i)\n",
    "            hidden_i = Conv2D(8, 1, activation='relu')(hidden_i)\n",
    "            hidden_i = AveragePooling2D((2, 2))(hidden_i)\n",
    "            hidden_i = Dropout(0.5)(hidden_i)\n",
    "            hidden_i = Conv2D(4, 1, activation='relu')(hidden_i)\n",
    "            hidden_i = AveragePooling2D((2, 2))(hidden_i)\n",
    "            hidden_i = Dropout(0.5)(hidden_i)\n",
    "            hidden_i = Flatten()(hidden_i)\n",
    "\n",
    "            hidden_layers.append(hidden_i)\n",
    "        elif feature.shape[1] <20:\n",
    "            combined_feature = np.hstack((combined_feature, feature))\n",
    "            \n",
    "        else:  # For series features\n",
    "            input_i = Input(shape=(feature.shape[1],))\n",
    "            input_layers.append(input_i)\n",
    "            hidden_i = Lambda(lambda x: x[:, :, None])(input_i)  # Add a new dimension\n",
    "            hidden_i = LSTM(4)(hidden_i)\n",
    "            hidden_layers.append(hidden_i)\n",
    "    input_i = Input(shape=(combined_feature.shape[1],))\n",
    "    input_layers.append(input_i)\n",
    "    dense_num = np.max((1, int(combined_feature.shape[1]/2)))\n",
    "    hidden_i = Dense(dense_num, activation='relu')(input_i)\n",
    "    hidden_layers.append(hidden_i)\n",
    "    print(combined_feature.shape)\n",
    "    concat_layer = concatenate(hidden_layers)\n",
    "    h = Dropout(0.2)(concat_layer)\n",
    "    h = Dense(64, activation='relu')(h)\n",
    "    h = Dense(32, activation='relu')(h)\n",
    "    output_layer = Dense(2)(h)\n",
    "    model = Model(inputs=input_layers, outputs=output_layer)\n",
    "    model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1acdad6-8e79-47f2-b3f6-9190d0bfad0f",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-12-09T07:56:16.049961Z",
     "iopub.status.busy": "2023-12-09T07:56:16.049849Z",
     "iopub.status.idle": "2023-12-09T14:03:10.893173Z",
     "shell.execute_reply": "2023-12-09T14:03:10.892564Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-09 02:56:46.121652: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-09 02:56:46.132875: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-09 02:56:46.133119: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-09 02:56:46.137109: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-09 02:56:46.137307: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-09 02:56:46.137480: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-09 02:56:46.218590: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-09 02:56:46.218797: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-09 02:56:46.218962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-09 02:56:46.219103: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46608 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:41:00.0, compute capability: 8.6\n",
      "2023-12-09 02:56:46.219558: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_16 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_17 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_18 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_19 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_20 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_22 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_23 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_24 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_25 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_26 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_27 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_28 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_29 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_30 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_31 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_32 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "(29601, 13)\n",
      "Train on 29601 samples, validate on 3694 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-09 02:56:50.461401: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-09 02:56:50.461652: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-09 02:56:50.461823: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-09 02:56:50.462033: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-09 02:56:50.462205: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-09 02:56:50.462347: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46608 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:41:00.0, compute capability: 8.6\n",
      "2023-12-09 02:56:50.462379: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2023-12-09 02:56:50.588570: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n",
      "2023-12-09 02:56:50.977354: W tensorflow/c/c_api.cc:304] Operation '{name:'training/Adam/lstm_4/lstm_cell_4/bias/m/Assign' id:14331 op device:{requested: '', assigned: ''} def:{{{node training/Adam/lstm_4/lstm_cell_4/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training/Adam/lstm_4/lstm_cell_4/bias/m, training/Adam/lstm_4/lstm_cell_4/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-09 02:56:55.722014: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29601/29601 [==============================] - ETA: 0s - loss: 3.8250"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.11/site-packages/keras/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n",
      "2023-12-09 02:57:09.131354: W tensorflow/c/c_api.cc:304] Operation '{name:'loss_1/mul' id:5588 op device:{requested: '', assigned: ''} def:{{{node loss_1/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_1/mul/x, loss_1/dense_3_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 2.36254, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_0.h5\n",
      "29601/29601 [==============================] - 18s 621us/sample - loss: 3.8250 - val_loss: 2.3625\n",
      "Epoch 2/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.9078\n",
      "Epoch 2: val_loss improved from 2.36254 to 1.67516, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_0.h5\n",
      "29601/29601 [==============================] - 14s 478us/sample - loss: 1.9078 - val_loss: 1.6752\n",
      "Epoch 3/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.6687\n",
      "Epoch 3: val_loss improved from 1.67516 to 1.60733, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_0.h5\n",
      "29601/29601 [==============================] - 14s 478us/sample - loss: 1.6687 - val_loss: 1.6073\n",
      "Epoch 4/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.6039\n",
      "Epoch 4: val_loss improved from 1.60733 to 1.58490, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_0.h5\n",
      "29601/29601 [==============================] - 14s 479us/sample - loss: 1.6039 - val_loss: 1.5849\n",
      "Epoch 5/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.5824\n",
      "Epoch 5: val_loss improved from 1.58490 to 1.56948, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_0.h5\n",
      "29601/29601 [==============================] - 14s 489us/sample - loss: 1.5824 - val_loss: 1.5695\n",
      "Epoch 6/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.5683\n",
      "Epoch 6: val_loss improved from 1.56948 to 1.55833, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_0.h5\n",
      "29601/29601 [==============================] - 14s 478us/sample - loss: 1.5683 - val_loss: 1.5583\n",
      "Epoch 7/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.5582\n",
      "Epoch 7: val_loss improved from 1.55833 to 1.54562, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_0.h5\n",
      "29601/29601 [==============================] - 14s 478us/sample - loss: 1.5582 - val_loss: 1.5456\n",
      "Epoch 8/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.5527\n",
      "Epoch 8: val_loss improved from 1.54562 to 1.54081, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_0.h5\n",
      "29601/29601 [==============================] - 14s 479us/sample - loss: 1.5527 - val_loss: 1.5408\n",
      "Epoch 9/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.5481\n",
      "Epoch 9: val_loss improved from 1.54081 to 1.53151, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_0.h5\n",
      "29601/29601 [==============================] - 14s 478us/sample - loss: 1.5481 - val_loss: 1.5315\n",
      "Epoch 10/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.5391\n",
      "Epoch 10: val_loss improved from 1.53151 to 1.52720, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_0.h5\n",
      "29601/29601 [==============================] - 15s 514us/sample - loss: 1.5391 - val_loss: 1.5272\n",
      "Epoch 11/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.5360\n",
      "Epoch 11: val_loss improved from 1.52720 to 1.52022, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_0.h5\n",
      "29601/29601 [==============================] - 17s 560us/sample - loss: 1.5360 - val_loss: 1.5202\n",
      "Epoch 12/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.5310\n",
      "Epoch 12: val_loss improved from 1.52022 to 1.51897, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_0.h5\n",
      "29601/29601 [==============================] - 15s 512us/sample - loss: 1.5310 - val_loss: 1.5190\n",
      "Epoch 13/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.5252\n",
      "Epoch 13: val_loss improved from 1.51897 to 1.51476, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_0.h5\n",
      "29601/29601 [==============================] - 14s 479us/sample - loss: 1.5252 - val_loss: 1.5148\n",
      "Epoch 14/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.5234\n",
      "Epoch 14: val_loss improved from 1.51476 to 1.50858, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_0.h5\n",
      "29601/29601 [==============================] - 15s 516us/sample - loss: 1.5234 - val_loss: 1.5086\n",
      "Epoch 15/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.5180\n",
      "Epoch 15: val_loss did not improve from 1.50858\n",
      "29601/29601 [==============================] - 16s 531us/sample - loss: 1.5180 - val_loss: 1.5131\n",
      "Epoch 16/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.5143\n",
      "Epoch 16: val_loss improved from 1.50858 to 1.50225, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_0.h5\n",
      "29601/29601 [==============================] - 16s 524us/sample - loss: 1.5143 - val_loss: 1.5022\n",
      "Epoch 17/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.5108\n",
      "Epoch 17: val_loss improved from 1.50225 to 1.49797, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_0.h5\n",
      "29601/29601 [==============================] - 15s 508us/sample - loss: 1.5108 - val_loss: 1.4980\n",
      "Epoch 18/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.5091\n",
      "Epoch 18: val_loss improved from 1.49797 to 1.49679, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_0.h5\n",
      "29601/29601 [==============================] - 16s 538us/sample - loss: 1.5091 - val_loss: 1.4968\n",
      "Epoch 19/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.5044\n",
      "Epoch 19: val_loss improved from 1.49679 to 1.49030, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_0.h5\n",
      "29601/29601 [==============================] - 16s 524us/sample - loss: 1.5044 - val_loss: 1.4903\n",
      "Epoch 20/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.5021\n",
      "Epoch 20: val_loss did not improve from 1.49030\n",
      "29601/29601 [==============================] - 16s 535us/sample - loss: 1.5021 - val_loss: 1.4922\n",
      "Epoch 21/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4997\n",
      "Epoch 21: val_loss improved from 1.49030 to 1.48408, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_0.h5\n",
      "29601/29601 [==============================] - 16s 540us/sample - loss: 1.4997 - val_loss: 1.4841\n",
      "Epoch 22/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4971\n",
      "Epoch 22: val_loss did not improve from 1.48408\n",
      "29601/29601 [==============================] - 15s 514us/sample - loss: 1.4971 - val_loss: 1.4913\n",
      "Epoch 23/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4926\n",
      "Epoch 23: val_loss improved from 1.48408 to 1.48038, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_0.h5\n",
      "29601/29601 [==============================] - 14s 484us/sample - loss: 1.4926 - val_loss: 1.4804\n",
      "Epoch 24/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4908\n",
      "Epoch 24: val_loss improved from 1.48038 to 1.47963, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_0.h5\n",
      "29601/29601 [==============================] - 16s 551us/sample - loss: 1.4908 - val_loss: 1.4796\n",
      "Epoch 25/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4875\n",
      "Epoch 25: val_loss improved from 1.47963 to 1.47556, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_0.h5\n",
      "29601/29601 [==============================] - 17s 572us/sample - loss: 1.4875 - val_loss: 1.4756\n",
      "Epoch 26/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4844\n",
      "Epoch 26: val_loss improved from 1.47556 to 1.47347, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_0.h5\n",
      "29601/29601 [==============================] - 16s 525us/sample - loss: 1.4844 - val_loss: 1.4735\n",
      "Epoch 27/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4811\n",
      "Epoch 27: val_loss improved from 1.47347 to 1.47241, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_0.h5\n",
      "29601/29601 [==============================] - 17s 558us/sample - loss: 1.4811 - val_loss: 1.4724\n",
      "Epoch 28/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4764\n",
      "Epoch 28: val_loss did not improve from 1.47241\n",
      "29601/29601 [==============================] - 16s 534us/sample - loss: 1.4764 - val_loss: 1.4735\n",
      "Epoch 29/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4761\n",
      "Epoch 29: val_loss improved from 1.47241 to 1.46801, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_0.h5\n",
      "29601/29601 [==============================] - 16s 545us/sample - loss: 1.4761 - val_loss: 1.4680\n",
      "Epoch 30/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4734\n",
      "Epoch 30: val_loss improved from 1.46801 to 1.46361, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_0.h5\n",
      "29601/29601 [==============================] - 15s 503us/sample - loss: 1.4734 - val_loss: 1.4636\n",
      "Epoch 31/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4718\n",
      "Epoch 31: val_loss improved from 1.46361 to 1.45579, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_0.h5\n",
      "29601/29601 [==============================] - 17s 568us/sample - loss: 1.4718 - val_loss: 1.4558\n",
      "Epoch 32/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4707\n",
      "Epoch 32: val_loss did not improve from 1.45579\n",
      "29601/29601 [==============================] - 16s 557us/sample - loss: 1.4707 - val_loss: 1.4586\n",
      "Epoch 33/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4692\n",
      "Epoch 33: val_loss improved from 1.45579 to 1.45257, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_0.h5\n",
      "29601/29601 [==============================] - 16s 556us/sample - loss: 1.4692 - val_loss: 1.4526\n",
      "Epoch 34/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4642\n",
      "Epoch 34: val_loss improved from 1.45257 to 1.45132, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_0.h5\n",
      "29601/29601 [==============================] - 14s 474us/sample - loss: 1.4642 - val_loss: 1.4513\n",
      "Epoch 35/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4602\n",
      "Epoch 35: val_loss did not improve from 1.45132\n",
      "29601/29601 [==============================] - 14s 472us/sample - loss: 1.4602 - val_loss: 1.4524\n",
      "Epoch 36/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4604\n",
      "Epoch 36: val_loss improved from 1.45132 to 1.44334, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_0.h5\n",
      "29601/29601 [==============================] - 15s 493us/sample - loss: 1.4604 - val_loss: 1.4433\n",
      "Epoch 37/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4579\n",
      "Epoch 37: val_loss improved from 1.44334 to 1.44169, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_0.h5\n",
      "29601/29601 [==============================] - 16s 533us/sample - loss: 1.4579 - val_loss: 1.4417\n",
      "Epoch 38/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4586\n",
      "Epoch 38: val_loss improved from 1.44169 to 1.44025, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_0.h5\n",
      "29601/29601 [==============================] - 14s 476us/sample - loss: 1.4586 - val_loss: 1.4402\n",
      "Epoch 39/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4545\n",
      "Epoch 39: val_loss improved from 1.44025 to 1.43917, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_0.h5\n",
      "29601/29601 [==============================] - 14s 483us/sample - loss: 1.4545 - val_loss: 1.4392\n",
      "Epoch 40/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4546\n",
      "Epoch 40: val_loss did not improve from 1.43917\n",
      "29601/29601 [==============================] - 17s 558us/sample - loss: 1.4546 - val_loss: 1.4433\n",
      "Epoch 41/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4522\n",
      "Epoch 41: val_loss improved from 1.43917 to 1.43667, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_0.h5\n",
      "29601/29601 [==============================] - 17s 558us/sample - loss: 1.4522 - val_loss: 1.4367\n",
      "Epoch 42/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4515\n",
      "Epoch 42: val_loss did not improve from 1.43667\n",
      "29601/29601 [==============================] - 14s 481us/sample - loss: 1.4515 - val_loss: 1.4413\n",
      "Epoch 43/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4515\n",
      "Epoch 43: val_loss did not improve from 1.43667\n",
      "29601/29601 [==============================] - 15s 501us/sample - loss: 1.4515 - val_loss: 1.4384\n",
      "Epoch 44/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4463\n",
      "Epoch 44: val_loss did not improve from 1.43667\n",
      "29601/29601 [==============================] - 17s 559us/sample - loss: 1.4463 - val_loss: 1.4369\n",
      "Epoch 45/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4442\n",
      "Epoch 45: val_loss improved from 1.43667 to 1.43024, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_0.h5\n",
      "29601/29601 [==============================] - 15s 494us/sample - loss: 1.4442 - val_loss: 1.4302\n",
      "Epoch 46/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4427\n",
      "Epoch 46: val_loss did not improve from 1.43024\n",
      "29601/29601 [==============================] - 14s 481us/sample - loss: 1.4427 - val_loss: 1.4310\n",
      "Epoch 47/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4411\n",
      "Epoch 47: val_loss did not improve from 1.43024\n",
      "29601/29601 [==============================] - 15s 491us/sample - loss: 1.4411 - val_loss: 1.4305\n",
      "Epoch 48/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4420\n",
      "Epoch 48: val_loss improved from 1.43024 to 1.42224, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_0.h5\n",
      "29601/29601 [==============================] - 15s 494us/sample - loss: 1.4420 - val_loss: 1.4222\n",
      "Epoch 49/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4374\n",
      "Epoch 49: val_loss did not improve from 1.42224\n",
      "29601/29601 [==============================] - 14s 480us/sample - loss: 1.4374 - val_loss: 1.4282\n",
      "Epoch 50/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4399\n",
      "Epoch 50: val_loss did not improve from 1.42224\n",
      "29601/29601 [==============================] - 15s 491us/sample - loss: 1.4399 - val_loss: 1.4237\n",
      "Epoch 51/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4351\n",
      "Epoch 51: val_loss did not improve from 1.42224\n",
      "29601/29601 [==============================] - 17s 559us/sample - loss: 1.4351 - val_loss: 1.4248\n",
      "Epoch 52/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4357\n",
      "Epoch 52: val_loss improved from 1.42224 to 1.42058, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_0.h5\n",
      "29601/29601 [==============================] - 16s 530us/sample - loss: 1.4357 - val_loss: 1.4206\n",
      "Epoch 53/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4346\n",
      "Epoch 53: val_loss improved from 1.42058 to 1.41745, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_0.h5\n",
      "29601/29601 [==============================] - 15s 492us/sample - loss: 1.4346 - val_loss: 1.4174\n",
      "Epoch 54/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4328\n",
      "Epoch 54: val_loss did not improve from 1.41745\n",
      "29601/29601 [==============================] - 17s 560us/sample - loss: 1.4328 - val_loss: 1.4203\n",
      "Epoch 55/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4321\n",
      "Epoch 55: val_loss did not improve from 1.41745\n",
      "29601/29601 [==============================] - 14s 483us/sample - loss: 1.4321 - val_loss: 1.4181\n",
      "Epoch 56/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4292\n",
      "Epoch 56: val_loss did not improve from 1.41745\n",
      "29601/29601 [==============================] - 14s 477us/sample - loss: 1.4292 - val_loss: 1.4190\n",
      "Epoch 57/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4297\n",
      "Epoch 57: val_loss did not improve from 1.41745\n",
      "29601/29601 [==============================] - 14s 476us/sample - loss: 1.4297 - val_loss: 1.4214\n",
      "Epoch 58/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4280\n",
      "Epoch 58: val_loss improved from 1.41745 to 1.41383, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_0.h5\n",
      "29601/29601 [==============================] - 14s 479us/sample - loss: 1.4280 - val_loss: 1.4138\n",
      "Epoch 59/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4243\n",
      "Epoch 59: val_loss improved from 1.41383 to 1.40861, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_0.h5\n",
      "29601/29601 [==============================] - 14s 479us/sample - loss: 1.4243 - val_loss: 1.4086\n",
      "Epoch 60/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4220\n",
      "Epoch 60: val_loss improved from 1.40861 to 1.40542, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_0.h5\n",
      "29601/29601 [==============================] - 14s 480us/sample - loss: 1.4220 - val_loss: 1.4054\n",
      "Epoch 61/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4235\n",
      "Epoch 61: val_loss did not improve from 1.40542\n",
      "29601/29601 [==============================] - 14s 475us/sample - loss: 1.4235 - val_loss: 1.4068\n",
      "Epoch 62/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4215\n",
      "Epoch 62: val_loss did not improve from 1.40542\n",
      "29601/29601 [==============================] - 14s 469us/sample - loss: 1.4215 - val_loss: 1.4134\n",
      "Epoch 63/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4210\n",
      "Epoch 63: val_loss did not improve from 1.40542\n",
      "29601/29601 [==============================] - 15s 504us/sample - loss: 1.4210 - val_loss: 1.4114\n",
      "Epoch 64/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4164\n",
      "Epoch 64: val_loss did not improve from 1.40542\n",
      "29601/29601 [==============================] - 14s 474us/sample - loss: 1.4164 - val_loss: 1.4084\n",
      "Epoch 65/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4178\n",
      "Epoch 65: val_loss improved from 1.40542 to 1.40280, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_0.h5\n",
      "29601/29601 [==============================] - 14s 473us/sample - loss: 1.4178 - val_loss: 1.4028\n",
      "Epoch 66/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4177\n",
      "Epoch 66: val_loss did not improve from 1.40280\n",
      "29601/29601 [==============================] - 14s 474us/sample - loss: 1.4177 - val_loss: 1.4047\n",
      "Epoch 67/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4189\n",
      "Epoch 67: val_loss did not improve from 1.40280\n",
      "29601/29601 [==============================] - 14s 476us/sample - loss: 1.4189 - val_loss: 1.4049\n",
      "Epoch 68/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4129\n",
      "Epoch 68: val_loss did not improve from 1.40280\n",
      "29601/29601 [==============================] - 14s 476us/sample - loss: 1.4129 - val_loss: 1.4054\n",
      "Epoch 69/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4138\n",
      "Epoch 69: val_loss did not improve from 1.40280\n",
      "29601/29601 [==============================] - 14s 475us/sample - loss: 1.4138 - val_loss: 1.4049\n",
      "Epoch 70/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4141\n",
      "Epoch 70: val_loss improved from 1.40280 to 1.40118, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_0.h5\n",
      "29601/29601 [==============================] - 14s 479us/sample - loss: 1.4141 - val_loss: 1.4012\n",
      "Epoch 71/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4132\n",
      "Epoch 71: val_loss did not improve from 1.40118\n",
      "29601/29601 [==============================] - 14s 482us/sample - loss: 1.4132 - val_loss: 1.4054\n",
      "Epoch 72/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4096\n",
      "Epoch 72: val_loss did not improve from 1.40118\n",
      "29601/29601 [==============================] - 16s 548us/sample - loss: 1.4096 - val_loss: 1.4025\n",
      "Epoch 73/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4110\n",
      "Epoch 73: val_loss improved from 1.40118 to 1.40005, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_0.h5\n",
      "29601/29601 [==============================] - 14s 484us/sample - loss: 1.4110 - val_loss: 1.4000\n",
      "Epoch 74/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4085\n",
      "Epoch 74: val_loss did not improve from 1.40005\n",
      "29601/29601 [==============================] - 14s 475us/sample - loss: 1.4085 - val_loss: 1.4001\n",
      "Epoch 75/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4098\n",
      "Epoch 75: val_loss improved from 1.40005 to 1.39628, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_0.h5\n",
      "29601/29601 [==============================] - 14s 479us/sample - loss: 1.4098 - val_loss: 1.3963\n",
      "Epoch 76/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4063\n",
      "Epoch 76: val_loss improved from 1.39628 to 1.39393, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_0.h5\n",
      "29601/29601 [==============================] - 14s 479us/sample - loss: 1.4063 - val_loss: 1.3939\n",
      "Epoch 77/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4067\n",
      "Epoch 77: val_loss did not improve from 1.39393\n",
      "29601/29601 [==============================] - 15s 500us/sample - loss: 1.4067 - val_loss: 1.4016\n",
      "Epoch 78/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4073\n",
      "Epoch 78: val_loss did not improve from 1.39393\n",
      "29601/29601 [==============================] - 15s 509us/sample - loss: 1.4073 - val_loss: 1.3978\n",
      "Epoch 79/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4056\n",
      "Epoch 79: val_loss did not improve from 1.39393\n",
      "29601/29601 [==============================] - 15s 522us/sample - loss: 1.4056 - val_loss: 1.3983\n",
      "Epoch 80/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4042\n",
      "Epoch 80: val_loss did not improve from 1.39393\n",
      "29601/29601 [==============================] - 15s 493us/sample - loss: 1.4042 - val_loss: 1.3973\n",
      "Epoch 81/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4035\n",
      "Epoch 81: val_loss improved from 1.39393 to 1.39129, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_0.h5\n",
      "29601/29601 [==============================] - 14s 479us/sample - loss: 1.4035 - val_loss: 1.3913\n",
      "Epoch 82/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4013\n",
      "Epoch 82: val_loss did not improve from 1.39129\n",
      "29601/29601 [==============================] - 14s 476us/sample - loss: 1.4013 - val_loss: 1.3974\n",
      "Epoch 83/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4031\n",
      "Epoch 83: val_loss did not improve from 1.39129\n",
      "29601/29601 [==============================] - 14s 477us/sample - loss: 1.4031 - val_loss: 1.3988\n",
      "Epoch 84/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4024\n",
      "Epoch 84: val_loss did not improve from 1.39129\n",
      "29601/29601 [==============================] - 14s 477us/sample - loss: 1.4024 - val_loss: 1.3929\n",
      "Epoch 85/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3994\n",
      "Epoch 85: val_loss did not improve from 1.39129\n",
      "29601/29601 [==============================] - 14s 476us/sample - loss: 1.3994 - val_loss: 1.3941\n",
      "Epoch 86/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3978\n",
      "Epoch 86: val_loss did not improve from 1.39129\n",
      "29601/29601 [==============================] - 14s 476us/sample - loss: 1.3978 - val_loss: 1.3956\n",
      "Epoch 87/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3976\n",
      "Epoch 87: val_loss did not improve from 1.39129\n",
      "29601/29601 [==============================] - 14s 475us/sample - loss: 1.3976 - val_loss: 1.3964\n",
      "Epoch 88/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3990\n",
      "Epoch 88: val_loss did not improve from 1.39129\n",
      "29601/29601 [==============================] - 14s 475us/sample - loss: 1.3990 - val_loss: 1.3947\n",
      "Epoch 89/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3962\n",
      "Epoch 89: val_loss improved from 1.39129 to 1.38892, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_0.h5\n",
      "29601/29601 [==============================] - 14s 479us/sample - loss: 1.3962 - val_loss: 1.3889\n",
      "Epoch 90/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3954\n",
      "Epoch 90: val_loss did not improve from 1.38892\n",
      "29601/29601 [==============================] - 14s 477us/sample - loss: 1.3954 - val_loss: 1.3913\n",
      "Epoch 91/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3952\n",
      "Epoch 91: val_loss did not improve from 1.38892\n",
      "29601/29601 [==============================] - 14s 472us/sample - loss: 1.3952 - val_loss: 1.3897\n",
      "Epoch 92/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3956\n",
      "Epoch 92: val_loss did not improve from 1.38892\n",
      "29601/29601 [==============================] - 14s 469us/sample - loss: 1.3956 - val_loss: 1.3931\n",
      "Epoch 93/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3932\n",
      "Epoch 93: val_loss did not improve from 1.38892\n",
      "29601/29601 [==============================] - 14s 469us/sample - loss: 1.3932 - val_loss: 1.3930\n",
      "Epoch 94/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3934\n",
      "Epoch 94: val_loss did not improve from 1.38892\n",
      "29601/29601 [==============================] - 14s 472us/sample - loss: 1.3934 - val_loss: 1.3914\n",
      "Epoch 95/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3939\n",
      "Epoch 95: val_loss did not improve from 1.38892\n",
      "29601/29601 [==============================] - 19s 626us/sample - loss: 1.3939 - val_loss: 1.3898\n",
      "Epoch 96/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3907\n",
      "Epoch 96: val_loss improved from 1.38892 to 1.38756, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_0.h5\n",
      "29601/29601 [==============================] - 24s 818us/sample - loss: 1.3907 - val_loss: 1.3876\n",
      "Epoch 97/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3887\n",
      "Epoch 97: val_loss did not improve from 1.38756\n",
      "29601/29601 [==============================] - 22s 738us/sample - loss: 1.3887 - val_loss: 1.3923\n",
      "Epoch 98/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3894\n",
      "Epoch 98: val_loss did not improve from 1.38756\n",
      "29601/29601 [==============================] - 21s 721us/sample - loss: 1.3894 - val_loss: 1.3913\n",
      "Epoch 99/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3893\n",
      "Epoch 99: val_loss improved from 1.38756 to 1.38733, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_0.h5\n",
      "29601/29601 [==============================] - 22s 749us/sample - loss: 1.3893 - val_loss: 1.3873\n",
      "Epoch 100/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3867\n",
      "Epoch 100: val_loss improved from 1.38733 to 1.38511, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_0.h5\n",
      "29601/29601 [==============================] - 23s 789us/sample - loss: 1.3867 - val_loss: 1.3851\n",
      "Train on 29601 samples, validate on 3694 samples\n",
      "Epoch 1/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3846\n",
      "Epoch 1: val_loss improved from inf to 1.38312, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_1.h5\n",
      "29601/29601 [==============================] - 23s 766us/sample - loss: 1.3846 - val_loss: 1.3831\n",
      "Epoch 2/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3890\n",
      "Epoch 2: val_loss did not improve from 1.38312\n",
      "29601/29601 [==============================] - 24s 815us/sample - loss: 1.3890 - val_loss: 1.3848\n",
      "Epoch 3/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3848\n",
      "Epoch 3: val_loss improved from 1.38312 to 1.38165, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_1.h5\n",
      "29601/29601 [==============================] - 22s 746us/sample - loss: 1.3848 - val_loss: 1.3816\n",
      "Epoch 4/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3802\n",
      "Epoch 4: val_loss did not improve from 1.38165\n",
      "29601/29601 [==============================] - 19s 653us/sample - loss: 1.3802 - val_loss: 1.3852\n",
      "Epoch 5/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3820\n",
      "Epoch 5: val_loss did not improve from 1.38165\n",
      "29601/29601 [==============================] - 21s 693us/sample - loss: 1.3820 - val_loss: 1.3828\n",
      "Epoch 6/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3805\n",
      "Epoch 6: val_loss did not improve from 1.38165\n",
      "29601/29601 [==============================] - 24s 809us/sample - loss: 1.3805 - val_loss: 1.3850\n",
      "Epoch 7/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3817\n",
      "Epoch 7: val_loss improved from 1.38165 to 1.38080, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_1.h5\n",
      "29601/29601 [==============================] - 23s 778us/sample - loss: 1.3817 - val_loss: 1.3808\n",
      "Epoch 8/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3784\n",
      "Epoch 8: val_loss did not improve from 1.38080\n",
      "29601/29601 [==============================] - 23s 784us/sample - loss: 1.3784 - val_loss: 1.3849\n",
      "Epoch 9/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3794\n",
      "Epoch 9: val_loss improved from 1.38080 to 1.38032, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_1.h5\n",
      "29601/29601 [==============================] - 24s 802us/sample - loss: 1.3794 - val_loss: 1.3803\n",
      "Epoch 10/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3780\n",
      "Epoch 10: val_loss improved from 1.38032 to 1.37941, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_1.h5\n",
      "29601/29601 [==============================] - 23s 791us/sample - loss: 1.3780 - val_loss: 1.3794\n",
      "Epoch 11/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3773\n",
      "Epoch 11: val_loss did not improve from 1.37941\n",
      "29601/29601 [==============================] - 23s 764us/sample - loss: 1.3773 - val_loss: 1.3838\n",
      "Epoch 12/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3744\n",
      "Epoch 12: val_loss did not improve from 1.37941\n",
      "29601/29601 [==============================] - 23s 762us/sample - loss: 1.3744 - val_loss: 1.3816\n",
      "Epoch 13/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3759\n",
      "Epoch 13: val_loss improved from 1.37941 to 1.37845, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_1.h5\n",
      "29601/29601 [==============================] - 23s 790us/sample - loss: 1.3759 - val_loss: 1.3784\n",
      "Epoch 14/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3749\n",
      "Epoch 14: val_loss improved from 1.37845 to 1.37657, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_1.h5\n",
      "29601/29601 [==============================] - 21s 707us/sample - loss: 1.3749 - val_loss: 1.3766\n",
      "Epoch 15/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3770\n",
      "Epoch 15: val_loss did not improve from 1.37657\n",
      "29601/29601 [==============================] - 24s 799us/sample - loss: 1.3770 - val_loss: 1.3780\n",
      "Epoch 16/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3761\n",
      "Epoch 16: val_loss did not improve from 1.37657\n",
      "29601/29601 [==============================] - 23s 784us/sample - loss: 1.3761 - val_loss: 1.3893\n",
      "Epoch 17/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3776\n",
      "Epoch 17: val_loss did not improve from 1.37657\n",
      "29601/29601 [==============================] - 22s 758us/sample - loss: 1.3776 - val_loss: 1.3771\n",
      "Epoch 18/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3927\n",
      "Epoch 18: val_loss did not improve from 1.37657\n",
      "29601/29601 [==============================] - 21s 715us/sample - loss: 1.3927 - val_loss: 1.3846\n",
      "Epoch 19/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3737\n",
      "Epoch 19: val_loss did not improve from 1.37657\n",
      "29601/29601 [==============================] - 23s 762us/sample - loss: 1.3737 - val_loss: 1.3830\n",
      "Epoch 20/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3739\n",
      "Epoch 20: val_loss did not improve from 1.37657\n",
      "29601/29601 [==============================] - 23s 781us/sample - loss: 1.3739 - val_loss: 1.3772\n",
      "Epoch 21/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3718\n",
      "Epoch 21: val_loss improved from 1.37657 to 1.37451, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_1.h5\n",
      "29601/29601 [==============================] - 22s 758us/sample - loss: 1.3718 - val_loss: 1.3745\n",
      "Epoch 22/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3713\n",
      "Epoch 22: val_loss improved from 1.37451 to 1.37393, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_1.h5\n",
      "29601/29601 [==============================] - 22s 749us/sample - loss: 1.3713 - val_loss: 1.3739\n",
      "Epoch 23/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3710\n",
      "Epoch 23: val_loss did not improve from 1.37393\n",
      "29601/29601 [==============================] - 21s 723us/sample - loss: 1.3710 - val_loss: 1.3767\n",
      "Epoch 24/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3690\n",
      "Epoch 24: val_loss improved from 1.37393 to 1.37330, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_1.h5\n",
      "29601/29601 [==============================] - 20s 681us/sample - loss: 1.3690 - val_loss: 1.3733\n",
      "Epoch 25/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3691\n",
      "Epoch 25: val_loss did not improve from 1.37330\n",
      "29601/29601 [==============================] - 24s 802us/sample - loss: 1.3691 - val_loss: 1.3742\n",
      "Epoch 26/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3690\n",
      "Epoch 26: val_loss improved from 1.37330 to 1.37198, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_1.h5\n",
      "29601/29601 [==============================] - 22s 759us/sample - loss: 1.3690 - val_loss: 1.3720\n",
      "Epoch 27/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3655\n",
      "Epoch 27: val_loss did not improve from 1.37198\n",
      "29601/29601 [==============================] - 23s 788us/sample - loss: 1.3655 - val_loss: 1.3777\n",
      "Epoch 28/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3725\n",
      "Epoch 28: val_loss did not improve from 1.37198\n",
      "29601/29601 [==============================] - 24s 808us/sample - loss: 1.3725 - val_loss: 1.3779\n",
      "Epoch 29/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3688\n",
      "Epoch 29: val_loss did not improve from 1.37198\n",
      "29601/29601 [==============================] - 22s 743us/sample - loss: 1.3688 - val_loss: 1.3742\n",
      "Epoch 30/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3690\n",
      "Epoch 30: val_loss did not improve from 1.37198\n",
      "29601/29601 [==============================] - 23s 763us/sample - loss: 1.3690 - val_loss: 1.3750\n",
      "Epoch 31/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3697\n",
      "Epoch 31: val_loss improved from 1.37198 to 1.37076, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_1.h5\n",
      "29601/29601 [==============================] - 23s 772us/sample - loss: 1.3697 - val_loss: 1.3708\n",
      "Epoch 32/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3673\n",
      "Epoch 32: val_loss did not improve from 1.37076\n",
      "29601/29601 [==============================] - 22s 748us/sample - loss: 1.3673 - val_loss: 1.3751\n",
      "Epoch 33/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3640\n",
      "Epoch 33: val_loss did not improve from 1.37076\n",
      "29601/29601 [==============================] - 24s 809us/sample - loss: 1.3640 - val_loss: 1.3712\n",
      "Epoch 34/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3731\n",
      "Epoch 34: val_loss did not improve from 1.37076\n",
      "29601/29601 [==============================] - 23s 770us/sample - loss: 1.3731 - val_loss: 1.3754\n",
      "Epoch 35/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3651\n",
      "Epoch 35: val_loss did not improve from 1.37076\n",
      "29601/29601 [==============================] - 23s 774us/sample - loss: 1.3651 - val_loss: 1.3711\n",
      "Epoch 36/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3645\n",
      "Epoch 36: val_loss did not improve from 1.37076\n",
      "29601/29601 [==============================] - 24s 800us/sample - loss: 1.3645 - val_loss: 1.3757\n",
      "Epoch 37/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3657\n",
      "Epoch 37: val_loss did not improve from 1.37076\n",
      "29601/29601 [==============================] - 23s 768us/sample - loss: 1.3657 - val_loss: 1.3727\n",
      "Epoch 38/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3656\n",
      "Epoch 38: val_loss did not improve from 1.37076\n",
      "29601/29601 [==============================] - 23s 768us/sample - loss: 1.3656 - val_loss: 1.3728\n",
      "Epoch 39/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3647\n",
      "Epoch 39: val_loss did not improve from 1.37076\n",
      "29601/29601 [==============================] - 23s 786us/sample - loss: 1.3647 - val_loss: 1.3717\n",
      "Epoch 40/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3612\n",
      "Epoch 40: val_loss did not improve from 1.37076\n",
      "29601/29601 [==============================] - 22s 749us/sample - loss: 1.3612 - val_loss: 1.3765\n",
      "Epoch 41/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3619\n",
      "Epoch 41: val_loss did not improve from 1.37076\n",
      "29601/29601 [==============================] - 23s 792us/sample - loss: 1.3619 - val_loss: 1.3782\n",
      "Epoch 42/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3698\n",
      "Epoch 42: val_loss did not improve from 1.37076\n",
      "29601/29601 [==============================] - 23s 773us/sample - loss: 1.3698 - val_loss: 1.3744\n",
      "Epoch 43/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3609\n",
      "Epoch 43: val_loss did not improve from 1.37076\n",
      "29601/29601 [==============================] - 23s 779us/sample - loss: 1.3609 - val_loss: 1.3760\n",
      "Epoch 44/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3634\n",
      "Epoch 44: val_loss did not improve from 1.37076\n",
      "29601/29601 [==============================] - 24s 814us/sample - loss: 1.3634 - val_loss: 1.3770\n",
      "Epoch 45/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3627\n",
      "Epoch 45: val_loss did not improve from 1.37076\n",
      "29601/29601 [==============================] - 21s 714us/sample - loss: 1.3627 - val_loss: 1.3719\n",
      "Epoch 46/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3610\n",
      "Epoch 46: val_loss did not improve from 1.37076\n",
      "29601/29601 [==============================] - 21s 709us/sample - loss: 1.3610 - val_loss: 1.3741\n",
      "Epoch 47/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3601\n",
      "Epoch 47: val_loss did not improve from 1.37076\n",
      "29601/29601 [==============================] - 24s 796us/sample - loss: 1.3601 - val_loss: 1.3791\n",
      "Epoch 48/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3616\n",
      "Epoch 48: val_loss did not improve from 1.37076\n",
      "29601/29601 [==============================] - 23s 787us/sample - loss: 1.3616 - val_loss: 1.3839\n",
      "Epoch 49/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3586\n",
      "Epoch 49: val_loss did not improve from 1.37076\n",
      "29601/29601 [==============================] - 24s 826us/sample - loss: 1.3586 - val_loss: 1.3713\n",
      "Epoch 50/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3670\n",
      "Epoch 50: val_loss did not improve from 1.37076\n",
      "29601/29601 [==============================] - 24s 821us/sample - loss: 1.3670 - val_loss: 1.3758\n",
      "Epoch 51/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3590\n",
      "Epoch 51: val_loss did not improve from 1.37076\n",
      "29601/29601 [==============================] - 23s 790us/sample - loss: 1.3590 - val_loss: 1.3764\n",
      "Epoch 52/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3549\n",
      "Epoch 52: val_loss did not improve from 1.37076\n",
      "29601/29601 [==============================] - 23s 777us/sample - loss: 1.3549 - val_loss: 1.3760\n",
      "Epoch 53/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3579\n",
      "Epoch 53: val_loss did not improve from 1.37076\n",
      "29601/29601 [==============================] - 24s 795us/sample - loss: 1.3579 - val_loss: 1.3716\n",
      "Epoch 54/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3570\n",
      "Epoch 54: val_loss did not improve from 1.37076\n",
      "29601/29601 [==============================] - 23s 775us/sample - loss: 1.3570 - val_loss: 1.3731\n",
      "Epoch 55/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3546\n",
      "Epoch 55: val_loss improved from 1.37076 to 1.36530, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_1.h5\n",
      "29601/29601 [==============================] - 25s 833us/sample - loss: 1.3546 - val_loss: 1.3653\n",
      "Epoch 56/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3541\n",
      "Epoch 56: val_loss did not improve from 1.36530\n",
      "29601/29601 [==============================] - 22s 733us/sample - loss: 1.3541 - val_loss: 1.3700\n",
      "Epoch 57/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3520\n",
      "Epoch 57: val_loss did not improve from 1.36530\n",
      "29601/29601 [==============================] - 21s 707us/sample - loss: 1.3520 - val_loss: 1.3763\n",
      "Epoch 58/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3539\n",
      "Epoch 58: val_loss did not improve from 1.36530\n",
      "29601/29601 [==============================] - 21s 698us/sample - loss: 1.3539 - val_loss: 1.3741\n",
      "Epoch 59/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3537\n",
      "Epoch 59: val_loss did not improve from 1.36530\n",
      "29601/29601 [==============================] - 23s 768us/sample - loss: 1.3537 - val_loss: 1.3702\n",
      "Epoch 60/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3510\n",
      "Epoch 60: val_loss did not improve from 1.36530\n",
      "29601/29601 [==============================] - 22s 740us/sample - loss: 1.3510 - val_loss: 1.3717\n",
      "Epoch 61/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3490\n",
      "Epoch 61: val_loss did not improve from 1.36530\n",
      "29601/29601 [==============================] - 21s 704us/sample - loss: 1.3490 - val_loss: 1.3732\n",
      "Epoch 62/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3494\n",
      "Epoch 62: val_loss did not improve from 1.36530\n",
      "29601/29601 [==============================] - 22s 760us/sample - loss: 1.3494 - val_loss: 1.3703\n",
      "Epoch 63/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3472\n",
      "Epoch 63: val_loss did not improve from 1.36530\n",
      "29601/29601 [==============================] - 22s 746us/sample - loss: 1.3472 - val_loss: 1.3706\n",
      "Epoch 64/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3471\n",
      "Epoch 64: val_loss did not improve from 1.36530\n",
      "29601/29601 [==============================] - 24s 806us/sample - loss: 1.3471 - val_loss: 1.3711\n",
      "Epoch 65/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3497\n",
      "Epoch 65: val_loss did not improve from 1.36530\n",
      "29601/29601 [==============================] - 24s 803us/sample - loss: 1.3497 - val_loss: 1.3721\n",
      "Epoch 66/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3452\n",
      "Epoch 66: val_loss did not improve from 1.36530\n",
      "29601/29601 [==============================] - 20s 678us/sample - loss: 1.3452 - val_loss: 1.3772\n",
      "Epoch 67/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3465\n",
      "Epoch 67: val_loss did not improve from 1.36530\n",
      "29601/29601 [==============================] - 20s 677us/sample - loss: 1.3465 - val_loss: 1.3664\n",
      "Epoch 68/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3451\n",
      "Epoch 68: val_loss did not improve from 1.36530\n",
      "29601/29601 [==============================] - 22s 757us/sample - loss: 1.3451 - val_loss: 1.3699\n",
      "Epoch 69/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3462\n",
      "Epoch 69: val_loss did not improve from 1.36530\n",
      "29601/29601 [==============================] - 23s 773us/sample - loss: 1.3462 - val_loss: 1.3742\n",
      "Epoch 70/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3456\n",
      "Epoch 70: val_loss did not improve from 1.36530\n",
      "29601/29601 [==============================] - 24s 805us/sample - loss: 1.3456 - val_loss: 1.3668\n",
      "Epoch 71/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3462\n",
      "Epoch 71: val_loss did not improve from 1.36530\n",
      "29601/29601 [==============================] - 22s 757us/sample - loss: 1.3462 - val_loss: 1.3739\n",
      "Epoch 72/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3435\n",
      "Epoch 72: val_loss did not improve from 1.36530\n",
      "29601/29601 [==============================] - 22s 741us/sample - loss: 1.3435 - val_loss: 1.3718\n",
      "Epoch 73/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3444\n",
      "Epoch 73: val_loss did not improve from 1.36530\n",
      "29601/29601 [==============================] - 23s 793us/sample - loss: 1.3444 - val_loss: 1.3761\n",
      "Epoch 74/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3449\n",
      "Epoch 74: val_loss improved from 1.36530 to 1.36399, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_1.h5\n",
      "29601/29601 [==============================] - 24s 799us/sample - loss: 1.3449 - val_loss: 1.3640\n",
      "Epoch 75/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3409\n",
      "Epoch 75: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 23s 792us/sample - loss: 1.3409 - val_loss: 1.3681\n",
      "Epoch 76/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3445\n",
      "Epoch 76: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 24s 801us/sample - loss: 1.3445 - val_loss: 1.3719\n",
      "Epoch 77/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3411\n",
      "Epoch 77: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 20s 676us/sample - loss: 1.3411 - val_loss: 1.3687\n",
      "Epoch 78/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3450\n",
      "Epoch 78: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 22s 745us/sample - loss: 1.3450 - val_loss: 1.3660\n",
      "Epoch 79/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3423\n",
      "Epoch 79: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 22s 738us/sample - loss: 1.3423 - val_loss: 1.3730\n",
      "Epoch 80/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3406\n",
      "Epoch 80: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 22s 755us/sample - loss: 1.3406 - val_loss: 1.3714\n",
      "Epoch 81/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3392\n",
      "Epoch 81: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 24s 801us/sample - loss: 1.3392 - val_loss: 1.3738\n",
      "Epoch 82/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3414\n",
      "Epoch 82: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 22s 758us/sample - loss: 1.3414 - val_loss: 1.3668\n",
      "Epoch 83/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3407\n",
      "Epoch 83: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 21s 721us/sample - loss: 1.3407 - val_loss: 1.3651\n",
      "Epoch 84/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3390\n",
      "Epoch 84: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 21s 697us/sample - loss: 1.3390 - val_loss: 1.3709\n",
      "Epoch 85/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3369\n",
      "Epoch 85: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 23s 793us/sample - loss: 1.3369 - val_loss: 1.3709\n",
      "Epoch 86/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3384\n",
      "Epoch 86: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 22s 737us/sample - loss: 1.3384 - val_loss: 1.3649\n",
      "Epoch 87/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3355\n",
      "Epoch 87: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 22s 751us/sample - loss: 1.3355 - val_loss: 1.3723\n",
      "Epoch 88/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3382\n",
      "Epoch 88: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 23s 773us/sample - loss: 1.3382 - val_loss: 1.3682\n",
      "Epoch 89/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3373\n",
      "Epoch 89: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 23s 780us/sample - loss: 1.3373 - val_loss: 1.3642\n",
      "Epoch 90/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3352\n",
      "Epoch 90: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 24s 804us/sample - loss: 1.3352 - val_loss: 1.3732\n",
      "Epoch 91/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3344\n",
      "Epoch 91: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 23s 786us/sample - loss: 1.3344 - val_loss: 1.3738\n",
      "Epoch 92/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3356\n",
      "Epoch 92: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 21s 716us/sample - loss: 1.3356 - val_loss: 1.3653\n",
      "Epoch 93/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3354\n",
      "Epoch 93: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 22s 757us/sample - loss: 1.3354 - val_loss: 1.3739\n",
      "Epoch 94/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3360\n",
      "Epoch 94: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 22s 760us/sample - loss: 1.3360 - val_loss: 1.3765\n",
      "Epoch 95/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3341\n",
      "Epoch 95: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 23s 771us/sample - loss: 1.3341 - val_loss: 1.3697\n",
      "Epoch 96/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3348\n",
      "Epoch 96: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 22s 741us/sample - loss: 1.3348 - val_loss: 1.3689\n",
      "Epoch 97/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3364\n",
      "Epoch 97: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 21s 693us/sample - loss: 1.3364 - val_loss: 1.3692\n",
      "Epoch 98/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3333\n",
      "Epoch 98: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 21s 709us/sample - loss: 1.3333 - val_loss: 1.3644\n",
      "Epoch 99/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3311\n",
      "Epoch 99: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 21s 699us/sample - loss: 1.3311 - val_loss: 1.3681\n",
      "Epoch 100/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3311\n",
      "Epoch 100: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 21s 695us/sample - loss: 1.3311 - val_loss: 1.3655\n",
      "Train on 29601 samples, validate on 3694 samples\n",
      "Epoch 1/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3301\n",
      "Epoch 1: val_loss improved from inf to 1.37051, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_2.h5\n",
      "29601/29601 [==============================] - 23s 765us/sample - loss: 1.3301 - val_loss: 1.3705\n",
      "Epoch 2/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3329\n",
      "Epoch 2: val_loss improved from 1.37051 to 1.36669, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_2.h5\n",
      "29601/29601 [==============================] - 24s 803us/sample - loss: 1.3329 - val_loss: 1.3667\n",
      "Epoch 3/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3307\n",
      "Epoch 3: val_loss did not improve from 1.36669\n",
      "29601/29601 [==============================] - 23s 767us/sample - loss: 1.3307 - val_loss: 1.3672\n",
      "Epoch 4/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3275\n",
      "Epoch 4: val_loss did not improve from 1.36669\n",
      "29601/29601 [==============================] - 21s 700us/sample - loss: 1.3275 - val_loss: 1.3681\n",
      "Epoch 5/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3296\n",
      "Epoch 5: val_loss improved from 1.36669 to 1.36560, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_2.h5\n",
      "29601/29601 [==============================] - 21s 707us/sample - loss: 1.3296 - val_loss: 1.3656\n",
      "Epoch 6/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3258\n",
      "Epoch 6: val_loss did not improve from 1.36560\n",
      "29601/29601 [==============================] - 23s 781us/sample - loss: 1.3258 - val_loss: 1.3748\n",
      "Epoch 7/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3312\n",
      "Epoch 7: val_loss did not improve from 1.36560\n",
      "29601/29601 [==============================] - 22s 759us/sample - loss: 1.3312 - val_loss: 1.3701\n",
      "Epoch 8/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3273\n",
      "Epoch 8: val_loss did not improve from 1.36560\n",
      "29601/29601 [==============================] - 24s 798us/sample - loss: 1.3273 - val_loss: 1.3695\n",
      "Epoch 9/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3301\n",
      "Epoch 9: val_loss did not improve from 1.36560\n",
      "29601/29601 [==============================] - 23s 768us/sample - loss: 1.3301 - val_loss: 1.3742\n",
      "Epoch 10/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3277\n",
      "Epoch 10: val_loss did not improve from 1.36560\n",
      "29601/29601 [==============================] - 22s 735us/sample - loss: 1.3277 - val_loss: 1.3705\n",
      "Epoch 11/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3295\n",
      "Epoch 11: val_loss did not improve from 1.36560\n",
      "29601/29601 [==============================] - 21s 719us/sample - loss: 1.3295 - val_loss: 1.3684\n",
      "Epoch 12/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3290\n",
      "Epoch 12: val_loss did not improve from 1.36560\n",
      "29601/29601 [==============================] - 23s 784us/sample - loss: 1.3290 - val_loss: 1.3706\n",
      "Epoch 13/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3300\n",
      "Epoch 13: val_loss did not improve from 1.36560\n",
      "29601/29601 [==============================] - 22s 749us/sample - loss: 1.3300 - val_loss: 1.3667\n",
      "Epoch 14/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3301\n",
      "Epoch 14: val_loss improved from 1.36560 to 1.36487, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_2.h5\n",
      "29601/29601 [==============================] - 24s 808us/sample - loss: 1.3301 - val_loss: 1.3649\n",
      "Epoch 15/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3292\n",
      "Epoch 15: val_loss did not improve from 1.36487\n",
      "29601/29601 [==============================] - 23s 790us/sample - loss: 1.3292 - val_loss: 1.3658\n",
      "Epoch 16/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3306\n",
      "Epoch 16: val_loss did not improve from 1.36487\n",
      "29601/29601 [==============================] - 24s 801us/sample - loss: 1.3306 - val_loss: 1.3685\n",
      "Epoch 17/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3270\n",
      "Epoch 17: val_loss improved from 1.36487 to 1.36432, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_2.h5\n",
      "29601/29601 [==============================] - 24s 813us/sample - loss: 1.3270 - val_loss: 1.3643\n",
      "Epoch 18/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3318\n",
      "Epoch 18: val_loss did not improve from 1.36432\n",
      "29601/29601 [==============================] - 22s 756us/sample - loss: 1.3318 - val_loss: 1.3653\n",
      "Epoch 19/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3273\n",
      "Epoch 19: val_loss did not improve from 1.36432\n",
      "29601/29601 [==============================] - 24s 808us/sample - loss: 1.3273 - val_loss: 1.3681\n",
      "Epoch 20/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3315\n",
      "Epoch 20: val_loss did not improve from 1.36432\n",
      "29601/29601 [==============================] - 22s 757us/sample - loss: 1.3315 - val_loss: 1.3679\n",
      "Epoch 21/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3304\n",
      "Epoch 21: val_loss did not improve from 1.36432\n",
      "29601/29601 [==============================] - 22s 756us/sample - loss: 1.3304 - val_loss: 1.3697\n",
      "Epoch 22/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3262\n",
      "Epoch 22: val_loss did not improve from 1.36432\n",
      "29601/29601 [==============================] - 23s 783us/sample - loss: 1.3262 - val_loss: 1.3669\n",
      "Epoch 23/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3282\n",
      "Epoch 23: val_loss did not improve from 1.36432\n",
      "29601/29601 [==============================] - 24s 803us/sample - loss: 1.3282 - val_loss: 1.3728\n",
      "Epoch 24/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3284\n",
      "Epoch 24: val_loss did not improve from 1.36432\n",
      "29601/29601 [==============================] - 25s 854us/sample - loss: 1.3284 - val_loss: 1.3751\n",
      "Epoch 25/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3253\n",
      "Epoch 25: val_loss did not improve from 1.36432\n",
      "29601/29601 [==============================] - 23s 774us/sample - loss: 1.3253 - val_loss: 1.3723\n",
      "Epoch 26/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3310\n",
      "Epoch 26: val_loss did not improve from 1.36432\n",
      "29601/29601 [==============================] - 24s 801us/sample - loss: 1.3310 - val_loss: 1.3655\n",
      "Epoch 27/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3268\n",
      "Epoch 27: val_loss did not improve from 1.36432\n",
      "29601/29601 [==============================] - 22s 739us/sample - loss: 1.3268 - val_loss: 1.3670\n",
      "Epoch 28/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3270\n",
      "Epoch 28: val_loss did not improve from 1.36432\n",
      "29601/29601 [==============================] - 24s 820us/sample - loss: 1.3270 - val_loss: 1.3717\n",
      "Epoch 29/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3272\n",
      "Epoch 29: val_loss did not improve from 1.36432\n",
      "29601/29601 [==============================] - 21s 713us/sample - loss: 1.3272 - val_loss: 1.3705\n",
      "Epoch 30/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3273\n",
      "Epoch 30: val_loss improved from 1.36432 to 1.36391, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_2.h5\n",
      "29601/29601 [==============================] - 23s 769us/sample - loss: 1.3273 - val_loss: 1.3639\n",
      "Epoch 31/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3249\n",
      "Epoch 31: val_loss did not improve from 1.36391\n",
      "29601/29601 [==============================] - 22s 758us/sample - loss: 1.3249 - val_loss: 1.3687\n",
      "Epoch 32/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3271\n",
      "Epoch 32: val_loss did not improve from 1.36391\n",
      "29601/29601 [==============================] - 24s 799us/sample - loss: 1.3271 - val_loss: 1.3651\n",
      "Epoch 33/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3262\n",
      "Epoch 33: val_loss did not improve from 1.36391\n",
      "29601/29601 [==============================] - 23s 773us/sample - loss: 1.3262 - val_loss: 1.3642\n",
      "Epoch 34/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3282\n",
      "Epoch 34: val_loss did not improve from 1.36391\n",
      "29601/29601 [==============================] - 22s 727us/sample - loss: 1.3282 - val_loss: 1.3684\n",
      "Epoch 35/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3513\n",
      "Epoch 35: val_loss did not improve from 1.36391\n",
      "29601/29601 [==============================] - 22s 757us/sample - loss: 1.3513 - val_loss: 1.3770\n",
      "Epoch 36/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3358\n",
      "Epoch 36: val_loss did not improve from 1.36391\n",
      "29601/29601 [==============================] - 21s 716us/sample - loss: 1.3358 - val_loss: 1.3665\n",
      "Epoch 37/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3298\n",
      "Epoch 37: val_loss did not improve from 1.36391\n",
      "29601/29601 [==============================] - 21s 707us/sample - loss: 1.3298 - val_loss: 1.3660\n",
      "Epoch 38/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3312\n",
      "Epoch 38: val_loss did not improve from 1.36391\n",
      "29601/29601 [==============================] - 23s 771us/sample - loss: 1.3312 - val_loss: 1.3652\n",
      "Epoch 39/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3305\n",
      "Epoch 39: val_loss did not improve from 1.36391\n",
      "29601/29601 [==============================] - 23s 782us/sample - loss: 1.3305 - val_loss: 1.3690\n",
      "Epoch 40/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3304\n",
      "Epoch 40: val_loss did not improve from 1.36391\n",
      "29601/29601 [==============================] - 23s 776us/sample - loss: 1.3304 - val_loss: 1.3659\n",
      "Epoch 41/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3304\n",
      "Epoch 41: val_loss did not improve from 1.36391\n",
      "29601/29601 [==============================] - 24s 816us/sample - loss: 1.3304 - val_loss: 1.3683\n",
      "Epoch 42/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3291\n",
      "Epoch 42: val_loss improved from 1.36391 to 1.36332, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_2.h5\n",
      "29601/29601 [==============================] - 24s 799us/sample - loss: 1.3291 - val_loss: 1.3633\n",
      "Epoch 43/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3304\n",
      "Epoch 43: val_loss did not improve from 1.36332\n",
      "29601/29601 [==============================] - 21s 715us/sample - loss: 1.3304 - val_loss: 1.3653\n",
      "Epoch 44/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3282\n",
      "Epoch 44: val_loss did not improve from 1.36332\n",
      "29601/29601 [==============================] - 20s 689us/sample - loss: 1.3282 - val_loss: 1.3681\n",
      "Epoch 45/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3306\n",
      "Epoch 45: val_loss did not improve from 1.36332\n",
      "29601/29601 [==============================] - 23s 764us/sample - loss: 1.3306 - val_loss: 1.3675\n",
      "Epoch 46/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3302\n",
      "Epoch 46: val_loss did not improve from 1.36332\n",
      "29601/29601 [==============================] - 23s 761us/sample - loss: 1.3302 - val_loss: 1.3678\n",
      "Epoch 47/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3298\n",
      "Epoch 47: val_loss did not improve from 1.36332\n",
      "29601/29601 [==============================] - 22s 733us/sample - loss: 1.3298 - val_loss: 1.3663\n",
      "Epoch 48/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3259\n",
      "Epoch 48: val_loss did not improve from 1.36332\n",
      "29601/29601 [==============================] - 19s 629us/sample - loss: 1.3259 - val_loss: 1.3646\n",
      "Epoch 49/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3260\n",
      "Epoch 49: val_loss did not improve from 1.36332\n",
      "29601/29601 [==============================] - 20s 688us/sample - loss: 1.3260 - val_loss: 1.3662\n",
      "Epoch 50/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3278\n",
      "Epoch 50: val_loss did not improve from 1.36332\n",
      "29601/29601 [==============================] - 24s 799us/sample - loss: 1.3278 - val_loss: 1.3656\n",
      "Epoch 51/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3287\n",
      "Epoch 51: val_loss did not improve from 1.36332\n",
      "29601/29601 [==============================] - 24s 816us/sample - loss: 1.3287 - val_loss: 1.3638\n",
      "Epoch 52/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3286\n",
      "Epoch 52: val_loss did not improve from 1.36332\n",
      "29601/29601 [==============================] - 21s 712us/sample - loss: 1.3286 - val_loss: 1.3660\n",
      "Epoch 53/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3258\n",
      "Epoch 53: val_loss did not improve from 1.36332\n",
      "29601/29601 [==============================] - 22s 754us/sample - loss: 1.3258 - val_loss: 1.3673\n",
      "Epoch 54/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3270\n",
      "Epoch 54: val_loss did not improve from 1.36332\n",
      "29601/29601 [==============================] - 22s 747us/sample - loss: 1.3270 - val_loss: 1.3656\n",
      "Epoch 55/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3229\n",
      "Epoch 55: val_loss did not improve from 1.36332\n",
      "29601/29601 [==============================] - 20s 681us/sample - loss: 1.3229 - val_loss: 1.3650\n",
      "Epoch 56/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3257\n",
      "Epoch 56: val_loss did not improve from 1.36332\n",
      "29601/29601 [==============================] - 20s 685us/sample - loss: 1.3257 - val_loss: 1.3654\n",
      "Epoch 57/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3223\n",
      "Epoch 57: val_loss improved from 1.36332 to 1.36272, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_2.h5\n",
      "29601/29601 [==============================] - 22s 727us/sample - loss: 1.3223 - val_loss: 1.3627\n",
      "Epoch 58/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3221\n",
      "Epoch 58: val_loss improved from 1.36272 to 1.36237, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_2.h5\n",
      "29601/29601 [==============================] - 23s 781us/sample - loss: 1.3221 - val_loss: 1.3624\n",
      "Epoch 59/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3228\n",
      "Epoch 59: val_loss did not improve from 1.36237\n",
      "29601/29601 [==============================] - 22s 743us/sample - loss: 1.3228 - val_loss: 1.3658\n",
      "Epoch 60/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3196\n",
      "Epoch 60: val_loss did not improve from 1.36237\n",
      "29601/29601 [==============================] - 20s 683us/sample - loss: 1.3196 - val_loss: 1.3644\n",
      "Epoch 61/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3232\n",
      "Epoch 61: val_loss did not improve from 1.36237\n",
      "29601/29601 [==============================] - 22s 758us/sample - loss: 1.3232 - val_loss: 1.3640\n",
      "Epoch 62/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3261\n",
      "Epoch 62: val_loss did not improve from 1.36237\n",
      "29601/29601 [==============================] - 20s 669us/sample - loss: 1.3261 - val_loss: 1.3639\n",
      "Epoch 63/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3213\n",
      "Epoch 63: val_loss improved from 1.36237 to 1.36161, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_2.h5\n",
      "29601/29601 [==============================] - 23s 784us/sample - loss: 1.3213 - val_loss: 1.3616\n",
      "Epoch 64/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3177\n",
      "Epoch 64: val_loss did not improve from 1.36161\n",
      "29601/29601 [==============================] - 22s 753us/sample - loss: 1.3177 - val_loss: 1.3625\n",
      "Epoch 65/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3191\n",
      "Epoch 65: val_loss did not improve from 1.36161\n",
      "29601/29601 [==============================] - 23s 763us/sample - loss: 1.3191 - val_loss: 1.3661\n",
      "Epoch 66/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3195\n",
      "Epoch 66: val_loss did not improve from 1.36161\n",
      "29601/29601 [==============================] - 22s 746us/sample - loss: 1.3195 - val_loss: 1.3663\n",
      "Epoch 67/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3207\n",
      "Epoch 67: val_loss did not improve from 1.36161\n",
      "29601/29601 [==============================] - 23s 783us/sample - loss: 1.3207 - val_loss: 1.3632\n",
      "Epoch 68/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3170\n",
      "Epoch 68: val_loss did not improve from 1.36161\n",
      "29601/29601 [==============================] - 21s 704us/sample - loss: 1.3170 - val_loss: 1.3651\n",
      "Epoch 69/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3172\n",
      "Epoch 69: val_loss did not improve from 1.36161\n",
      "29601/29601 [==============================] - 22s 757us/sample - loss: 1.3172 - val_loss: 1.3655\n",
      "Epoch 70/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3173\n",
      "Epoch 70: val_loss did not improve from 1.36161\n",
      "29601/29601 [==============================] - 24s 796us/sample - loss: 1.3173 - val_loss: 1.3629\n",
      "Epoch 71/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3205\n",
      "Epoch 71: val_loss did not improve from 1.36161\n",
      "29601/29601 [==============================] - 23s 784us/sample - loss: 1.3205 - val_loss: 1.3618\n",
      "Epoch 72/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3134\n",
      "Epoch 72: val_loss improved from 1.36161 to 1.36124, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_2.h5\n",
      "29601/29601 [==============================] - 21s 708us/sample - loss: 1.3134 - val_loss: 1.3612\n",
      "Epoch 73/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3143\n",
      "Epoch 73: val_loss improved from 1.36124 to 1.35937, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_2.h5\n",
      "29601/29601 [==============================] - 23s 767us/sample - loss: 1.3143 - val_loss: 1.3594\n",
      "Epoch 74/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3164\n",
      "Epoch 74: val_loss did not improve from 1.35937\n",
      "29601/29601 [==============================] - 24s 827us/sample - loss: 1.3164 - val_loss: 1.3601\n",
      "Epoch 75/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3148\n",
      "Epoch 75: val_loss did not improve from 1.35937\n",
      "29601/29601 [==============================] - 20s 692us/sample - loss: 1.3148 - val_loss: 1.3654\n",
      "Epoch 76/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3146\n",
      "Epoch 76: val_loss did not improve from 1.35937\n",
      "29601/29601 [==============================] - 21s 703us/sample - loss: 1.3146 - val_loss: 1.3595\n",
      "Epoch 77/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3182\n",
      "Epoch 77: val_loss did not improve from 1.35937\n",
      "29601/29601 [==============================] - 22s 748us/sample - loss: 1.3182 - val_loss: 1.3618\n",
      "Epoch 78/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3119\n",
      "Epoch 78: val_loss did not improve from 1.35937\n",
      "29601/29601 [==============================] - 25s 831us/sample - loss: 1.3119 - val_loss: 1.3630\n",
      "Epoch 79/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3152\n",
      "Epoch 79: val_loss improved from 1.35937 to 1.35854, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_2.h5\n",
      "29601/29601 [==============================] - 23s 786us/sample - loss: 1.3152 - val_loss: 1.3585\n",
      "Epoch 80/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3130\n",
      "Epoch 80: val_loss did not improve from 1.35854\n",
      "29601/29601 [==============================] - 22s 746us/sample - loss: 1.3130 - val_loss: 1.3622\n",
      "Epoch 81/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3113\n",
      "Epoch 81: val_loss did not improve from 1.35854\n",
      "29601/29601 [==============================] - 24s 805us/sample - loss: 1.3113 - val_loss: 1.3599\n",
      "Epoch 82/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3109\n",
      "Epoch 82: val_loss did not improve from 1.35854\n",
      "29601/29601 [==============================] - 24s 805us/sample - loss: 1.3109 - val_loss: 1.3599\n",
      "Epoch 83/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3133\n",
      "Epoch 83: val_loss did not improve from 1.35854\n",
      "29601/29601 [==============================] - 24s 804us/sample - loss: 1.3133 - val_loss: 1.3639\n",
      "Epoch 84/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3128\n",
      "Epoch 84: val_loss did not improve from 1.35854\n",
      "29601/29601 [==============================] - 22s 754us/sample - loss: 1.3128 - val_loss: 1.3641\n",
      "Epoch 85/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3099\n",
      "Epoch 85: val_loss did not improve from 1.35854\n",
      "29601/29601 [==============================] - 23s 771us/sample - loss: 1.3099 - val_loss: 1.3593\n",
      "Epoch 86/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3131\n",
      "Epoch 86: val_loss did not improve from 1.35854\n",
      "29601/29601 [==============================] - 23s 777us/sample - loss: 1.3131 - val_loss: 1.3629\n",
      "Epoch 87/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3159\n",
      "Epoch 87: val_loss did not improve from 1.35854\n",
      "29601/29601 [==============================] - 23s 786us/sample - loss: 1.3159 - val_loss: 1.3639\n",
      "Epoch 88/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3114\n",
      "Epoch 88: val_loss did not improve from 1.35854\n",
      "29601/29601 [==============================] - 23s 777us/sample - loss: 1.3114 - val_loss: 1.3612\n",
      "Epoch 89/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3128\n",
      "Epoch 89: val_loss did not improve from 1.35854\n",
      "29601/29601 [==============================] - 23s 764us/sample - loss: 1.3128 - val_loss: 1.3607\n",
      "Epoch 90/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3126\n",
      "Epoch 90: val_loss did not improve from 1.35854\n",
      "29601/29601 [==============================] - 22s 754us/sample - loss: 1.3126 - val_loss: 1.3682\n",
      "Epoch 91/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3110\n",
      "Epoch 91: val_loss did not improve from 1.35854\n",
      "29601/29601 [==============================] - 24s 798us/sample - loss: 1.3110 - val_loss: 1.3632\n",
      "Epoch 92/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3137\n",
      "Epoch 92: val_loss did not improve from 1.35854\n",
      "29601/29601 [==============================] - 24s 812us/sample - loss: 1.3137 - val_loss: 1.3604\n",
      "Epoch 93/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3101\n",
      "Epoch 93: val_loss did not improve from 1.35854\n",
      "29601/29601 [==============================] - 24s 805us/sample - loss: 1.3101 - val_loss: 1.3651\n",
      "Epoch 94/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3094\n",
      "Epoch 94: val_loss did not improve from 1.35854\n",
      "29601/29601 [==============================] - 22s 732us/sample - loss: 1.3094 - val_loss: 1.3611\n",
      "Epoch 95/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3069\n",
      "Epoch 95: val_loss did not improve from 1.35854\n",
      "29601/29601 [==============================] - 21s 722us/sample - loss: 1.3069 - val_loss: 1.3638\n",
      "Epoch 96/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3061\n",
      "Epoch 96: val_loss did not improve from 1.35854\n",
      "29601/29601 [==============================] - 24s 800us/sample - loss: 1.3061 - val_loss: 1.3612\n",
      "Epoch 97/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3093\n",
      "Epoch 97: val_loss did not improve from 1.35854\n",
      "29601/29601 [==============================] - 24s 811us/sample - loss: 1.3093 - val_loss: 1.3629\n",
      "Epoch 98/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3084\n",
      "Epoch 98: val_loss did not improve from 1.35854\n",
      "29601/29601 [==============================] - 25s 836us/sample - loss: 1.3084 - val_loss: 1.3638\n",
      "Epoch 99/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3067\n",
      "Epoch 99: val_loss did not improve from 1.35854\n",
      "29601/29601 [==============================] - 21s 693us/sample - loss: 1.3067 - val_loss: 1.3636\n",
      "Epoch 100/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3084\n",
      "Epoch 100: val_loss did not improve from 1.35854\n",
      "29601/29601 [==============================] - 20s 688us/sample - loss: 1.3084 - val_loss: 1.3596\n",
      "Train on 29601 samples, validate on 3694 samples\n",
      "Epoch 1/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3098\n",
      "Epoch 1: val_loss improved from inf to 1.37054, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_3.h5\n",
      "29601/29601 [==============================] - 23s 767us/sample - loss: 1.3098 - val_loss: 1.3705\n",
      "Epoch 2/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3103\n",
      "Epoch 2: val_loss improved from 1.37054 to 1.35940, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_3.h5\n",
      "29601/29601 [==============================] - 23s 776us/sample - loss: 1.3103 - val_loss: 1.3594\n",
      "Epoch 3/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3079\n",
      "Epoch 3: val_loss did not improve from 1.35940\n",
      "29601/29601 [==============================] - 22s 747us/sample - loss: 1.3079 - val_loss: 1.3647\n",
      "Epoch 4/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3051\n",
      "Epoch 4: val_loss did not improve from 1.35940\n",
      "29601/29601 [==============================] - 23s 769us/sample - loss: 1.3051 - val_loss: 1.3603\n",
      "Epoch 5/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3057\n",
      "Epoch 5: val_loss did not improve from 1.35940\n",
      "29601/29601 [==============================] - 23s 772us/sample - loss: 1.3057 - val_loss: 1.3628\n",
      "Epoch 6/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3067\n",
      "Epoch 6: val_loss improved from 1.35940 to 1.35697, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_3.h5\n",
      "29601/29601 [==============================] - 25s 835us/sample - loss: 1.3067 - val_loss: 1.3570\n",
      "Epoch 7/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3083\n",
      "Epoch 7: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 22s 740us/sample - loss: 1.3083 - val_loss: 1.3641\n",
      "Epoch 8/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3065\n",
      "Epoch 8: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 22s 748us/sample - loss: 1.3065 - val_loss: 1.3635\n",
      "Epoch 9/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3125\n",
      "Epoch 9: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 21s 709us/sample - loss: 1.3125 - val_loss: 1.3609\n",
      "Epoch 10/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3073\n",
      "Epoch 10: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 24s 821us/sample - loss: 1.3073 - val_loss: 1.3636\n",
      "Epoch 11/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3089\n",
      "Epoch 11: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 23s 776us/sample - loss: 1.3089 - val_loss: 1.3628\n",
      "Epoch 12/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3111\n",
      "Epoch 12: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 22s 744us/sample - loss: 1.3111 - val_loss: 1.3651\n",
      "Epoch 13/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3092\n",
      "Epoch 13: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 23s 778us/sample - loss: 1.3092 - val_loss: 1.3697\n",
      "Epoch 14/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3076\n",
      "Epoch 14: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 24s 802us/sample - loss: 1.3076 - val_loss: 1.3625\n",
      "Epoch 15/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3096\n",
      "Epoch 15: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 23s 782us/sample - loss: 1.3096 - val_loss: 1.3682\n",
      "Epoch 16/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3102\n",
      "Epoch 16: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 23s 769us/sample - loss: 1.3102 - val_loss: 1.3655\n",
      "Epoch 17/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3098\n",
      "Epoch 17: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 25s 834us/sample - loss: 1.3098 - val_loss: 1.3667\n",
      "Epoch 18/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3147\n",
      "Epoch 18: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 20s 691us/sample - loss: 1.3147 - val_loss: 1.3696\n",
      "Epoch 19/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3108\n",
      "Epoch 19: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 22s 744us/sample - loss: 1.3108 - val_loss: 1.3653\n",
      "Epoch 20/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3098\n",
      "Epoch 20: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 21s 723us/sample - loss: 1.3098 - val_loss: 1.3618\n",
      "Epoch 21/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3125\n",
      "Epoch 21: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 23s 764us/sample - loss: 1.3125 - val_loss: 1.3617\n",
      "Epoch 22/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3090\n",
      "Epoch 22: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 21s 716us/sample - loss: 1.3090 - val_loss: 1.3702\n",
      "Epoch 23/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3098\n",
      "Epoch 23: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 21s 699us/sample - loss: 1.3098 - val_loss: 1.3651\n",
      "Epoch 24/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3082\n",
      "Epoch 24: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 20s 671us/sample - loss: 1.3082 - val_loss: 1.3687\n",
      "Epoch 25/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3117\n",
      "Epoch 25: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 21s 724us/sample - loss: 1.3117 - val_loss: 1.3675\n",
      "Epoch 26/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3121\n",
      "Epoch 26: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 23s 781us/sample - loss: 1.3121 - val_loss: 1.3691\n",
      "Epoch 27/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3091\n",
      "Epoch 27: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 22s 746us/sample - loss: 1.3091 - val_loss: 1.3749\n",
      "Epoch 28/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3136\n",
      "Epoch 28: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 23s 786us/sample - loss: 1.3136 - val_loss: 1.3730\n",
      "Epoch 29/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3100\n",
      "Epoch 29: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 24s 801us/sample - loss: 1.3100 - val_loss: 1.3668\n",
      "Epoch 30/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3091\n",
      "Epoch 30: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 23s 771us/sample - loss: 1.3091 - val_loss: 1.3729\n",
      "Epoch 31/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3083\n",
      "Epoch 31: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 20s 687us/sample - loss: 1.3083 - val_loss: 1.3694\n",
      "Epoch 32/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3127\n",
      "Epoch 32: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 21s 719us/sample - loss: 1.3127 - val_loss: 1.3670\n",
      "Epoch 33/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3157\n",
      "Epoch 33: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 22s 728us/sample - loss: 1.3157 - val_loss: 1.3702\n",
      "Epoch 34/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3215\n",
      "Epoch 34: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 19s 631us/sample - loss: 1.3215 - val_loss: 1.3755\n",
      "Epoch 35/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3193\n",
      "Epoch 35: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 21s 713us/sample - loss: 1.3193 - val_loss: 1.3705\n",
      "Epoch 36/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3165\n",
      "Epoch 36: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 22s 736us/sample - loss: 1.3165 - val_loss: 1.3684\n",
      "Epoch 37/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3173\n",
      "Epoch 37: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 21s 706us/sample - loss: 1.3173 - val_loss: 1.3697\n",
      "Epoch 38/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3224\n",
      "Epoch 38: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 21s 714us/sample - loss: 1.3224 - val_loss: 1.3764\n",
      "Epoch 39/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3241\n",
      "Epoch 39: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 22s 750us/sample - loss: 1.3241 - val_loss: 1.3721\n",
      "Epoch 40/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3216\n",
      "Epoch 40: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 25s 829us/sample - loss: 1.3216 - val_loss: 1.3703\n",
      "Epoch 41/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3192\n",
      "Epoch 41: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 22s 758us/sample - loss: 1.3192 - val_loss: 1.3711\n",
      "Epoch 42/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3189\n",
      "Epoch 42: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 23s 775us/sample - loss: 1.3189 - val_loss: 1.3705\n",
      "Epoch 43/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3235\n",
      "Epoch 43: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 23s 787us/sample - loss: 1.3235 - val_loss: 1.3799\n",
      "Epoch 44/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3225\n",
      "Epoch 44: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 21s 716us/sample - loss: 1.3225 - val_loss: 1.3698\n",
      "Epoch 45/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3243\n",
      "Epoch 45: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 23s 787us/sample - loss: 1.3243 - val_loss: 1.3711\n",
      "Epoch 46/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3222\n",
      "Epoch 46: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 23s 788us/sample - loss: 1.3222 - val_loss: 1.3708\n",
      "Epoch 47/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3220\n",
      "Epoch 47: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 23s 766us/sample - loss: 1.3220 - val_loss: 1.3690\n",
      "Epoch 48/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3210\n",
      "Epoch 48: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 22s 752us/sample - loss: 1.3210 - val_loss: 1.3695\n",
      "Epoch 49/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3204\n",
      "Epoch 49: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 24s 820us/sample - loss: 1.3204 - val_loss: 1.3713\n",
      "Epoch 50/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3195\n",
      "Epoch 50: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 24s 818us/sample - loss: 1.3195 - val_loss: 1.3708\n",
      "Epoch 51/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3193\n",
      "Epoch 51: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 25s 831us/sample - loss: 1.3193 - val_loss: 1.3698\n",
      "Epoch 52/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3183\n",
      "Epoch 52: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 24s 794us/sample - loss: 1.3183 - val_loss: 1.3685\n",
      "Epoch 53/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3175\n",
      "Epoch 53: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 25s 832us/sample - loss: 1.3175 - val_loss: 1.3720\n",
      "Epoch 54/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3152\n",
      "Epoch 54: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 25s 829us/sample - loss: 1.3152 - val_loss: 1.3731\n",
      "Epoch 55/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3195\n",
      "Epoch 55: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 23s 766us/sample - loss: 1.3195 - val_loss: 1.3693\n",
      "Epoch 56/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3149\n",
      "Epoch 56: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 22s 755us/sample - loss: 1.3149 - val_loss: 1.3733\n",
      "Epoch 57/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3181\n",
      "Epoch 57: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 22s 732us/sample - loss: 1.3181 - val_loss: 1.3677\n",
      "Epoch 58/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3144\n",
      "Epoch 58: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 22s 727us/sample - loss: 1.3144 - val_loss: 1.3674\n",
      "Epoch 59/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3133\n",
      "Epoch 59: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 24s 809us/sample - loss: 1.3133 - val_loss: 1.3714\n",
      "Epoch 60/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3160\n",
      "Epoch 60: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 23s 762us/sample - loss: 1.3160 - val_loss: 1.3718\n",
      "Epoch 61/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3122\n",
      "Epoch 61: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 22s 727us/sample - loss: 1.3122 - val_loss: 1.3718\n",
      "Epoch 62/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3121\n",
      "Epoch 62: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 22s 736us/sample - loss: 1.3121 - val_loss: 1.3720\n",
      "Epoch 63/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3105\n",
      "Epoch 63: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 21s 706us/sample - loss: 1.3105 - val_loss: 1.3679\n",
      "Epoch 64/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3119\n",
      "Epoch 64: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 24s 812us/sample - loss: 1.3119 - val_loss: 1.3686\n",
      "Epoch 65/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3085\n",
      "Epoch 65: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 21s 695us/sample - loss: 1.3085 - val_loss: 1.3666\n",
      "Epoch 66/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3131\n",
      "Epoch 66: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 20s 665us/sample - loss: 1.3131 - val_loss: 1.3616\n",
      "Epoch 67/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3120\n",
      "Epoch 67: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 22s 735us/sample - loss: 1.3120 - val_loss: 1.3680\n",
      "Epoch 68/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3119\n",
      "Epoch 68: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 23s 784us/sample - loss: 1.3119 - val_loss: 1.3645\n",
      "Epoch 69/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3134\n",
      "Epoch 69: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 24s 826us/sample - loss: 1.3134 - val_loss: 1.3709\n",
      "Epoch 70/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3081\n",
      "Epoch 70: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 24s 806us/sample - loss: 1.3081 - val_loss: 1.3691\n",
      "Epoch 71/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3087\n",
      "Epoch 71: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 22s 743us/sample - loss: 1.3087 - val_loss: 1.3666\n",
      "Epoch 72/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3092\n",
      "Epoch 72: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 24s 806us/sample - loss: 1.3092 - val_loss: 1.3700\n",
      "Epoch 73/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3075\n",
      "Epoch 73: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 22s 728us/sample - loss: 1.3075 - val_loss: 1.3656\n",
      "Epoch 74/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3080\n",
      "Epoch 74: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 21s 716us/sample - loss: 1.3080 - val_loss: 1.3653\n",
      "Epoch 75/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3073\n",
      "Epoch 75: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 22s 741us/sample - loss: 1.3073 - val_loss: 1.3683\n",
      "Epoch 76/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3062\n",
      "Epoch 76: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 24s 806us/sample - loss: 1.3062 - val_loss: 1.3714\n",
      "Epoch 77/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3057\n",
      "Epoch 77: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 22s 746us/sample - loss: 1.3057 - val_loss: 1.3744\n",
      "Epoch 78/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3048\n",
      "Epoch 78: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 24s 820us/sample - loss: 1.3048 - val_loss: 1.3690\n",
      "Epoch 79/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3061\n",
      "Epoch 79: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 24s 795us/sample - loss: 1.3061 - val_loss: 1.3748\n",
      "Epoch 80/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3049\n",
      "Epoch 80: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 21s 721us/sample - loss: 1.3049 - val_loss: 1.3731\n",
      "Epoch 81/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3069\n",
      "Epoch 81: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 23s 773us/sample - loss: 1.3069 - val_loss: 1.3667\n",
      "Epoch 82/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3039\n",
      "Epoch 82: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 22s 757us/sample - loss: 1.3039 - val_loss: 1.3724\n",
      "Epoch 83/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3026\n",
      "Epoch 83: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 22s 736us/sample - loss: 1.3026 - val_loss: 1.3702\n",
      "Epoch 84/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3075\n",
      "Epoch 84: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 23s 766us/sample - loss: 1.3075 - val_loss: 1.3661\n",
      "Epoch 85/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3061\n",
      "Epoch 85: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 24s 801us/sample - loss: 1.3061 - val_loss: 1.3692\n",
      "Epoch 86/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3055\n",
      "Epoch 86: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 22s 749us/sample - loss: 1.3055 - val_loss: 1.3664\n",
      "Epoch 87/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3040\n",
      "Epoch 87: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 23s 770us/sample - loss: 1.3040 - val_loss: 1.3655\n",
      "Epoch 88/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3038\n",
      "Epoch 88: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 22s 748us/sample - loss: 1.3038 - val_loss: 1.3666\n",
      "Epoch 89/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3025\n",
      "Epoch 89: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 21s 718us/sample - loss: 1.3025 - val_loss: 1.3671\n",
      "Epoch 90/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3035\n",
      "Epoch 90: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 22s 744us/sample - loss: 1.3035 - val_loss: 1.3682\n",
      "Epoch 91/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3047\n",
      "Epoch 91: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 21s 714us/sample - loss: 1.3047 - val_loss: 1.3696\n",
      "Epoch 92/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3024\n",
      "Epoch 92: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 22s 758us/sample - loss: 1.3024 - val_loss: 1.3692\n",
      "Epoch 93/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3023\n",
      "Epoch 93: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 22s 745us/sample - loss: 1.3023 - val_loss: 1.3647\n",
      "Epoch 94/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3020\n",
      "Epoch 94: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 22s 743us/sample - loss: 1.3020 - val_loss: 1.3651\n",
      "Epoch 95/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3030\n",
      "Epoch 95: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 22s 760us/sample - loss: 1.3030 - val_loss: 1.3630\n",
      "Epoch 96/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2993\n",
      "Epoch 96: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 23s 767us/sample - loss: 1.2993 - val_loss: 1.3654\n",
      "Epoch 97/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3047\n",
      "Epoch 97: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 22s 742us/sample - loss: 1.3047 - val_loss: 1.3722\n",
      "Epoch 98/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2994\n",
      "Epoch 98: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 24s 805us/sample - loss: 1.2994 - val_loss: 1.3703\n",
      "Epoch 99/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3033\n",
      "Epoch 99: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 22s 742us/sample - loss: 1.3033 - val_loss: 1.3671\n",
      "Epoch 100/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3012\n",
      "Epoch 100: val_loss did not improve from 1.35697\n",
      "29601/29601 [==============================] - 23s 765us/sample - loss: 1.3012 - val_loss: 1.3661\n",
      "Train on 29601 samples, validate on 3694 samples\n",
      "Epoch 1/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2993\n",
      "Epoch 1: val_loss improved from inf to 1.36988, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_4.h5\n",
      "29601/29601 [==============================] - 23s 770us/sample - loss: 1.2993 - val_loss: 1.3699\n",
      "Epoch 2/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3017\n",
      "Epoch 2: val_loss did not improve from 1.36988\n",
      "29601/29601 [==============================] - 24s 797us/sample - loss: 1.3017 - val_loss: 1.3724\n",
      "Epoch 3/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2964\n",
      "Epoch 3: val_loss improved from 1.36988 to 1.36693, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_4.h5\n",
      "29601/29601 [==============================] - 22s 745us/sample - loss: 1.2964 - val_loss: 1.3669\n",
      "Epoch 4/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2987\n",
      "Epoch 4: val_loss did not improve from 1.36693\n",
      "29601/29601 [==============================] - 23s 788us/sample - loss: 1.2987 - val_loss: 1.3761\n",
      "Epoch 5/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2978\n",
      "Epoch 5: val_loss did not improve from 1.36693\n",
      "29601/29601 [==============================] - 20s 679us/sample - loss: 1.2978 - val_loss: 1.3675\n",
      "Epoch 6/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2978\n",
      "Epoch 6: val_loss did not improve from 1.36693\n",
      "29601/29601 [==============================] - 21s 725us/sample - loss: 1.2978 - val_loss: 1.3709\n",
      "Epoch 7/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2974\n",
      "Epoch 7: val_loss improved from 1.36693 to 1.36429, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_4.h5\n",
      "29601/29601 [==============================] - 23s 779us/sample - loss: 1.2974 - val_loss: 1.3643\n",
      "Epoch 8/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2979\n",
      "Epoch 8: val_loss improved from 1.36429 to 1.36360, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_4.h5\n",
      "29601/29601 [==============================] - 22s 759us/sample - loss: 1.2979 - val_loss: 1.3636\n",
      "Epoch 9/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2966\n",
      "Epoch 9: val_loss improved from 1.36360 to 1.36288, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_4.h5\n",
      "29601/29601 [==============================] - 22s 757us/sample - loss: 1.2966 - val_loss: 1.3629\n",
      "Epoch 10/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3020\n",
      "Epoch 10: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 24s 806us/sample - loss: 1.3020 - val_loss: 1.3710\n",
      "Epoch 11/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2998\n",
      "Epoch 11: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 23s 780us/sample - loss: 1.2998 - val_loss: 1.3728\n",
      "Epoch 12/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3000\n",
      "Epoch 12: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 24s 805us/sample - loss: 1.3000 - val_loss: 1.3683\n",
      "Epoch 13/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2983\n",
      "Epoch 13: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 22s 728us/sample - loss: 1.2983 - val_loss: 1.3690\n",
      "Epoch 14/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2994\n",
      "Epoch 14: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 23s 763us/sample - loss: 1.2994 - val_loss: 1.3683\n",
      "Epoch 15/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3062\n",
      "Epoch 15: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 23s 765us/sample - loss: 1.3062 - val_loss: 1.3677\n",
      "Epoch 16/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3041\n",
      "Epoch 16: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 24s 810us/sample - loss: 1.3041 - val_loss: 1.3693\n",
      "Epoch 17/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3036\n",
      "Epoch 17: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 22s 754us/sample - loss: 1.3036 - val_loss: 1.3674\n",
      "Epoch 18/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3045\n",
      "Epoch 18: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 23s 785us/sample - loss: 1.3045 - val_loss: 1.3685\n",
      "Epoch 19/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3035\n",
      "Epoch 19: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 21s 720us/sample - loss: 1.3035 - val_loss: 1.3701\n",
      "Epoch 20/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3021\n",
      "Epoch 20: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 22s 749us/sample - loss: 1.3021 - val_loss: 1.3727\n",
      "Epoch 21/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3027\n",
      "Epoch 21: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 23s 789us/sample - loss: 1.3027 - val_loss: 1.3740\n",
      "Epoch 22/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3118\n",
      "Epoch 22: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 21s 714us/sample - loss: 1.3118 - val_loss: 1.3704\n",
      "Epoch 23/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3059\n",
      "Epoch 23: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 23s 765us/sample - loss: 1.3059 - val_loss: 1.3715\n",
      "Epoch 24/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3052\n",
      "Epoch 24: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 22s 733us/sample - loss: 1.3052 - val_loss: 1.3732\n",
      "Epoch 25/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3040\n",
      "Epoch 25: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 23s 761us/sample - loss: 1.3040 - val_loss: 1.3720\n",
      "Epoch 26/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3117\n",
      "Epoch 26: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 24s 814us/sample - loss: 1.3117 - val_loss: 1.3737\n",
      "Epoch 27/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3098\n",
      "Epoch 27: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 23s 781us/sample - loss: 1.3098 - val_loss: 1.3745\n",
      "Epoch 28/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3074\n",
      "Epoch 28: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 24s 823us/sample - loss: 1.3074 - val_loss: 1.3720\n",
      "Epoch 29/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3103\n",
      "Epoch 29: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 23s 772us/sample - loss: 1.3103 - val_loss: 1.3773\n",
      "Epoch 30/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3098\n",
      "Epoch 30: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 22s 754us/sample - loss: 1.3098 - val_loss: 1.3732\n",
      "Epoch 31/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3133\n",
      "Epoch 31: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 25s 831us/sample - loss: 1.3133 - val_loss: 1.3784\n",
      "Epoch 32/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3227\n",
      "Epoch 32: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 23s 767us/sample - loss: 1.3227 - val_loss: 1.3752\n",
      "Epoch 33/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3144\n",
      "Epoch 33: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 22s 755us/sample - loss: 1.3144 - val_loss: 1.3748\n",
      "Epoch 34/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3106\n",
      "Epoch 34: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 23s 786us/sample - loss: 1.3106 - val_loss: 1.3760\n",
      "Epoch 35/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3102\n",
      "Epoch 35: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 24s 822us/sample - loss: 1.3102 - val_loss: 1.3771\n",
      "Epoch 36/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3107\n",
      "Epoch 36: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 24s 822us/sample - loss: 1.3107 - val_loss: 1.3694\n",
      "Epoch 37/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3072\n",
      "Epoch 37: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 24s 824us/sample - loss: 1.3072 - val_loss: 1.3686\n",
      "Epoch 38/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3106\n",
      "Epoch 38: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 24s 827us/sample - loss: 1.3106 - val_loss: 1.3732\n",
      "Epoch 39/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3122\n",
      "Epoch 39: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 24s 824us/sample - loss: 1.3122 - val_loss: 1.3717\n",
      "Epoch 40/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3121\n",
      "Epoch 40: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 24s 827us/sample - loss: 1.3121 - val_loss: 1.3732\n",
      "Epoch 41/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3194\n",
      "Epoch 41: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 24s 827us/sample - loss: 1.3194 - val_loss: 1.3733\n",
      "Epoch 42/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3115\n",
      "Epoch 42: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 25s 833us/sample - loss: 1.3115 - val_loss: 1.3738\n",
      "Epoch 43/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3123\n",
      "Epoch 43: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 24s 827us/sample - loss: 1.3123 - val_loss: 1.3808\n",
      "Epoch 44/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3611\n",
      "Epoch 44: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 25s 834us/sample - loss: 1.3611 - val_loss: 1.3838\n",
      "Epoch 45/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3311\n",
      "Epoch 45: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 25s 832us/sample - loss: 1.3311 - val_loss: 1.3818\n",
      "Epoch 46/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3260\n",
      "Epoch 46: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 25s 828us/sample - loss: 1.3260 - val_loss: 1.3817\n",
      "Epoch 47/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3273\n",
      "Epoch 47: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 25s 829us/sample - loss: 1.3273 - val_loss: 1.3731\n",
      "Epoch 48/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3200\n",
      "Epoch 48: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 24s 827us/sample - loss: 1.3200 - val_loss: 1.3810\n",
      "Epoch 49/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3249\n",
      "Epoch 49: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 24s 826us/sample - loss: 1.3249 - val_loss: 1.3747\n",
      "Epoch 50/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3296\n",
      "Epoch 50: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 24s 826us/sample - loss: 1.3296 - val_loss: 1.3767\n",
      "Epoch 51/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3284\n",
      "Epoch 51: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 25s 830us/sample - loss: 1.3284 - val_loss: 1.3752\n",
      "Epoch 52/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3296\n",
      "Epoch 52: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 25s 829us/sample - loss: 1.3296 - val_loss: 1.3749\n",
      "Epoch 53/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3246\n",
      "Epoch 53: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 24s 828us/sample - loss: 1.3246 - val_loss: 1.3734\n",
      "Epoch 54/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3245\n",
      "Epoch 54: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 24s 828us/sample - loss: 1.3245 - val_loss: 1.3729\n",
      "Epoch 55/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3209\n",
      "Epoch 55: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 24s 825us/sample - loss: 1.3209 - val_loss: 1.3712\n",
      "Epoch 56/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3224\n",
      "Epoch 56: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 25s 828us/sample - loss: 1.3224 - val_loss: 1.3715\n",
      "Epoch 57/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3179\n",
      "Epoch 57: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 24s 827us/sample - loss: 1.3179 - val_loss: 1.3685\n",
      "Epoch 58/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3196\n",
      "Epoch 58: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 25s 837us/sample - loss: 1.3196 - val_loss: 1.3700\n",
      "Epoch 59/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3200\n",
      "Epoch 59: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 24s 821us/sample - loss: 1.3200 - val_loss: 1.3697\n",
      "Epoch 60/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3196\n",
      "Epoch 60: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 25s 832us/sample - loss: 1.3196 - val_loss: 1.3719\n",
      "Epoch 61/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3244\n",
      "Epoch 61: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 24s 826us/sample - loss: 1.3244 - val_loss: 1.3726\n",
      "Epoch 62/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3190\n",
      "Epoch 62: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 24s 826us/sample - loss: 1.3190 - val_loss: 1.3674\n",
      "Epoch 63/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3179\n",
      "Epoch 63: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 25s 829us/sample - loss: 1.3179 - val_loss: 1.3629\n",
      "Epoch 64/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3165\n",
      "Epoch 64: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 25s 829us/sample - loss: 1.3165 - val_loss: 1.3718\n",
      "Epoch 65/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3170\n",
      "Epoch 65: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 24s 822us/sample - loss: 1.3170 - val_loss: 1.3712\n",
      "Epoch 66/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3172\n",
      "Epoch 66: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 24s 826us/sample - loss: 1.3172 - val_loss: 1.3666\n",
      "Epoch 67/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3133\n",
      "Epoch 67: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 24s 828us/sample - loss: 1.3133 - val_loss: 1.3690\n",
      "Epoch 68/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3110\n",
      "Epoch 68: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 25s 830us/sample - loss: 1.3110 - val_loss: 1.3660\n",
      "Epoch 69/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3118\n",
      "Epoch 69: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 24s 821us/sample - loss: 1.3118 - val_loss: 1.3697\n",
      "Epoch 70/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3140\n",
      "Epoch 70: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 25s 831us/sample - loss: 1.3140 - val_loss: 1.3654\n",
      "Epoch 71/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3098\n",
      "Epoch 71: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 25s 833us/sample - loss: 1.3098 - val_loss: 1.3695\n",
      "Epoch 72/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3102\n",
      "Epoch 72: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 25s 828us/sample - loss: 1.3102 - val_loss: 1.3712\n",
      "Epoch 73/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3095\n",
      "Epoch 73: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 24s 825us/sample - loss: 1.3095 - val_loss: 1.3677\n",
      "Epoch 74/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3107\n",
      "Epoch 74: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 24s 823us/sample - loss: 1.3107 - val_loss: 1.3664\n",
      "Epoch 75/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3096\n",
      "Epoch 75: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 25s 833us/sample - loss: 1.3096 - val_loss: 1.3679\n",
      "Epoch 76/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3050\n",
      "Epoch 76: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 25s 831us/sample - loss: 1.3050 - val_loss: 1.3659\n",
      "Epoch 77/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3086\n",
      "Epoch 77: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 24s 828us/sample - loss: 1.3086 - val_loss: 1.3629\n",
      "Epoch 78/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3069\n",
      "Epoch 78: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 25s 834us/sample - loss: 1.3069 - val_loss: 1.3669\n",
      "Epoch 79/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3080\n",
      "Epoch 79: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 24s 826us/sample - loss: 1.3080 - val_loss: 1.3661\n",
      "Epoch 80/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3021\n",
      "Epoch 80: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 24s 825us/sample - loss: 1.3021 - val_loss: 1.3655\n",
      "Epoch 81/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3054\n",
      "Epoch 81: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 25s 834us/sample - loss: 1.3054 - val_loss: 1.3721\n",
      "Epoch 82/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3061\n",
      "Epoch 82: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 25s 831us/sample - loss: 1.3061 - val_loss: 1.3677\n",
      "Epoch 83/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3054\n",
      "Epoch 83: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 25s 829us/sample - loss: 1.3054 - val_loss: 1.3653\n",
      "Epoch 84/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3051\n",
      "Epoch 84: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 24s 827us/sample - loss: 1.3051 - val_loss: 1.3639\n",
      "Epoch 85/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3034\n",
      "Epoch 85: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 24s 827us/sample - loss: 1.3034 - val_loss: 1.3674\n",
      "Epoch 86/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3029\n",
      "Epoch 86: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 24s 827us/sample - loss: 1.3029 - val_loss: 1.3659\n",
      "Epoch 87/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3013\n",
      "Epoch 87: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 25s 833us/sample - loss: 1.3013 - val_loss: 1.3640\n",
      "Epoch 88/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3024\n",
      "Epoch 88: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 25s 829us/sample - loss: 1.3024 - val_loss: 1.3656\n",
      "Epoch 89/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3014\n",
      "Epoch 89: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 25s 829us/sample - loss: 1.3014 - val_loss: 1.3653\n",
      "Epoch 90/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3013\n",
      "Epoch 90: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 24s 812us/sample - loss: 1.3013 - val_loss: 1.3676\n",
      "Epoch 91/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3004\n",
      "Epoch 91: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 24s 819us/sample - loss: 1.3004 - val_loss: 1.3698\n",
      "Epoch 92/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3035\n",
      "Epoch 92: val_loss did not improve from 1.36288\n",
      "29601/29601 [==============================] - 24s 820us/sample - loss: 1.3035 - val_loss: 1.3681\n",
      "Epoch 93/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3003\n",
      "Epoch 93: val_loss improved from 1.36288 to 1.36153, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_4.h5\n",
      "29601/29601 [==============================] - 24s 819us/sample - loss: 1.3003 - val_loss: 1.3615\n",
      "Epoch 94/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3006\n",
      "Epoch 94: val_loss did not improve from 1.36153\n",
      "29601/29601 [==============================] - 24s 805us/sample - loss: 1.3006 - val_loss: 1.3660\n",
      "Epoch 95/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2994\n",
      "Epoch 95: val_loss did not improve from 1.36153\n",
      "29601/29601 [==============================] - 23s 765us/sample - loss: 1.2994 - val_loss: 1.3665\n",
      "Epoch 96/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3003\n",
      "Epoch 96: val_loss did not improve from 1.36153\n",
      "29601/29601 [==============================] - 22s 758us/sample - loss: 1.3003 - val_loss: 1.3717\n",
      "Epoch 97/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3002\n",
      "Epoch 97: val_loss did not improve from 1.36153\n",
      "29601/29601 [==============================] - 24s 800us/sample - loss: 1.3002 - val_loss: 1.3663\n",
      "Epoch 98/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3016\n",
      "Epoch 98: val_loss did not improve from 1.36153\n",
      "29601/29601 [==============================] - 23s 792us/sample - loss: 1.3016 - val_loss: 1.3661\n",
      "Epoch 99/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3026\n",
      "Epoch 99: val_loss did not improve from 1.36153\n",
      "29601/29601 [==============================] - 23s 793us/sample - loss: 1.3026 - val_loss: 1.3673\n",
      "Epoch 100/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2983\n",
      "Epoch 100: val_loss did not improve from 1.36153\n",
      "29601/29601 [==============================] - 24s 798us/sample - loss: 1.2983 - val_loss: 1.3645\n",
      "Train on 29601 samples, validate on 3694 samples\n",
      "Epoch 1/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3001\n",
      "Epoch 1: val_loss improved from inf to 1.36411, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_5.h5\n",
      "29601/29601 [==============================] - 24s 802us/sample - loss: 1.3001 - val_loss: 1.3641\n",
      "Epoch 2/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2971\n",
      "Epoch 2: val_loss improved from 1.36411 to 1.36354, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_5.h5\n",
      "29601/29601 [==============================] - 24s 825us/sample - loss: 1.2971 - val_loss: 1.3635\n",
      "Epoch 3/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2963\n",
      "Epoch 3: val_loss did not improve from 1.36354\n",
      "29601/29601 [==============================] - 24s 818us/sample - loss: 1.2963 - val_loss: 1.3676\n",
      "Epoch 4/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2966\n",
      "Epoch 4: val_loss did not improve from 1.36354\n",
      "29601/29601 [==============================] - 24s 821us/sample - loss: 1.2966 - val_loss: 1.3729\n",
      "Epoch 5/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2963\n",
      "Epoch 5: val_loss improved from 1.36354 to 1.36235, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_5.h5\n",
      "29601/29601 [==============================] - 24s 820us/sample - loss: 1.2963 - val_loss: 1.3623\n",
      "Epoch 6/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2910\n",
      "Epoch 6: val_loss did not improve from 1.36235\n",
      "29601/29601 [==============================] - 24s 814us/sample - loss: 1.2910 - val_loss: 1.3648\n",
      "Epoch 7/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2950\n",
      "Epoch 7: val_loss did not improve from 1.36235\n",
      "29601/29601 [==============================] - 24s 819us/sample - loss: 1.2950 - val_loss: 1.3650\n",
      "Epoch 8/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2942\n",
      "Epoch 8: val_loss did not improve from 1.36235\n",
      "29601/29601 [==============================] - 21s 703us/sample - loss: 1.2942 - val_loss: 1.3653\n",
      "Epoch 9/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3053\n",
      "Epoch 9: val_loss improved from 1.36235 to 1.36020, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_5.h5\n",
      "29601/29601 [==============================] - 22s 729us/sample - loss: 1.3053 - val_loss: 1.3602\n",
      "Epoch 10/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3001\n",
      "Epoch 10: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 740us/sample - loss: 1.3001 - val_loss: 1.3689\n",
      "Epoch 11/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3208\n",
      "Epoch 11: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 744us/sample - loss: 1.3208 - val_loss: 1.3728\n",
      "Epoch 12/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3036\n",
      "Epoch 12: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 750us/sample - loss: 1.3036 - val_loss: 1.3671\n",
      "Epoch 13/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2988\n",
      "Epoch 13: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 740us/sample - loss: 1.2988 - val_loss: 1.3685\n",
      "Epoch 14/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2980\n",
      "Epoch 14: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 731us/sample - loss: 1.2980 - val_loss: 1.3703\n",
      "Epoch 15/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2979\n",
      "Epoch 15: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 739us/sample - loss: 1.2979 - val_loss: 1.3659\n",
      "Epoch 16/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3236\n",
      "Epoch 16: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 738us/sample - loss: 1.3236 - val_loss: 1.3729\n",
      "Epoch 17/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3068\n",
      "Epoch 17: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 23s 772us/sample - loss: 1.3068 - val_loss: 1.3689\n",
      "Epoch 18/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3032\n",
      "Epoch 18: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 745us/sample - loss: 1.3032 - val_loss: 1.3663\n",
      "Epoch 19/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3007\n",
      "Epoch 19: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 752us/sample - loss: 1.3007 - val_loss: 1.3690\n",
      "Epoch 20/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3007\n",
      "Epoch 20: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 21s 701us/sample - loss: 1.3007 - val_loss: 1.3736\n",
      "Epoch 21/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3008\n",
      "Epoch 21: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 20s 683us/sample - loss: 1.3008 - val_loss: 1.3668\n",
      "Epoch 22/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3012\n",
      "Epoch 22: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 21s 712us/sample - loss: 1.3012 - val_loss: 1.3682\n",
      "Epoch 23/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3010\n",
      "Epoch 23: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 732us/sample - loss: 1.3010 - val_loss: 1.3694\n",
      "Epoch 24/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3039\n",
      "Epoch 24: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 21s 703us/sample - loss: 1.3039 - val_loss: 1.3729\n",
      "Epoch 25/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3028\n",
      "Epoch 25: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 21s 718us/sample - loss: 1.3028 - val_loss: 1.3640\n",
      "Epoch 26/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3044\n",
      "Epoch 26: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 23s 762us/sample - loss: 1.3044 - val_loss: 1.3717\n",
      "Epoch 27/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3022\n",
      "Epoch 27: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 745us/sample - loss: 1.3022 - val_loss: 1.3666\n",
      "Epoch 28/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3029\n",
      "Epoch 28: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 23s 768us/sample - loss: 1.3029 - val_loss: 1.3707\n",
      "Epoch 29/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3046\n",
      "Epoch 29: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 23s 763us/sample - loss: 1.3046 - val_loss: 1.3705\n",
      "Epoch 30/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3040\n",
      "Epoch 30: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 23s 761us/sample - loss: 1.3040 - val_loss: 1.3725\n",
      "Epoch 31/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3053\n",
      "Epoch 31: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 744us/sample - loss: 1.3053 - val_loss: 1.3707\n",
      "Epoch 32/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3100\n",
      "Epoch 32: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 732us/sample - loss: 1.3100 - val_loss: 1.3740\n",
      "Epoch 33/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3231\n",
      "Epoch 33: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 749us/sample - loss: 1.3231 - val_loss: 1.3758\n",
      "Epoch 34/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3129\n",
      "Epoch 34: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 23s 782us/sample - loss: 1.3129 - val_loss: 1.3737\n",
      "Epoch 35/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3112\n",
      "Epoch 35: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 751us/sample - loss: 1.3112 - val_loss: 1.3713\n",
      "Epoch 36/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3123\n",
      "Epoch 36: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 727us/sample - loss: 1.3123 - val_loss: 1.3746\n",
      "Epoch 37/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3134\n",
      "Epoch 37: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 738us/sample - loss: 1.3134 - val_loss: 1.3733\n",
      "Epoch 38/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3147\n",
      "Epoch 38: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 21s 723us/sample - loss: 1.3147 - val_loss: 1.3762\n",
      "Epoch 39/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3155\n",
      "Epoch 39: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 732us/sample - loss: 1.3155 - val_loss: 1.3733\n",
      "Epoch 40/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3153\n",
      "Epoch 40: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 23s 763us/sample - loss: 1.3153 - val_loss: 1.3699\n",
      "Epoch 41/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3148\n",
      "Epoch 41: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 23s 778us/sample - loss: 1.3148 - val_loss: 1.3714\n",
      "Epoch 42/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3143\n",
      "Epoch 42: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 727us/sample - loss: 1.3143 - val_loss: 1.3742\n",
      "Epoch 43/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3161\n",
      "Epoch 43: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 751us/sample - loss: 1.3161 - val_loss: 1.3699\n",
      "Epoch 44/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3179\n",
      "Epoch 44: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 727us/sample - loss: 1.3179 - val_loss: 1.3727\n",
      "Epoch 45/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3190\n",
      "Epoch 45: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 733us/sample - loss: 1.3190 - val_loss: 1.3724\n",
      "Epoch 46/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3155\n",
      "Epoch 46: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 753us/sample - loss: 1.3155 - val_loss: 1.3741\n",
      "Epoch 47/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3201\n",
      "Epoch 47: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 748us/sample - loss: 1.3201 - val_loss: 1.3817\n",
      "Epoch 48/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3206\n",
      "Epoch 48: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 21s 709us/sample - loss: 1.3206 - val_loss: 1.3775\n",
      "Epoch 49/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3201\n",
      "Epoch 49: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 21s 725us/sample - loss: 1.3201 - val_loss: 1.3786\n",
      "Epoch 50/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3477\n",
      "Epoch 50: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 742us/sample - loss: 1.3477 - val_loss: 1.3786\n",
      "Epoch 51/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3285\n",
      "Epoch 51: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 746us/sample - loss: 1.3285 - val_loss: 1.3780\n",
      "Epoch 52/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3276\n",
      "Epoch 52: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 21s 710us/sample - loss: 1.3276 - val_loss: 1.3748\n",
      "Epoch 53/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3216\n",
      "Epoch 53: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 747us/sample - loss: 1.3216 - val_loss: 1.3766\n",
      "Epoch 54/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3206\n",
      "Epoch 54: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 747us/sample - loss: 1.3206 - val_loss: 1.3773\n",
      "Epoch 55/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3212\n",
      "Epoch 55: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 23s 766us/sample - loss: 1.3212 - val_loss: 1.3782\n",
      "Epoch 56/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3164\n",
      "Epoch 56: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 740us/sample - loss: 1.3164 - val_loss: 1.3768\n",
      "Epoch 57/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3164\n",
      "Epoch 57: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 756us/sample - loss: 1.3164 - val_loss: 1.3797\n",
      "Epoch 58/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3190\n",
      "Epoch 58: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 23s 774us/sample - loss: 1.3190 - val_loss: 1.3730\n",
      "Epoch 59/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3143\n",
      "Epoch 59: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 741us/sample - loss: 1.3143 - val_loss: 1.3716\n",
      "Epoch 60/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3112\n",
      "Epoch 60: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 21s 708us/sample - loss: 1.3112 - val_loss: 1.3722\n",
      "Epoch 61/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3145\n",
      "Epoch 61: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 735us/sample - loss: 1.3145 - val_loss: 1.3704\n",
      "Epoch 62/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3138\n",
      "Epoch 62: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 745us/sample - loss: 1.3138 - val_loss: 1.3734\n",
      "Epoch 63/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3107\n",
      "Epoch 63: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 23s 765us/sample - loss: 1.3107 - val_loss: 1.3745\n",
      "Epoch 64/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3121\n",
      "Epoch 64: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 755us/sample - loss: 1.3121 - val_loss: 1.3700\n",
      "Epoch 65/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3090\n",
      "Epoch 65: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 746us/sample - loss: 1.3090 - val_loss: 1.3730\n",
      "Epoch 66/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3096\n",
      "Epoch 66: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 742us/sample - loss: 1.3096 - val_loss: 1.3770\n",
      "Epoch 67/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3080\n",
      "Epoch 67: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 753us/sample - loss: 1.3080 - val_loss: 1.3709\n",
      "Epoch 68/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3146\n",
      "Epoch 68: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 758us/sample - loss: 1.3146 - val_loss: 1.3734\n",
      "Epoch 69/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3110\n",
      "Epoch 69: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 746us/sample - loss: 1.3110 - val_loss: 1.3715\n",
      "Epoch 70/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3091\n",
      "Epoch 70: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 21s 718us/sample - loss: 1.3091 - val_loss: 1.3706\n",
      "Epoch 71/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3104\n",
      "Epoch 71: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 740us/sample - loss: 1.3104 - val_loss: 1.3711\n",
      "Epoch 72/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3068\n",
      "Epoch 72: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 745us/sample - loss: 1.3068 - val_loss: 1.3684\n",
      "Epoch 73/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3058\n",
      "Epoch 73: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 752us/sample - loss: 1.3058 - val_loss: 1.3677\n",
      "Epoch 74/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3049\n",
      "Epoch 74: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 742us/sample - loss: 1.3049 - val_loss: 1.3753\n",
      "Epoch 75/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3065\n",
      "Epoch 75: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 756us/sample - loss: 1.3065 - val_loss: 1.3692\n",
      "Epoch 76/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3029\n",
      "Epoch 76: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 750us/sample - loss: 1.3029 - val_loss: 1.3709\n",
      "Epoch 77/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3062\n",
      "Epoch 77: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 736us/sample - loss: 1.3062 - val_loss: 1.3703\n",
      "Epoch 78/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3041\n",
      "Epoch 78: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 760us/sample - loss: 1.3041 - val_loss: 1.3703\n",
      "Epoch 79/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3038\n",
      "Epoch 79: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 736us/sample - loss: 1.3038 - val_loss: 1.3657\n",
      "Epoch 80/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3012\n",
      "Epoch 80: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 757us/sample - loss: 1.3012 - val_loss: 1.3702\n",
      "Epoch 81/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3009\n",
      "Epoch 81: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 736us/sample - loss: 1.3009 - val_loss: 1.3718\n",
      "Epoch 82/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3036\n",
      "Epoch 82: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 754us/sample - loss: 1.3036 - val_loss: 1.3780\n",
      "Epoch 83/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3020\n",
      "Epoch 83: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 740us/sample - loss: 1.3020 - val_loss: 1.3682\n",
      "Epoch 84/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3006\n",
      "Epoch 84: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 737us/sample - loss: 1.3006 - val_loss: 1.3698\n",
      "Epoch 85/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3014\n",
      "Epoch 85: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 759us/sample - loss: 1.3014 - val_loss: 1.3684\n",
      "Epoch 86/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3019\n",
      "Epoch 86: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 740us/sample - loss: 1.3019 - val_loss: 1.3754\n",
      "Epoch 87/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3022\n",
      "Epoch 87: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 742us/sample - loss: 1.3022 - val_loss: 1.3691\n",
      "Epoch 88/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3020\n",
      "Epoch 88: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 754us/sample - loss: 1.3020 - val_loss: 1.3666\n",
      "Epoch 89/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3011\n",
      "Epoch 89: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 759us/sample - loss: 1.3011 - val_loss: 1.3698\n",
      "Epoch 90/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2986\n",
      "Epoch 90: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 746us/sample - loss: 1.2986 - val_loss: 1.3721\n",
      "Epoch 91/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2968\n",
      "Epoch 91: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 753us/sample - loss: 1.2968 - val_loss: 1.3700\n",
      "Epoch 92/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2972\n",
      "Epoch 92: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 756us/sample - loss: 1.2972 - val_loss: 1.3750\n",
      "Epoch 93/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3008\n",
      "Epoch 93: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 743us/sample - loss: 1.3008 - val_loss: 1.3707\n",
      "Epoch 94/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2971\n",
      "Epoch 94: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 21s 721us/sample - loss: 1.2971 - val_loss: 1.3716\n",
      "Epoch 95/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2987\n",
      "Epoch 95: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 740us/sample - loss: 1.2987 - val_loss: 1.3702\n",
      "Epoch 96/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3028\n",
      "Epoch 96: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 738us/sample - loss: 1.3028 - val_loss: 1.3729\n",
      "Epoch 97/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2964\n",
      "Epoch 97: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 743us/sample - loss: 1.2964 - val_loss: 1.3683\n",
      "Epoch 98/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2944\n",
      "Epoch 98: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 748us/sample - loss: 1.2944 - val_loss: 1.3698\n",
      "Epoch 99/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2961\n",
      "Epoch 99: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 759us/sample - loss: 1.2961 - val_loss: 1.3658\n",
      "Epoch 100/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2942\n",
      "Epoch 100: val_loss did not improve from 1.36020\n",
      "29601/29601 [==============================] - 22s 754us/sample - loss: 1.2942 - val_loss: 1.3704\n",
      "Train on 29601 samples, validate on 3694 samples\n",
      "Epoch 1/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2982\n",
      "Epoch 1: val_loss improved from inf to 1.37097, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_6.h5\n",
      "29601/29601 [==============================] - 22s 747us/sample - loss: 1.2982 - val_loss: 1.3710\n",
      "Epoch 2/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2920\n",
      "Epoch 2: val_loss improved from 1.37097 to 1.36672, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_6.h5\n",
      "29601/29601 [==============================] - 22s 743us/sample - loss: 1.2920 - val_loss: 1.3667\n",
      "Epoch 3/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2925\n",
      "Epoch 3: val_loss did not improve from 1.36672\n",
      "29601/29601 [==============================] - 22s 746us/sample - loss: 1.2925 - val_loss: 1.3718\n",
      "Epoch 4/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2943\n",
      "Epoch 4: val_loss improved from 1.36672 to 1.36636, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_6.h5\n",
      "29601/29601 [==============================] - 22s 754us/sample - loss: 1.2943 - val_loss: 1.3664\n",
      "Epoch 5/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2896\n",
      "Epoch 5: val_loss did not improve from 1.36636\n",
      "29601/29601 [==============================] - 22s 760us/sample - loss: 1.2896 - val_loss: 1.3674\n",
      "Epoch 6/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2888\n",
      "Epoch 6: val_loss did not improve from 1.36636\n",
      "29601/29601 [==============================] - 22s 743us/sample - loss: 1.2888 - val_loss: 1.3708\n",
      "Epoch 7/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2919\n",
      "Epoch 7: val_loss improved from 1.36636 to 1.36567, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_6.h5\n",
      "29601/29601 [==============================] - 22s 752us/sample - loss: 1.2919 - val_loss: 1.3657\n",
      "Epoch 8/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2938\n",
      "Epoch 8: val_loss did not improve from 1.36567\n",
      "29601/29601 [==============================] - 23s 764us/sample - loss: 1.2938 - val_loss: 1.3660\n",
      "Epoch 9/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2937\n",
      "Epoch 9: val_loss did not improve from 1.36567\n",
      "29601/29601 [==============================] - 23s 762us/sample - loss: 1.2937 - val_loss: 1.3729\n",
      "Epoch 10/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3113\n",
      "Epoch 10: val_loss did not improve from 1.36567\n",
      "29601/29601 [==============================] - 21s 723us/sample - loss: 1.3113 - val_loss: 1.3766\n",
      "Epoch 11/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2978\n",
      "Epoch 11: val_loss did not improve from 1.36567\n",
      "29601/29601 [==============================] - 24s 827us/sample - loss: 1.2978 - val_loss: 1.3770\n",
      "Epoch 12/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2969\n",
      "Epoch 12: val_loss did not improve from 1.36567\n",
      "29601/29601 [==============================] - 25s 829us/sample - loss: 1.2969 - val_loss: 1.3681\n",
      "Epoch 13/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2941\n",
      "Epoch 13: val_loss did not improve from 1.36567\n",
      "29601/29601 [==============================] - 25s 830us/sample - loss: 1.2941 - val_loss: 1.3706\n",
      "Epoch 14/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2948\n",
      "Epoch 14: val_loss did not improve from 1.36567\n",
      "29601/29601 [==============================] - 24s 827us/sample - loss: 1.2948 - val_loss: 1.3717\n",
      "Epoch 15/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2952\n",
      "Epoch 15: val_loss did not improve from 1.36567\n",
      "29601/29601 [==============================] - 25s 828us/sample - loss: 1.2952 - val_loss: 1.3716\n",
      "Epoch 16/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2946\n",
      "Epoch 16: val_loss did not improve from 1.36567\n",
      "29601/29601 [==============================] - 24s 826us/sample - loss: 1.2946 - val_loss: 1.3732\n",
      "Epoch 17/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2975\n",
      "Epoch 17: val_loss did not improve from 1.36567\n",
      "29601/29601 [==============================] - 24s 820us/sample - loss: 1.2975 - val_loss: 1.3669\n",
      "Epoch 18/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2951\n",
      "Epoch 18: val_loss did not improve from 1.36567\n",
      "29601/29601 [==============================] - 24s 826us/sample - loss: 1.2951 - val_loss: 1.3716\n",
      "Epoch 19/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2965\n",
      "Epoch 19: val_loss did not improve from 1.36567\n",
      "29601/29601 [==============================] - 24s 824us/sample - loss: 1.2965 - val_loss: 1.3718\n",
      "Epoch 20/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2972\n",
      "Epoch 20: val_loss did not improve from 1.36567\n",
      "29601/29601 [==============================] - 25s 829us/sample - loss: 1.2972 - val_loss: 1.3738\n",
      "Epoch 21/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2981\n",
      "Epoch 21: val_loss did not improve from 1.36567\n",
      "29601/29601 [==============================] - 24s 827us/sample - loss: 1.2981 - val_loss: 1.3717\n",
      "Epoch 22/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2969\n",
      "Epoch 22: val_loss did not improve from 1.36567\n",
      "29601/29601 [==============================] - 24s 826us/sample - loss: 1.2969 - val_loss: 1.3715\n",
      "Epoch 23/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2985\n",
      "Epoch 23: val_loss did not improve from 1.36567\n",
      "29601/29601 [==============================] - 24s 827us/sample - loss: 1.2985 - val_loss: 1.3747\n",
      "Epoch 24/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3206\n",
      "Epoch 24: val_loss did not improve from 1.36567\n",
      "29601/29601 [==============================] - 24s 827us/sample - loss: 1.3206 - val_loss: 1.3715\n",
      "Epoch 25/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3084\n",
      "Epoch 25: val_loss did not improve from 1.36567\n",
      "29601/29601 [==============================] - 24s 826us/sample - loss: 1.3084 - val_loss: 1.3668\n",
      "Epoch 26/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3059\n",
      "Epoch 26: val_loss improved from 1.36567 to 1.36500, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_6.h5\n",
      "29601/29601 [==============================] - 25s 831us/sample - loss: 1.3059 - val_loss: 1.3650\n",
      "Epoch 27/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3128\n",
      "Epoch 27: val_loss did not improve from 1.36500\n",
      "29601/29601 [==============================] - 25s 832us/sample - loss: 1.3128 - val_loss: 1.3679\n",
      "Epoch 28/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3119\n",
      "Epoch 28: val_loss did not improve from 1.36500\n",
      "29601/29601 [==============================] - 24s 824us/sample - loss: 1.3119 - val_loss: 1.3667\n",
      "Epoch 29/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3067\n",
      "Epoch 29: val_loss did not improve from 1.36500\n",
      "29601/29601 [==============================] - 24s 826us/sample - loss: 1.3067 - val_loss: 1.3693\n",
      "Epoch 30/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3075\n",
      "Epoch 30: val_loss did not improve from 1.36500\n",
      "29601/29601 [==============================] - 24s 824us/sample - loss: 1.3075 - val_loss: 1.3660\n",
      "Epoch 31/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3103\n",
      "Epoch 31: val_loss improved from 1.36500 to 1.36351, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_6.h5\n",
      "29601/29601 [==============================] - 25s 835us/sample - loss: 1.3103 - val_loss: 1.3635\n",
      "Epoch 32/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3084\n",
      "Epoch 32: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 24s 827us/sample - loss: 1.3084 - val_loss: 1.3704\n",
      "Epoch 33/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3129\n",
      "Epoch 33: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 25s 829us/sample - loss: 1.3129 - val_loss: 1.3696\n",
      "Epoch 34/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3102\n",
      "Epoch 34: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 25s 829us/sample - loss: 1.3102 - val_loss: 1.3706\n",
      "Epoch 35/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3139\n",
      "Epoch 35: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 25s 834us/sample - loss: 1.3139 - val_loss: 1.3734\n",
      "Epoch 36/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3129\n",
      "Epoch 36: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 25s 830us/sample - loss: 1.3129 - val_loss: 1.3703\n",
      "Epoch 37/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3128\n",
      "Epoch 37: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 24s 822us/sample - loss: 1.3128 - val_loss: 1.3696\n",
      "Epoch 38/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3169\n",
      "Epoch 38: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 25s 829us/sample - loss: 1.3169 - val_loss: 1.3760\n",
      "Epoch 39/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3163\n",
      "Epoch 39: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 24s 825us/sample - loss: 1.3163 - val_loss: 1.3749\n",
      "Epoch 40/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3378\n",
      "Epoch 40: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 25s 832us/sample - loss: 1.3378 - val_loss: 1.3825\n",
      "Epoch 41/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3235\n",
      "Epoch 41: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 24s 824us/sample - loss: 1.3235 - val_loss: 1.3803\n",
      "Epoch 42/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3209\n",
      "Epoch 42: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 25s 829us/sample - loss: 1.3209 - val_loss: 1.3738\n",
      "Epoch 43/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3227\n",
      "Epoch 43: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 25s 830us/sample - loss: 1.3227 - val_loss: 1.3719\n",
      "Epoch 44/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3213\n",
      "Epoch 44: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 24s 825us/sample - loss: 1.3213 - val_loss: 1.3754\n",
      "Epoch 45/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3277\n",
      "Epoch 45: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 24s 824us/sample - loss: 1.3277 - val_loss: 1.3988\n",
      "Epoch 46/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3467\n",
      "Epoch 46: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 25s 828us/sample - loss: 1.3467 - val_loss: 1.3805\n",
      "Epoch 47/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3344\n",
      "Epoch 47: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 25s 829us/sample - loss: 1.3344 - val_loss: 1.3871\n",
      "Epoch 48/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3355\n",
      "Epoch 48: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 24s 823us/sample - loss: 1.3355 - val_loss: 1.3877\n",
      "Epoch 49/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3357\n",
      "Epoch 49: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 25s 828us/sample - loss: 1.3357 - val_loss: 1.3775\n",
      "Epoch 50/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3356\n",
      "Epoch 50: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 24s 825us/sample - loss: 1.3356 - val_loss: 1.3796\n",
      "Epoch 51/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3370\n",
      "Epoch 51: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 24s 824us/sample - loss: 1.3370 - val_loss: 1.3764\n",
      "Epoch 52/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3356\n",
      "Epoch 52: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 25s 831us/sample - loss: 1.3356 - val_loss: 1.3839\n",
      "Epoch 53/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3347\n",
      "Epoch 53: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 25s 829us/sample - loss: 1.3347 - val_loss: 1.3784\n",
      "Epoch 54/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3324\n",
      "Epoch 54: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 25s 832us/sample - loss: 1.3324 - val_loss: 1.3736\n",
      "Epoch 55/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3265\n",
      "Epoch 55: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 25s 836us/sample - loss: 1.3265 - val_loss: 1.3767\n",
      "Epoch 56/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3317\n",
      "Epoch 56: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 25s 834us/sample - loss: 1.3317 - val_loss: 1.3746\n",
      "Epoch 57/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3293\n",
      "Epoch 57: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 25s 838us/sample - loss: 1.3293 - val_loss: 1.3729\n",
      "Epoch 58/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3245\n",
      "Epoch 58: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 24s 827us/sample - loss: 1.3245 - val_loss: 1.3696\n",
      "Epoch 59/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3207\n",
      "Epoch 59: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 24s 826us/sample - loss: 1.3207 - val_loss: 1.3748\n",
      "Epoch 60/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3236\n",
      "Epoch 60: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 24s 823us/sample - loss: 1.3236 - val_loss: 1.3767\n",
      "Epoch 61/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3194\n",
      "Epoch 61: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 24s 824us/sample - loss: 1.3194 - val_loss: 1.3714\n",
      "Epoch 62/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3203\n",
      "Epoch 62: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 25s 830us/sample - loss: 1.3203 - val_loss: 1.3787\n",
      "Epoch 63/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3207\n",
      "Epoch 63: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 24s 826us/sample - loss: 1.3207 - val_loss: 1.3700\n",
      "Epoch 64/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3191\n",
      "Epoch 64: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 24s 826us/sample - loss: 1.3191 - val_loss: 1.3697\n",
      "Epoch 65/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3176\n",
      "Epoch 65: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 24s 817us/sample - loss: 1.3176 - val_loss: 1.3728\n",
      "Epoch 66/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3183\n",
      "Epoch 66: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 24s 813us/sample - loss: 1.3183 - val_loss: 1.3717\n",
      "Epoch 67/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3170\n",
      "Epoch 67: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 24s 814us/sample - loss: 1.3170 - val_loss: 1.3728\n",
      "Epoch 68/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3178\n",
      "Epoch 68: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 24s 814us/sample - loss: 1.3178 - val_loss: 1.3713\n",
      "Epoch 69/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3160\n",
      "Epoch 69: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 24s 819us/sample - loss: 1.3160 - val_loss: 1.3715\n",
      "Epoch 70/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3134\n",
      "Epoch 70: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 24s 808us/sample - loss: 1.3134 - val_loss: 1.3733\n",
      "Epoch 71/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3118\n",
      "Epoch 71: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 24s 816us/sample - loss: 1.3118 - val_loss: 1.3734\n",
      "Epoch 72/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3126\n",
      "Epoch 72: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 24s 825us/sample - loss: 1.3126 - val_loss: 1.3741\n",
      "Epoch 73/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3145\n",
      "Epoch 73: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 24s 815us/sample - loss: 1.3145 - val_loss: 1.3771\n",
      "Epoch 74/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3135\n",
      "Epoch 74: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 24s 811us/sample - loss: 1.3135 - val_loss: 1.3690\n",
      "Epoch 75/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3159\n",
      "Epoch 75: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 24s 812us/sample - loss: 1.3159 - val_loss: 1.3675\n",
      "Epoch 76/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3103\n",
      "Epoch 76: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 24s 810us/sample - loss: 1.3103 - val_loss: 1.3695\n",
      "Epoch 77/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3102\n",
      "Epoch 77: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 24s 822us/sample - loss: 1.3102 - val_loss: 1.3783\n",
      "Epoch 78/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3102\n",
      "Epoch 78: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 24s 817us/sample - loss: 1.3102 - val_loss: 1.3743\n",
      "Epoch 79/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3094\n",
      "Epoch 79: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 24s 816us/sample - loss: 1.3094 - val_loss: 1.3704\n",
      "Epoch 80/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3107\n",
      "Epoch 80: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 21s 723us/sample - loss: 1.3107 - val_loss: 1.3759\n",
      "Epoch 81/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3087\n",
      "Epoch 81: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 21s 720us/sample - loss: 1.3087 - val_loss: 1.3706\n",
      "Epoch 82/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3086\n",
      "Epoch 82: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 22s 735us/sample - loss: 1.3086 - val_loss: 1.3760\n",
      "Epoch 83/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3104\n",
      "Epoch 83: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 22s 740us/sample - loss: 1.3104 - val_loss: 1.3710\n",
      "Epoch 84/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3081\n",
      "Epoch 84: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 22s 737us/sample - loss: 1.3081 - val_loss: 1.3730\n",
      "Epoch 85/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3081\n",
      "Epoch 85: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 22s 754us/sample - loss: 1.3081 - val_loss: 1.3727\n",
      "Epoch 86/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3102\n",
      "Epoch 86: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 22s 743us/sample - loss: 1.3102 - val_loss: 1.3685\n",
      "Epoch 87/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3053\n",
      "Epoch 87: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 22s 737us/sample - loss: 1.3053 - val_loss: 1.3705\n",
      "Epoch 88/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3078\n",
      "Epoch 88: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 23s 762us/sample - loss: 1.3078 - val_loss: 1.3705\n",
      "Epoch 89/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3068\n",
      "Epoch 89: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 22s 741us/sample - loss: 1.3068 - val_loss: 1.3686\n",
      "Epoch 90/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3062\n",
      "Epoch 90: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 22s 752us/sample - loss: 1.3062 - val_loss: 1.3683\n",
      "Epoch 91/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3044\n",
      "Epoch 91: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 22s 752us/sample - loss: 1.3044 - val_loss: 1.3706\n",
      "Epoch 92/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3028\n",
      "Epoch 92: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 23s 760us/sample - loss: 1.3028 - val_loss: 1.3670\n",
      "Epoch 93/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3030\n",
      "Epoch 93: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 22s 745us/sample - loss: 1.3030 - val_loss: 1.3699\n",
      "Epoch 94/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3048\n",
      "Epoch 94: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 22s 754us/sample - loss: 1.3048 - val_loss: 1.3668\n",
      "Epoch 95/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3023\n",
      "Epoch 95: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 22s 758us/sample - loss: 1.3023 - val_loss: 1.3707\n",
      "Epoch 96/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3024\n",
      "Epoch 96: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 23s 788us/sample - loss: 1.3024 - val_loss: 1.3663\n",
      "Epoch 97/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3042\n",
      "Epoch 97: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 24s 796us/sample - loss: 1.3042 - val_loss: 1.3649\n",
      "Epoch 98/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3022\n",
      "Epoch 98: val_loss did not improve from 1.36351\n",
      "29601/29601 [==============================] - 24s 797us/sample - loss: 1.3022 - val_loss: 1.3722\n",
      "Epoch 99/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3037\n",
      "Epoch 99: val_loss improved from 1.36351 to 1.36262, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_6.h5\n",
      "29601/29601 [==============================] - 24s 799us/sample - loss: 1.3037 - val_loss: 1.3626\n",
      "Epoch 100/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3019\n",
      "Epoch 100: val_loss did not improve from 1.36262\n",
      "29601/29601 [==============================] - 23s 789us/sample - loss: 1.3019 - val_loss: 1.3684\n",
      "Train on 29601 samples, validate on 3694 samples\n",
      "Epoch 1/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3039\n",
      "Epoch 1: val_loss improved from inf to 1.36603, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_7.h5\n",
      "29601/29601 [==============================] - 24s 810us/sample - loss: 1.3039 - val_loss: 1.3660\n",
      "Epoch 2/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3013\n",
      "Epoch 2: val_loss did not improve from 1.36603\n",
      "29601/29601 [==============================] - 24s 798us/sample - loss: 1.3013 - val_loss: 1.3704\n",
      "Epoch 3/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2998\n",
      "Epoch 3: val_loss improved from 1.36603 to 1.36460, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_7.h5\n",
      "29601/29601 [==============================] - 24s 811us/sample - loss: 1.2998 - val_loss: 1.3646\n",
      "Epoch 4/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2951\n",
      "Epoch 4: val_loss did not improve from 1.36460\n",
      "29601/29601 [==============================] - 23s 791us/sample - loss: 1.2951 - val_loss: 1.3651\n",
      "Epoch 5/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2992\n",
      "Epoch 5: val_loss did not improve from 1.36460\n",
      "29601/29601 [==============================] - 24s 795us/sample - loss: 1.2992 - val_loss: 1.3647\n",
      "Epoch 6/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2985\n",
      "Epoch 6: val_loss did not improve from 1.36460\n",
      "29601/29601 [==============================] - 24s 801us/sample - loss: 1.2985 - val_loss: 1.3649\n",
      "Epoch 7/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2954\n",
      "Epoch 7: val_loss improved from 1.36460 to 1.36405, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_7.h5\n",
      "29601/29601 [==============================] - 24s 795us/sample - loss: 1.2954 - val_loss: 1.3641\n",
      "Epoch 8/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3002\n",
      "Epoch 8: val_loss did not improve from 1.36405\n",
      "29601/29601 [==============================] - 23s 792us/sample - loss: 1.3002 - val_loss: 1.3732\n",
      "Epoch 9/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2967\n",
      "Epoch 9: val_loss did not improve from 1.36405\n",
      "29601/29601 [==============================] - 24s 801us/sample - loss: 1.2967 - val_loss: 1.3710\n",
      "Epoch 10/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2959\n",
      "Epoch 10: val_loss improved from 1.36405 to 1.36073, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_7.h5\n",
      "29601/29601 [==============================] - 24s 809us/sample - loss: 1.2959 - val_loss: 1.3607\n",
      "Epoch 11/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2968\n",
      "Epoch 11: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 23s 793us/sample - loss: 1.2968 - val_loss: 1.3704\n",
      "Epoch 12/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3019\n",
      "Epoch 12: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 24s 801us/sample - loss: 1.3019 - val_loss: 1.3765\n",
      "Epoch 13/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3018\n",
      "Epoch 13: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 24s 800us/sample - loss: 1.3018 - val_loss: 1.3699\n",
      "Epoch 14/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3001\n",
      "Epoch 14: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 23s 782us/sample - loss: 1.3001 - val_loss: 1.3666\n",
      "Epoch 15/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2984\n",
      "Epoch 15: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 23s 794us/sample - loss: 1.2984 - val_loss: 1.3694\n",
      "Epoch 16/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2988\n",
      "Epoch 16: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 23s 787us/sample - loss: 1.2988 - val_loss: 1.3667\n",
      "Epoch 17/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2973\n",
      "Epoch 17: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 24s 797us/sample - loss: 1.2973 - val_loss: 1.3700\n",
      "Epoch 18/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2980\n",
      "Epoch 18: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 23s 793us/sample - loss: 1.2980 - val_loss: 1.3711\n",
      "Epoch 19/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3011\n",
      "Epoch 19: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 24s 794us/sample - loss: 1.3011 - val_loss: 1.3696\n",
      "Epoch 20/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3017\n",
      "Epoch 20: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 23s 779us/sample - loss: 1.3017 - val_loss: 1.3671\n",
      "Epoch 21/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3003\n",
      "Epoch 21: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 24s 796us/sample - loss: 1.3003 - val_loss: 1.3697\n",
      "Epoch 22/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3006\n",
      "Epoch 22: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 24s 801us/sample - loss: 1.3006 - val_loss: 1.3651\n",
      "Epoch 23/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3125\n",
      "Epoch 23: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 23s 785us/sample - loss: 1.3125 - val_loss: 1.3725\n",
      "Epoch 24/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3100\n",
      "Epoch 24: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 23s 787us/sample - loss: 1.3100 - val_loss: 1.3681\n",
      "Epoch 25/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3078\n",
      "Epoch 25: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 23s 786us/sample - loss: 1.3078 - val_loss: 1.3749\n",
      "Epoch 26/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3065\n",
      "Epoch 26: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 24s 799us/sample - loss: 1.3065 - val_loss: 1.3688\n",
      "Epoch 27/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3062\n",
      "Epoch 27: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 24s 798us/sample - loss: 1.3062 - val_loss: 1.3775\n",
      "Epoch 28/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3160\n",
      "Epoch 28: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 24s 797us/sample - loss: 1.3160 - val_loss: 1.3702\n",
      "Epoch 29/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3156\n",
      "Epoch 29: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 24s 795us/sample - loss: 1.3156 - val_loss: 1.3673\n",
      "Epoch 30/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3130\n",
      "Epoch 30: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 24s 807us/sample - loss: 1.3130 - val_loss: 1.3682\n",
      "Epoch 31/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3159\n",
      "Epoch 31: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 23s 794us/sample - loss: 1.3159 - val_loss: 1.3718\n",
      "Epoch 32/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3117\n",
      "Epoch 32: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 23s 790us/sample - loss: 1.3117 - val_loss: 1.3719\n",
      "Epoch 33/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3156\n",
      "Epoch 33: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 24s 799us/sample - loss: 1.3156 - val_loss: 1.3743\n",
      "Epoch 34/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3153\n",
      "Epoch 34: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 24s 800us/sample - loss: 1.3153 - val_loss: 1.3701\n",
      "Epoch 35/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3150\n",
      "Epoch 35: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 24s 795us/sample - loss: 1.3150 - val_loss: 1.3729\n",
      "Epoch 36/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3162\n",
      "Epoch 36: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 23s 786us/sample - loss: 1.3162 - val_loss: 1.3679\n",
      "Epoch 37/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3199\n",
      "Epoch 37: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 23s 788us/sample - loss: 1.3199 - val_loss: 1.3720\n",
      "Epoch 38/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3175\n",
      "Epoch 38: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 23s 790us/sample - loss: 1.3175 - val_loss: 1.3690\n",
      "Epoch 39/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3151\n",
      "Epoch 39: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 23s 785us/sample - loss: 1.3151 - val_loss: 1.3709\n",
      "Epoch 40/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3193\n",
      "Epoch 40: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 24s 797us/sample - loss: 1.3193 - val_loss: 1.3729\n",
      "Epoch 41/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3206\n",
      "Epoch 41: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 24s 802us/sample - loss: 1.3206 - val_loss: 1.3738\n",
      "Epoch 42/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3218\n",
      "Epoch 42: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 22s 753us/sample - loss: 1.3218 - val_loss: 1.3699\n",
      "Epoch 43/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3217\n",
      "Epoch 43: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 21s 701us/sample - loss: 1.3217 - val_loss: 1.3672\n",
      "Epoch 44/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3187\n",
      "Epoch 44: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 17s 576us/sample - loss: 1.3187 - val_loss: 1.3783\n",
      "Epoch 45/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3198\n",
      "Epoch 45: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 22s 750us/sample - loss: 1.3198 - val_loss: 1.3788\n",
      "Epoch 46/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3204\n",
      "Epoch 46: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 22s 741us/sample - loss: 1.3204 - val_loss: 1.3742\n",
      "Epoch 47/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3294\n",
      "Epoch 47: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 22s 753us/sample - loss: 1.3294 - val_loss: 1.3792\n",
      "Epoch 48/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3338\n",
      "Epoch 48: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 22s 750us/sample - loss: 1.3338 - val_loss: 1.3765\n",
      "Epoch 49/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3297\n",
      "Epoch 49: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 22s 742us/sample - loss: 1.3297 - val_loss: 1.3740\n",
      "Epoch 50/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3667\n",
      "Epoch 50: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 22s 749us/sample - loss: 1.3667 - val_loss: 1.3822\n",
      "Epoch 51/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3491\n",
      "Epoch 51: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 22s 748us/sample - loss: 1.3491 - val_loss: 1.3868\n",
      "Epoch 52/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3443\n",
      "Epoch 52: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 23s 764us/sample - loss: 1.3443 - val_loss: 1.3844\n",
      "Epoch 53/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3395\n",
      "Epoch 53: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 22s 745us/sample - loss: 1.3395 - val_loss: 1.3828\n",
      "Epoch 54/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3358\n",
      "Epoch 54: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 22s 758us/sample - loss: 1.3358 - val_loss: 1.3795\n",
      "Epoch 55/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3345\n",
      "Epoch 55: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 22s 756us/sample - loss: 1.3345 - val_loss: 1.3831\n",
      "Epoch 56/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3322\n",
      "Epoch 56: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 22s 753us/sample - loss: 1.3322 - val_loss: 1.3783\n",
      "Epoch 57/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3312\n",
      "Epoch 57: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 22s 760us/sample - loss: 1.3312 - val_loss: 1.3799\n",
      "Epoch 58/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3309\n",
      "Epoch 58: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 22s 743us/sample - loss: 1.3309 - val_loss: 1.3772\n",
      "Epoch 59/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3302\n",
      "Epoch 59: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 22s 733us/sample - loss: 1.3302 - val_loss: 1.3768\n",
      "Epoch 60/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3252\n",
      "Epoch 60: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 22s 756us/sample - loss: 1.3252 - val_loss: 1.3808\n",
      "Epoch 61/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3268\n",
      "Epoch 61: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 23s 765us/sample - loss: 1.3268 - val_loss: 1.3750\n",
      "Epoch 62/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3236\n",
      "Epoch 62: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 22s 752us/sample - loss: 1.3236 - val_loss: 1.3717\n",
      "Epoch 63/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3227\n",
      "Epoch 63: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 23s 765us/sample - loss: 1.3227 - val_loss: 1.3776\n",
      "Epoch 64/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3233\n",
      "Epoch 64: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 23s 763us/sample - loss: 1.3233 - val_loss: 1.3737\n",
      "Epoch 65/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3233\n",
      "Epoch 65: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 22s 756us/sample - loss: 1.3233 - val_loss: 1.3728\n",
      "Epoch 66/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3219\n",
      "Epoch 66: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 23s 760us/sample - loss: 1.3219 - val_loss: 1.3684\n",
      "Epoch 67/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3188\n",
      "Epoch 67: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 22s 753us/sample - loss: 1.3188 - val_loss: 1.3801\n",
      "Epoch 68/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3181\n",
      "Epoch 68: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 22s 753us/sample - loss: 1.3181 - val_loss: 1.3698\n",
      "Epoch 69/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3172\n",
      "Epoch 69: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 22s 756us/sample - loss: 1.3172 - val_loss: 1.3729\n",
      "Epoch 70/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3176\n",
      "Epoch 70: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 22s 747us/sample - loss: 1.3176 - val_loss: 1.3701\n",
      "Epoch 71/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3174\n",
      "Epoch 71: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 22s 745us/sample - loss: 1.3174 - val_loss: 1.3724\n",
      "Epoch 72/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3190\n",
      "Epoch 72: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 23s 763us/sample - loss: 1.3190 - val_loss: 1.3716\n",
      "Epoch 73/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3162\n",
      "Epoch 73: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 22s 750us/sample - loss: 1.3162 - val_loss: 1.3745\n",
      "Epoch 74/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3153\n",
      "Epoch 74: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 22s 744us/sample - loss: 1.3153 - val_loss: 1.3739\n",
      "Epoch 75/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3131\n",
      "Epoch 75: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 23s 761us/sample - loss: 1.3131 - val_loss: 1.3713\n",
      "Epoch 76/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3164\n",
      "Epoch 76: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 22s 756us/sample - loss: 1.3164 - val_loss: 1.3667\n",
      "Epoch 77/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3155\n",
      "Epoch 77: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 22s 754us/sample - loss: 1.3155 - val_loss: 1.3678\n",
      "Epoch 78/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3141\n",
      "Epoch 78: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 22s 752us/sample - loss: 1.3141 - val_loss: 1.3686\n",
      "Epoch 79/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3131\n",
      "Epoch 79: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 22s 742us/sample - loss: 1.3131 - val_loss: 1.3707\n",
      "Epoch 80/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3109\n",
      "Epoch 80: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 22s 752us/sample - loss: 1.3109 - val_loss: 1.3689\n",
      "Epoch 81/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3100\n",
      "Epoch 81: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 22s 744us/sample - loss: 1.3100 - val_loss: 1.3685\n",
      "Epoch 82/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3087\n",
      "Epoch 82: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 22s 751us/sample - loss: 1.3087 - val_loss: 1.3714\n",
      "Epoch 83/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3117\n",
      "Epoch 83: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 22s 753us/sample - loss: 1.3117 - val_loss: 1.3721\n",
      "Epoch 84/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3117\n",
      "Epoch 84: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 23s 767us/sample - loss: 1.3117 - val_loss: 1.3685\n",
      "Epoch 85/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3105\n",
      "Epoch 85: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 22s 757us/sample - loss: 1.3105 - val_loss: 1.3672\n",
      "Epoch 86/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3110\n",
      "Epoch 86: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 22s 747us/sample - loss: 1.3110 - val_loss: 1.3707\n",
      "Epoch 87/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3109\n",
      "Epoch 87: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 22s 749us/sample - loss: 1.3109 - val_loss: 1.3682\n",
      "Epoch 88/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3127\n",
      "Epoch 88: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 22s 743us/sample - loss: 1.3127 - val_loss: 1.3649\n",
      "Epoch 89/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3068\n",
      "Epoch 89: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 22s 751us/sample - loss: 1.3068 - val_loss: 1.3651\n",
      "Epoch 90/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3077\n",
      "Epoch 90: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 22s 746us/sample - loss: 1.3077 - val_loss: 1.3699\n",
      "Epoch 91/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3063\n",
      "Epoch 91: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 22s 742us/sample - loss: 1.3063 - val_loss: 1.3650\n",
      "Epoch 92/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3069\n",
      "Epoch 92: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 22s 759us/sample - loss: 1.3069 - val_loss: 1.3617\n",
      "Epoch 93/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3072\n",
      "Epoch 93: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 22s 747us/sample - loss: 1.3072 - val_loss: 1.3681\n",
      "Epoch 94/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3086\n",
      "Epoch 94: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 22s 737us/sample - loss: 1.3086 - val_loss: 1.3652\n",
      "Epoch 95/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3050\n",
      "Epoch 95: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 22s 749us/sample - loss: 1.3050 - val_loss: 1.3676\n",
      "Epoch 96/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3064\n",
      "Epoch 96: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 22s 743us/sample - loss: 1.3064 - val_loss: 1.3662\n",
      "Epoch 97/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3046\n",
      "Epoch 97: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 22s 742us/sample - loss: 1.3046 - val_loss: 1.3665\n",
      "Epoch 98/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3054\n",
      "Epoch 98: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 22s 750us/sample - loss: 1.3054 - val_loss: 1.3646\n",
      "Epoch 99/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3034\n",
      "Epoch 99: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 22s 752us/sample - loss: 1.3034 - val_loss: 1.3743\n",
      "Epoch 100/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3060\n",
      "Epoch 100: val_loss did not improve from 1.36073\n",
      "29601/29601 [==============================] - 22s 752us/sample - loss: 1.3060 - val_loss: 1.3713\n",
      "Train on 29601 samples, validate on 3694 samples\n",
      "Epoch 1/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3031\n",
      "Epoch 1: val_loss improved from inf to 1.36411, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_8.h5\n",
      "29601/29601 [==============================] - 23s 763us/sample - loss: 1.3031 - val_loss: 1.3641\n",
      "Epoch 2/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3038\n",
      "Epoch 2: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 751us/sample - loss: 1.3038 - val_loss: 1.3679\n",
      "Epoch 3/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3002\n",
      "Epoch 3: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 23s 771us/sample - loss: 1.3002 - val_loss: 1.3644\n",
      "Epoch 4/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2987\n",
      "Epoch 4: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 23s 769us/sample - loss: 1.2987 - val_loss: 1.3686\n",
      "Epoch 5/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2992\n",
      "Epoch 5: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 759us/sample - loss: 1.2992 - val_loss: 1.3723\n",
      "Epoch 6/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2968\n",
      "Epoch 6: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 752us/sample - loss: 1.2968 - val_loss: 1.3647\n",
      "Epoch 7/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2957\n",
      "Epoch 7: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 755us/sample - loss: 1.2957 - val_loss: 1.3674\n",
      "Epoch 8/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2977\n",
      "Epoch 8: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 748us/sample - loss: 1.2977 - val_loss: 1.3657\n",
      "Epoch 9/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3018\n",
      "Epoch 9: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 746us/sample - loss: 1.3018 - val_loss: 1.3695\n",
      "Epoch 10/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3012\n",
      "Epoch 10: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 23s 764us/sample - loss: 1.3012 - val_loss: 1.3720\n",
      "Epoch 11/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2977\n",
      "Epoch 11: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 756us/sample - loss: 1.2977 - val_loss: 1.3683\n",
      "Epoch 12/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3026\n",
      "Epoch 12: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 749us/sample - loss: 1.3026 - val_loss: 1.3691\n",
      "Epoch 13/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3034\n",
      "Epoch 13: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 23s 767us/sample - loss: 1.3034 - val_loss: 1.3713\n",
      "Epoch 14/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3036\n",
      "Epoch 14: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 23s 763us/sample - loss: 1.3036 - val_loss: 1.3657\n",
      "Epoch 15/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3014\n",
      "Epoch 15: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 754us/sample - loss: 1.3014 - val_loss: 1.3679\n",
      "Epoch 16/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3006\n",
      "Epoch 16: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 23s 764us/sample - loss: 1.3006 - val_loss: 1.3717\n",
      "Epoch 17/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3033\n",
      "Epoch 17: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 758us/sample - loss: 1.3033 - val_loss: 1.3693\n",
      "Epoch 18/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3047\n",
      "Epoch 18: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 23s 762us/sample - loss: 1.3047 - val_loss: 1.3789\n",
      "Epoch 19/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3078\n",
      "Epoch 19: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 742us/sample - loss: 1.3078 - val_loss: 1.3653\n",
      "Epoch 20/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3095\n",
      "Epoch 20: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 758us/sample - loss: 1.3095 - val_loss: 1.3755\n",
      "Epoch 21/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3070\n",
      "Epoch 21: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 749us/sample - loss: 1.3070 - val_loss: 1.3750\n",
      "Epoch 22/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3066\n",
      "Epoch 22: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 759us/sample - loss: 1.3066 - val_loss: 1.3641\n",
      "Epoch 23/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3059\n",
      "Epoch 23: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 23s 763us/sample - loss: 1.3059 - val_loss: 1.3659\n",
      "Epoch 24/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3095\n",
      "Epoch 24: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 743us/sample - loss: 1.3095 - val_loss: 1.3711\n",
      "Epoch 25/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3093\n",
      "Epoch 25: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 756us/sample - loss: 1.3093 - val_loss: 1.3794\n",
      "Epoch 26/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3326\n",
      "Epoch 26: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 755us/sample - loss: 1.3326 - val_loss: 1.3747\n",
      "Epoch 27/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3169\n",
      "Epoch 27: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 756us/sample - loss: 1.3169 - val_loss: 1.3705\n",
      "Epoch 28/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3144\n",
      "Epoch 28: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 755us/sample - loss: 1.3144 - val_loss: 1.3736\n",
      "Epoch 29/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3145\n",
      "Epoch 29: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 752us/sample - loss: 1.3145 - val_loss: 1.3667\n",
      "Epoch 30/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3162\n",
      "Epoch 30: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 740us/sample - loss: 1.3162 - val_loss: 1.3668\n",
      "Epoch 31/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3291\n",
      "Epoch 31: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 748us/sample - loss: 1.3291 - val_loss: 1.3779\n",
      "Epoch 32/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3241\n",
      "Epoch 32: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 748us/sample - loss: 1.3241 - val_loss: 1.3709\n",
      "Epoch 33/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3204\n",
      "Epoch 33: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 752us/sample - loss: 1.3204 - val_loss: 1.3691\n",
      "Epoch 34/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3209\n",
      "Epoch 34: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 740us/sample - loss: 1.3209 - val_loss: 1.3687\n",
      "Epoch 35/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3238\n",
      "Epoch 35: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 740us/sample - loss: 1.3238 - val_loss: 1.3702\n",
      "Epoch 36/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3298\n",
      "Epoch 36: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 754us/sample - loss: 1.3298 - val_loss: 1.3684\n",
      "Epoch 37/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3221\n",
      "Epoch 37: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 23s 763us/sample - loss: 1.3221 - val_loss: 1.3695\n",
      "Epoch 38/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3278\n",
      "Epoch 38: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 744us/sample - loss: 1.3278 - val_loss: 1.3731\n",
      "Epoch 39/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3268\n",
      "Epoch 39: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 23s 767us/sample - loss: 1.3268 - val_loss: 1.3707\n",
      "Epoch 40/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3242\n",
      "Epoch 40: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 754us/sample - loss: 1.3242 - val_loss: 1.3731\n",
      "Epoch 41/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3269\n",
      "Epoch 41: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 757us/sample - loss: 1.3269 - val_loss: 1.3764\n",
      "Epoch 42/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3275\n",
      "Epoch 42: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 744us/sample - loss: 1.3275 - val_loss: 1.3649\n",
      "Epoch 43/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3289\n",
      "Epoch 43: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 750us/sample - loss: 1.3289 - val_loss: 1.3712\n",
      "Epoch 44/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3377\n",
      "Epoch 44: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 748us/sample - loss: 1.3377 - val_loss: 1.3753\n",
      "Epoch 45/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.4152\n",
      "Epoch 45: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 758us/sample - loss: 1.4152 - val_loss: 1.3779\n",
      "Epoch 46/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3479\n",
      "Epoch 46: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 23s 768us/sample - loss: 1.3479 - val_loss: 1.3797\n",
      "Epoch 47/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3370\n",
      "Epoch 47: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 756us/sample - loss: 1.3370 - val_loss: 1.3734\n",
      "Epoch 48/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3355\n",
      "Epoch 48: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 739us/sample - loss: 1.3355 - val_loss: 1.3728\n",
      "Epoch 49/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3392\n",
      "Epoch 49: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 23s 770us/sample - loss: 1.3392 - val_loss: 1.3731\n",
      "Epoch 50/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3393\n",
      "Epoch 50: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 748us/sample - loss: 1.3393 - val_loss: 1.3781\n",
      "Epoch 51/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3391\n",
      "Epoch 51: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 746us/sample - loss: 1.3391 - val_loss: 1.3833\n",
      "Epoch 52/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3494\n",
      "Epoch 52: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 753us/sample - loss: 1.3494 - val_loss: 1.3846\n",
      "Epoch 53/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3433\n",
      "Epoch 53: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 757us/sample - loss: 1.3433 - val_loss: 1.3818\n",
      "Epoch 54/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3402\n",
      "Epoch 54: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 753us/sample - loss: 1.3402 - val_loss: 1.3819\n",
      "Epoch 55/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3449\n",
      "Epoch 55: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 742us/sample - loss: 1.3449 - val_loss: 1.3966\n",
      "Epoch 56/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3423\n",
      "Epoch 56: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 23s 767us/sample - loss: 1.3423 - val_loss: 1.3852\n",
      "Epoch 57/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3427\n",
      "Epoch 57: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 23s 761us/sample - loss: 1.3427 - val_loss: 1.3807\n",
      "Epoch 58/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3378\n",
      "Epoch 58: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 755us/sample - loss: 1.3378 - val_loss: 1.3832\n",
      "Epoch 59/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3380\n",
      "Epoch 59: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 760us/sample - loss: 1.3380 - val_loss: 1.3808\n",
      "Epoch 60/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3365\n",
      "Epoch 60: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 744us/sample - loss: 1.3365 - val_loss: 1.3840\n",
      "Epoch 61/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3381\n",
      "Epoch 61: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 23s 764us/sample - loss: 1.3381 - val_loss: 1.3823\n",
      "Epoch 62/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3332\n",
      "Epoch 62: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 755us/sample - loss: 1.3332 - val_loss: 1.3781\n",
      "Epoch 63/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3380\n",
      "Epoch 63: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 758us/sample - loss: 1.3380 - val_loss: 1.3833\n",
      "Epoch 64/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3310\n",
      "Epoch 64: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 743us/sample - loss: 1.3310 - val_loss: 1.3853\n",
      "Epoch 65/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3323\n",
      "Epoch 65: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 759us/sample - loss: 1.3323 - val_loss: 1.3887\n",
      "Epoch 66/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3304\n",
      "Epoch 66: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 23s 768us/sample - loss: 1.3304 - val_loss: 1.3815\n",
      "Epoch 67/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3280\n",
      "Epoch 67: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 23s 761us/sample - loss: 1.3280 - val_loss: 1.3776\n",
      "Epoch 68/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3305\n",
      "Epoch 68: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 749us/sample - loss: 1.3305 - val_loss: 1.3854\n",
      "Epoch 69/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3310\n",
      "Epoch 69: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 749us/sample - loss: 1.3310 - val_loss: 1.3858\n",
      "Epoch 70/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3262\n",
      "Epoch 70: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 752us/sample - loss: 1.3262 - val_loss: 1.3823\n",
      "Epoch 71/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3291\n",
      "Epoch 71: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 23s 768us/sample - loss: 1.3291 - val_loss: 1.3756\n",
      "Epoch 72/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3267\n",
      "Epoch 72: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 759us/sample - loss: 1.3267 - val_loss: 1.3756\n",
      "Epoch 73/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3234\n",
      "Epoch 73: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 757us/sample - loss: 1.3234 - val_loss: 1.3812\n",
      "Epoch 74/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3252\n",
      "Epoch 74: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 756us/sample - loss: 1.3252 - val_loss: 1.3755\n",
      "Epoch 75/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3271\n",
      "Epoch 75: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 745us/sample - loss: 1.3271 - val_loss: 1.3781\n",
      "Epoch 76/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3237\n",
      "Epoch 76: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 752us/sample - loss: 1.3237 - val_loss: 1.3758\n",
      "Epoch 77/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3216\n",
      "Epoch 77: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 754us/sample - loss: 1.3216 - val_loss: 1.3764\n",
      "Epoch 78/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3280\n",
      "Epoch 78: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 23s 762us/sample - loss: 1.3280 - val_loss: 1.3803\n",
      "Epoch 79/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3226\n",
      "Epoch 79: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 748us/sample - loss: 1.3226 - val_loss: 1.3726\n",
      "Epoch 80/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3233\n",
      "Epoch 80: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 740us/sample - loss: 1.3233 - val_loss: 1.3739\n",
      "Epoch 81/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3233\n",
      "Epoch 81: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 23s 775us/sample - loss: 1.3233 - val_loss: 1.3828\n",
      "Epoch 82/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3208\n",
      "Epoch 82: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 756us/sample - loss: 1.3208 - val_loss: 1.3804\n",
      "Epoch 83/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3202\n",
      "Epoch 83: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 737us/sample - loss: 1.3202 - val_loss: 1.3730\n",
      "Epoch 84/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3249\n",
      "Epoch 84: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 23s 762us/sample - loss: 1.3249 - val_loss: 1.3731\n",
      "Epoch 85/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3198\n",
      "Epoch 85: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 753us/sample - loss: 1.3198 - val_loss: 1.3819\n",
      "Epoch 86/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3207\n",
      "Epoch 86: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 757us/sample - loss: 1.3207 - val_loss: 1.3769\n",
      "Epoch 87/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3186\n",
      "Epoch 87: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 23s 767us/sample - loss: 1.3186 - val_loss: 1.3793\n",
      "Epoch 88/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3203\n",
      "Epoch 88: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 753us/sample - loss: 1.3203 - val_loss: 1.3719\n",
      "Epoch 89/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3179\n",
      "Epoch 89: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 742us/sample - loss: 1.3179 - val_loss: 1.3725\n",
      "Epoch 90/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3201\n",
      "Epoch 90: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 739us/sample - loss: 1.3201 - val_loss: 1.3752\n",
      "Epoch 91/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3190\n",
      "Epoch 91: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 23s 769us/sample - loss: 1.3190 - val_loss: 1.3725\n",
      "Epoch 92/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3150\n",
      "Epoch 92: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 740us/sample - loss: 1.3150 - val_loss: 1.3721\n",
      "Epoch 93/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3150\n",
      "Epoch 93: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 748us/sample - loss: 1.3150 - val_loss: 1.3783\n",
      "Epoch 94/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3157\n",
      "Epoch 94: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 746us/sample - loss: 1.3157 - val_loss: 1.3701\n",
      "Epoch 95/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3159\n",
      "Epoch 95: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 759us/sample - loss: 1.3159 - val_loss: 1.3711\n",
      "Epoch 96/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3135\n",
      "Epoch 96: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 748us/sample - loss: 1.3135 - val_loss: 1.3747\n",
      "Epoch 97/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3149\n",
      "Epoch 97: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 760us/sample - loss: 1.3149 - val_loss: 1.3715\n",
      "Epoch 98/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3124\n",
      "Epoch 98: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 758us/sample - loss: 1.3124 - val_loss: 1.3785\n",
      "Epoch 99/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3152\n",
      "Epoch 99: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 753us/sample - loss: 1.3152 - val_loss: 1.3719\n",
      "Epoch 100/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3129\n",
      "Epoch 100: val_loss did not improve from 1.36411\n",
      "29601/29601 [==============================] - 22s 744us/sample - loss: 1.3129 - val_loss: 1.3749\n",
      "Train on 29601 samples, validate on 3694 samples\n",
      "Epoch 1/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3126\n",
      "Epoch 1: val_loss improved from inf to 1.36944, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_9.h5\n",
      "29601/29601 [==============================] - 23s 763us/sample - loss: 1.3126 - val_loss: 1.3694\n",
      "Epoch 2/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3154\n",
      "Epoch 2: val_loss did not improve from 1.36944\n",
      "29601/29601 [==============================] - 23s 761us/sample - loss: 1.3154 - val_loss: 1.3774\n",
      "Epoch 3/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3082\n",
      "Epoch 3: val_loss did not improve from 1.36944\n",
      "29601/29601 [==============================] - 22s 758us/sample - loss: 1.3082 - val_loss: 1.3778\n",
      "Epoch 4/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3065\n",
      "Epoch 4: val_loss did not improve from 1.36944\n",
      "29601/29601 [==============================] - 22s 746us/sample - loss: 1.3065 - val_loss: 1.3710\n",
      "Epoch 5/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3060\n",
      "Epoch 5: val_loss did not improve from 1.36944\n",
      "29601/29601 [==============================] - 22s 745us/sample - loss: 1.3060 - val_loss: 1.3701\n",
      "Epoch 6/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3032\n",
      "Epoch 6: val_loss did not improve from 1.36944\n",
      "29601/29601 [==============================] - 23s 765us/sample - loss: 1.3032 - val_loss: 1.3708\n",
      "Epoch 7/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3012\n",
      "Epoch 7: val_loss improved from 1.36944 to 1.36399, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_9.h5\n",
      "29601/29601 [==============================] - 23s 761us/sample - loss: 1.3012 - val_loss: 1.3640\n",
      "Epoch 8/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3008\n",
      "Epoch 8: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 22s 730us/sample - loss: 1.3008 - val_loss: 1.3672\n",
      "Epoch 9/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3034\n",
      "Epoch 9: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 23s 764us/sample - loss: 1.3034 - val_loss: 1.3702\n",
      "Epoch 10/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3013\n",
      "Epoch 10: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 22s 756us/sample - loss: 1.3013 - val_loss: 1.3696\n",
      "Epoch 11/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3011\n",
      "Epoch 11: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 22s 736us/sample - loss: 1.3011 - val_loss: 1.3712\n",
      "Epoch 12/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.2980\n",
      "Epoch 12: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 22s 756us/sample - loss: 1.2980 - val_loss: 1.3678\n",
      "Epoch 13/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3057\n",
      "Epoch 13: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 22s 756us/sample - loss: 1.3057 - val_loss: 1.3736\n",
      "Epoch 14/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3413\n",
      "Epoch 14: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 23s 761us/sample - loss: 1.3413 - val_loss: 1.3761\n",
      "Epoch 15/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3124\n",
      "Epoch 15: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 22s 741us/sample - loss: 1.3124 - val_loss: 1.3731\n",
      "Epoch 16/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3110\n",
      "Epoch 16: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 22s 748us/sample - loss: 1.3110 - val_loss: 1.3702\n",
      "Epoch 17/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3099\n",
      "Epoch 17: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 22s 743us/sample - loss: 1.3099 - val_loss: 1.3693\n",
      "Epoch 18/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3091\n",
      "Epoch 18: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 22s 750us/sample - loss: 1.3091 - val_loss: 1.3778\n",
      "Epoch 19/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3102\n",
      "Epoch 19: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 23s 764us/sample - loss: 1.3102 - val_loss: 1.3696\n",
      "Epoch 20/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3084\n",
      "Epoch 20: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 22s 759us/sample - loss: 1.3084 - val_loss: 1.3722\n",
      "Epoch 21/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3069\n",
      "Epoch 21: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 22s 748us/sample - loss: 1.3069 - val_loss: 1.3804\n",
      "Epoch 22/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3111\n",
      "Epoch 22: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 22s 733us/sample - loss: 1.3111 - val_loss: 1.3720\n",
      "Epoch 23/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3112\n",
      "Epoch 23: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 22s 744us/sample - loss: 1.3112 - val_loss: 1.3758\n",
      "Epoch 24/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3118\n",
      "Epoch 24: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 22s 752us/sample - loss: 1.3118 - val_loss: 1.3689\n",
      "Epoch 25/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3105\n",
      "Epoch 25: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 22s 746us/sample - loss: 1.3105 - val_loss: 1.3751\n",
      "Epoch 26/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3124\n",
      "Epoch 26: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 22s 751us/sample - loss: 1.3124 - val_loss: 1.3735\n",
      "Epoch 27/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3096\n",
      "Epoch 27: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 22s 749us/sample - loss: 1.3096 - val_loss: 1.3704\n",
      "Epoch 28/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3121\n",
      "Epoch 28: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 22s 747us/sample - loss: 1.3121 - val_loss: 1.3680\n",
      "Epoch 29/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3225\n",
      "Epoch 29: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 22s 751us/sample - loss: 1.3225 - val_loss: 1.3767\n",
      "Epoch 30/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3155\n",
      "Epoch 30: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 23s 764us/sample - loss: 1.3155 - val_loss: 1.3705\n",
      "Epoch 31/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3143\n",
      "Epoch 31: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 22s 746us/sample - loss: 1.3143 - val_loss: 1.3686\n",
      "Epoch 32/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3201\n",
      "Epoch 32: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 22s 743us/sample - loss: 1.3201 - val_loss: 1.3744\n",
      "Epoch 33/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3173\n",
      "Epoch 33: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 22s 754us/sample - loss: 1.3173 - val_loss: 1.3724\n",
      "Epoch 34/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3156\n",
      "Epoch 34: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 23s 762us/sample - loss: 1.3156 - val_loss: 1.3715\n",
      "Epoch 35/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3215\n",
      "Epoch 35: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 23s 763us/sample - loss: 1.3215 - val_loss: 1.3674\n",
      "Epoch 36/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3175\n",
      "Epoch 36: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 22s 750us/sample - loss: 1.3175 - val_loss: 1.3674\n",
      "Epoch 37/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3209\n",
      "Epoch 37: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 22s 752us/sample - loss: 1.3209 - val_loss: 1.3734\n",
      "Epoch 38/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3212\n",
      "Epoch 38: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 23s 779us/sample - loss: 1.3212 - val_loss: 1.3712\n",
      "Epoch 39/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3350\n",
      "Epoch 39: val_loss did not improve from 1.36399\n",
      "29601/29601 [==============================] - 22s 748us/sample - loss: 1.3350 - val_loss: 1.3653\n",
      "Epoch 40/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3272\n",
      "Epoch 40: val_loss improved from 1.36399 to 1.36220, saving model to ./checkpoints/Adaptive_prune_model_remove_structure_9.h5\n",
      "29601/29601 [==============================] - 22s 759us/sample - loss: 1.3272 - val_loss: 1.3622\n",
      "Epoch 41/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3268\n",
      "Epoch 41: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 22s 734us/sample - loss: 1.3268 - val_loss: 1.3688\n",
      "Epoch 42/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3276\n",
      "Epoch 42: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 23s 768us/sample - loss: 1.3276 - val_loss: 1.3749\n",
      "Epoch 43/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3354\n",
      "Epoch 43: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 22s 753us/sample - loss: 1.3354 - val_loss: 1.3703\n",
      "Epoch 44/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3271\n",
      "Epoch 44: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 22s 745us/sample - loss: 1.3271 - val_loss: 1.3698\n",
      "Epoch 45/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3301\n",
      "Epoch 45: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 22s 743us/sample - loss: 1.3301 - val_loss: 1.3791\n",
      "Epoch 46/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3324\n",
      "Epoch 46: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 22s 757us/sample - loss: 1.3324 - val_loss: 1.3767\n",
      "Epoch 47/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3377\n",
      "Epoch 47: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 23s 766us/sample - loss: 1.3377 - val_loss: 1.3791\n",
      "Epoch 48/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3386\n",
      "Epoch 48: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 22s 760us/sample - loss: 1.3386 - val_loss: 1.3807\n",
      "Epoch 49/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3395\n",
      "Epoch 49: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 22s 744us/sample - loss: 1.3395 - val_loss: 1.3815\n",
      "Epoch 50/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3747\n",
      "Epoch 50: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 22s 756us/sample - loss: 1.3747 - val_loss: 1.3810\n",
      "Epoch 51/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3529\n",
      "Epoch 51: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 22s 748us/sample - loss: 1.3529 - val_loss: 1.3736\n",
      "Epoch 52/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3521\n",
      "Epoch 52: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 22s 736us/sample - loss: 1.3521 - val_loss: 1.3727\n",
      "Epoch 53/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3466\n",
      "Epoch 53: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 22s 753us/sample - loss: 1.3466 - val_loss: 1.3713\n",
      "Epoch 54/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3477\n",
      "Epoch 54: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 22s 759us/sample - loss: 1.3477 - val_loss: 1.3753\n",
      "Epoch 55/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3476\n",
      "Epoch 55: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 22s 758us/sample - loss: 1.3476 - val_loss: 1.3764\n",
      "Epoch 56/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3448\n",
      "Epoch 56: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 22s 743us/sample - loss: 1.3448 - val_loss: 1.3767\n",
      "Epoch 57/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3441\n",
      "Epoch 57: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 22s 756us/sample - loss: 1.3441 - val_loss: 1.3759\n",
      "Epoch 58/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3407\n",
      "Epoch 58: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 22s 758us/sample - loss: 1.3407 - val_loss: 1.3764\n",
      "Epoch 59/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3441\n",
      "Epoch 59: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 22s 745us/sample - loss: 1.3441 - val_loss: 1.3749\n",
      "Epoch 60/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3429\n",
      "Epoch 60: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 22s 751us/sample - loss: 1.3429 - val_loss: 1.3785\n",
      "Epoch 61/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3388\n",
      "Epoch 61: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 23s 771us/sample - loss: 1.3388 - val_loss: 1.3743\n",
      "Epoch 62/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3361\n",
      "Epoch 62: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 22s 743us/sample - loss: 1.3361 - val_loss: 1.3694\n",
      "Epoch 63/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3384\n",
      "Epoch 63: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 22s 754us/sample - loss: 1.3384 - val_loss: 1.3741\n",
      "Epoch 64/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3391\n",
      "Epoch 64: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 22s 740us/sample - loss: 1.3391 - val_loss: 1.3737\n",
      "Epoch 65/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3365\n",
      "Epoch 65: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 22s 747us/sample - loss: 1.3365 - val_loss: 1.3743\n",
      "Epoch 66/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3370\n",
      "Epoch 66: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 23s 764us/sample - loss: 1.3370 - val_loss: 1.3782\n",
      "Epoch 67/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3360\n",
      "Epoch 67: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 22s 739us/sample - loss: 1.3360 - val_loss: 1.3749\n",
      "Epoch 68/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3324\n",
      "Epoch 68: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 22s 758us/sample - loss: 1.3324 - val_loss: 1.3710\n",
      "Epoch 69/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3350\n",
      "Epoch 69: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 23s 774us/sample - loss: 1.3350 - val_loss: 1.3717\n",
      "Epoch 70/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3352\n",
      "Epoch 70: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 22s 742us/sample - loss: 1.3352 - val_loss: 1.3764\n",
      "Epoch 71/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3359\n",
      "Epoch 71: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 23s 766us/sample - loss: 1.3359 - val_loss: 1.3709\n",
      "Epoch 72/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3356\n",
      "Epoch 72: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 22s 758us/sample - loss: 1.3356 - val_loss: 1.3739\n",
      "Epoch 73/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3322\n",
      "Epoch 73: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 22s 757us/sample - loss: 1.3322 - val_loss: 1.3733\n",
      "Epoch 74/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3353\n",
      "Epoch 74: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 22s 737us/sample - loss: 1.3353 - val_loss: 1.3748\n",
      "Epoch 75/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3344\n",
      "Epoch 75: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 22s 747us/sample - loss: 1.3344 - val_loss: 1.3818\n",
      "Epoch 76/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3325\n",
      "Epoch 76: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 22s 742us/sample - loss: 1.3325 - val_loss: 1.3742\n",
      "Epoch 77/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3311\n",
      "Epoch 77: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 22s 744us/sample - loss: 1.3311 - val_loss: 1.3753\n",
      "Epoch 78/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3307\n",
      "Epoch 78: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 22s 753us/sample - loss: 1.3307 - val_loss: 1.3676\n",
      "Epoch 79/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3288\n",
      "Epoch 79: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 22s 742us/sample - loss: 1.3288 - val_loss: 1.3785\n",
      "Epoch 80/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3287\n",
      "Epoch 80: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 23s 763us/sample - loss: 1.3287 - val_loss: 1.3724\n",
      "Epoch 81/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3301\n",
      "Epoch 81: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 22s 755us/sample - loss: 1.3301 - val_loss: 1.3806\n",
      "Epoch 82/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3285\n",
      "Epoch 82: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 22s 740us/sample - loss: 1.3285 - val_loss: 1.3714\n",
      "Epoch 83/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3295\n",
      "Epoch 83: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 21s 717us/sample - loss: 1.3295 - val_loss: 1.3730\n",
      "Epoch 84/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3282\n",
      "Epoch 84: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 22s 740us/sample - loss: 1.3282 - val_loss: 1.3710\n",
      "Epoch 85/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3275\n",
      "Epoch 85: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 23s 766us/sample - loss: 1.3275 - val_loss: 1.3720\n",
      "Epoch 86/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3273\n",
      "Epoch 86: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 22s 749us/sample - loss: 1.3273 - val_loss: 1.3739\n",
      "Epoch 87/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3271\n",
      "Epoch 87: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 22s 752us/sample - loss: 1.3271 - val_loss: 1.3711\n",
      "Epoch 88/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3281\n",
      "Epoch 88: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 22s 746us/sample - loss: 1.3281 - val_loss: 1.3740\n",
      "Epoch 89/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3291\n",
      "Epoch 89: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 22s 743us/sample - loss: 1.3291 - val_loss: 1.3791\n",
      "Epoch 90/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3289\n",
      "Epoch 90: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 22s 741us/sample - loss: 1.3289 - val_loss: 1.3761\n",
      "Epoch 91/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3260\n",
      "Epoch 91: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 22s 759us/sample - loss: 1.3260 - val_loss: 1.3724\n",
      "Epoch 92/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3332\n",
      "Epoch 92: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 23s 765us/sample - loss: 1.3332 - val_loss: 1.3703\n",
      "Epoch 93/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3276\n",
      "Epoch 93: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 22s 743us/sample - loss: 1.3276 - val_loss: 1.3720\n",
      "Epoch 94/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3244\n",
      "Epoch 94: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 21s 716us/sample - loss: 1.3244 - val_loss: 1.3743\n",
      "Epoch 95/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3230\n",
      "Epoch 95: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 23s 773us/sample - loss: 1.3230 - val_loss: 1.3762\n",
      "Epoch 96/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3247\n",
      "Epoch 96: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 22s 757us/sample - loss: 1.3247 - val_loss: 1.3677\n",
      "Epoch 97/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3225\n",
      "Epoch 97: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 24s 816us/sample - loss: 1.3225 - val_loss: 1.3727\n",
      "Epoch 98/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3225\n",
      "Epoch 98: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 21s 725us/sample - loss: 1.3225 - val_loss: 1.3738\n",
      "Epoch 99/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3232\n",
      "Epoch 99: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 23s 774us/sample - loss: 1.3232 - val_loss: 1.3765\n",
      "Epoch 100/100\n",
      "29601/29601 [==============================] - ETA: 0s - loss: 1.3251\n",
      "Epoch 100: val_loss did not improve from 1.36220\n",
      "29601/29601 [==============================] - 23s 779us/sample - loss: 1.3251 - val_loss: 1.3726\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import numpy as np\n",
    "\n",
    "class PruningCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, initial_pruning_factor=0.1, final_pruning_factor=0.5, start_epoch=0, end_epoch=None, frequency=1):\n",
    "        super(PruningCallback, self).__init__()\n",
    "        self.initial_pruning_factor = initial_pruning_factor\n",
    "        self.final_pruning_factor = final_pruning_factor\n",
    "        self.start_epoch = start_epoch\n",
    "        self.end_epoch = end_epoch if end_epoch is not None else np.inf\n",
    "        self.frequency = frequency\n",
    "        self.pruned_weights = {}\n",
    "        self.layer_importance = {}\n",
    "\n",
    "    def get_pruning_factor(self, epoch):\n",
    "        if epoch < self.start_epoch:\n",
    "            return 0\n",
    "        if epoch > self.end_epoch:\n",
    "            return self.final_pruning_factor\n",
    "        return self.initial_pruning_factor + (self.final_pruning_factor - self.initial_pruning_factor) * (epoch - self.start_epoch) / (self.end_epoch - self.start_epoch)\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        total_weight_magnitude = 0\n",
    "        for layer in self.model.layers:\n",
    "            if hasattr(layer, 'get_weights'):\n",
    "                weights = layer.get_weights()\n",
    "                layer_norm = sum(np.linalg.norm(w) for w in weights)\n",
    "                total_weight_magnitude += layer_norm\n",
    "                self.layer_importance[layer.name] = layer_norm\n",
    "    \n",
    "        # Normalize the layer importance values so they sum up to 1\n",
    "        for layer_name in self.layer_importance:\n",
    "            self.layer_importance[layer_name] /= total_weight_magnitude\n",
    "    # def on_train_begin(self, logs=None):\n",
    "    #     total_weight_magnitude = sum([np.linalg.norm(layer.get_weights()) for layer in self.model.layers if hasattr(layer, 'get_weights')])\n",
    "    #     for layer in self.model.layers:\n",
    "    #         if hasattr(layer, 'get_weights'):\n",
    "    #             self.layer_importance[layer.name] = np.linalg.norm(layer.get_weights()) / total_weight_magnitude\n",
    "\n",
    "    def get_layer_pruning_factor(self, layer_name, global_pruning_factor):\n",
    "        if layer_name in self.layer_importance:\n",
    "            importance = self.layer_importance[layer_name]\n",
    "            adjusted_pruning_factor = global_pruning_factor * (1 - importance)\n",
    "            return min(adjusted_pruning_factor, 1)  # Ensure the pruning factor is not greater than 1\n",
    "        return global_pruning_factor\n",
    "    def prune_weights(self, layer, global_pruning_factor):\n",
    "        \n",
    "        weights = layer.get_weights()\n",
    "        layer_name = layer.name\n",
    "        pruning_factor = self.get_layer_pruning_factor(layer_name, global_pruning_factor)\n",
    "\n",
    "        if layer_name not in self.pruned_weights:\n",
    "            self.pruned_weights[layer_name] = [np.zeros_like(w, dtype=bool) for w in weights]\n",
    "        \n",
    "        for i in range(len(weights)):\n",
    "            weight = weights[i]\n",
    "            # print(weight.shape)\n",
    "            # print(weight.size)\n",
    "            if weight.ndim > 1:  # Only prune dense or convolutional layers\n",
    "                unpruned_weights = np.logical_not(self.pruned_weights[layer_name][i])\n",
    "                num_unpruned = np.sum(unpruned_weights)\n",
    "                num_pruning = min(num_unpruned, int(weight.size * pruning_factor) - np.sum(self.pruned_weights[layer_name][i]))\n",
    "                num_pruning = int(weight.size * pruning_factor) - np.sum(self.pruned_weights[layer_name][i])\n",
    "                if num_pruning > 0:\n",
    "                    unpruned_flat_indices = np.flatnonzero(unpruned_weights)\n",
    "                    abs_unpruned_weights = np.abs(weight[unpruned_weights])\n",
    "                    pruning_flat_indices = np.argpartition(abs_unpruned_weights, num_pruning)[:num_pruning]\n",
    "                    \n",
    "                    indices = np.unravel_index(pruning_flat_indices, weight.shape)\n",
    "                    self.pruned_weights[layer_name][i][indices] = True\n",
    "\n",
    "                weights[i] = weights[i]*(~self.pruned_weights[layer_name][i])\n",
    "                \n",
    "        layer.set_weights(weights)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch - self.start_epoch) % self.frequency != 0:\n",
    "            return\n",
    "\n",
    "        pruning_factor = self.get_pruning_factor(epoch)\n",
    "        for layer in self.model.layers:\n",
    "            if isinstance(layer, (tf.keras.layers.Dense, tf.keras.layers.Conv2D)):\n",
    "                self.prune_weights(layer, pruning_factor)\n",
    "model = individual_model(X_train_normalized)\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "num_epochs = 100\n",
    "rates = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]\n",
    "idx = 0\n",
    "for r in rates:\n",
    "# Gradually prune weights in dense and convolutional layers from 10% to 50% over the course of training, starting from epoch 0.\n",
    "    model_name = './checkpoints/Adaptive_prune_model_remove_structure_'+str(idx)+'.h5'\n",
    "    idx = idx+1\n",
    "    checkpoint = ModelCheckpoint(model_name, monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
    "    pruning_callback = PruningCallback(initial_pruning_factor=0.01, final_pruning_factor=r, start_epoch=5, end_epoch=50, frequency=1)\n",
    "    model.fit(X_train_normalized_new, y_train, epochs=num_epochs, batch_size=1024, validation_data=(X_val_normalized_new, y_val), callbacks=[pruning_callback, checkpoint])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a228e32-f25a-42e7-9d27-035d6c02b74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python3.11/site-packages/tensorflow/python/ops/init_ops.py:94: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/lib/python3.11/site-packages/tensorflow/python/ops/init_ops.py:94: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/lib/python3.11/site-packages/tensorflow/python/ops/init_ops.py:94: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_16 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_17 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_18 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_19 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_20 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_22 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_23 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_24 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_25 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_26 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_27 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_28 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_29 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_30 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_31 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_32 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 23:49:39.718360: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-13 23:49:39.718576: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-13 23:49:39.718757: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-13 23:49:39.730127: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-13 23:49:39.730367: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-13 23:49:39.730661: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-13 23:49:39.730839: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-13 23:49:39.731007: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-13 23:49:39.731170: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-13 23:49:39.913441: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-13 23:49:39.913647: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-13 23:49:39.913843: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-13 23:49:39.914007: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-13 23:49:39.914161: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-13 23:49:39.914314: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-13 23:49:39.914466: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-13 23:49:39.914618: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-13 23:49:39.914777: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-13 23:49:39.924792: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-13 23:49:39.924984: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-13 23:49:39.925163: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-13 23:49:39.925332: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-13 23:49:39.925510: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-13 23:49:39.925677: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-13 23:49:39.925843: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-13 23:49:39.925991: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1325 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2023-12-13 23:49:39.926292: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-13 23:49:39.926438: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 46608 MB memory:  -> device: 1, name: NVIDIA RTX A6000, pci bus id: 0000:2d:00.0, compute capability: 8.6\n",
      "2023-12-13 23:49:39.926794: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-13 23:49:39.926942: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 46608 MB memory:  -> device: 2, name: NVIDIA RTX A6000, pci bus id: 0000:41:00.0, compute capability: 8.6\n",
      "2023-12-13 23:49:39.927417: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2023-12-13 23:49:41.632575: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-13 23:49:41.632888: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-13 23:49:41.633108: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-13 23:49:41.633307: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-13 23:49:41.633494: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-13 23:49:41.633680: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-13 23:49:41.633906: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-13 23:49:41.634085: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-13 23:49:41.634265: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-13 23:49:41.634510: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-13 23:49:41.634682: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-13 23:49:41.634849: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-13 23:49:41.635017: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-13 23:49:41.635156: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1325 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2023-12-13 23:49:41.635207: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-13 23:49:41.635342: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 46608 MB memory:  -> device: 1, name: NVIDIA RTX A6000, pci bus id: 0000:2d:00.0, compute capability: 8.6\n",
      "2023-12-13 23:49:41.635399: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-13 23:49:41.635534: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 46608 MB memory:  -> device: 2, name: NVIDIA RTX A6000, pci bus id: 0000:41:00.0, compute capability: 8.6\n",
      "2023-12-13 23:49:41.635581: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2023-12-13 23:49:41.678570: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n",
      "2023-12-13 23:49:41.757033: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_15/lstm_cell_15/recurrent_kernel/Assign' id:2597 op device:{requested: '', assigned: ''} def:{{{node lstm_15/lstm_cell_15/recurrent_kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_15/lstm_cell_15/recurrent_kernel, lstm_15/lstm_cell_15/recurrent_kernel/Initializer/mul_1)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-13 23:49:42.566203: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm/lstm_cell/kernel/v/Assign' id:6566 op device:{requested: '', assigned: ''} def:{{{node lstm/lstm_cell/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm/lstm_cell/kernel/v, lstm/lstm_cell/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "/usr/lib/python3.11/site-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "2023-12-13 23:49:43.215106: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_3/BiasAdd' id:5550 op device:{requested: '', assigned: ''} def:{{{node dense_3/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_3/MatMul, dense_3/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-13 23:49:43.699286: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE:\n",
      "1.3876348087659605\n",
      "1.3189926980382802\n"
     ]
    }
   ],
   "source": [
    "## Evaluation on test data\n",
    "import tensorflow as tf\n",
    "model = tf.keras.models.load_model(\"./checkpoints/Adaptive_prune_model_remove_structure_0.h5\")\n",
    "from feature_emotion import feature_extract, split_data, label_unique_tuples\n",
    "trace_wlk_num = label_unique_tuples(people_nums_all, walk_nums_all, trace_nums_all)\n",
    "trace_wlk_num = np.array(trace_wlk_num )\n",
    "\n",
    "y_pred = model.predict(X_test_normalized_new)\n",
    "print('Test MAE:')\n",
    "err = np.mean(np.abs(y_pred - y_test))\n",
    "print(err)\n",
    "## Evaluation: trace median vote\n",
    "trace_num_test = trace_wlk_num[test_idx]\n",
    "u = np.unique(trace_num_test)\n",
    "pred_trace = np.empty((0, 2))\n",
    "gt_trace = np.empty((0, 2))\n",
    "for i in u:\n",
    "  trace_idx = np.where(trace_num_test == i)[0]\n",
    "  y_pred_trace = y_pred[trace_idx,:]\n",
    "  pred = np.mean(y_pred_trace, axis = 0)\n",
    "  pred_trace = np.vstack((pred_trace, pred))\n",
    "  gt_t = y_test[trace_idx[0],:]\n",
    "  gt_trace = np.vstack((gt_trace, gt_t))\n",
    "print(np.mean(np.abs(pred_trace-gt_trace)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5eeae46-4092-438d-bb37-069db0c8f000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_16 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_17 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_18 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_19 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_20 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_22 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_23 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_24 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_25 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_26 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_27 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_28 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_29 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_30 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_31 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_32 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 23:50:05.027873: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_25_1/lstm_cell_58/recurrent_kernel/Assign' id:12163 op device:{requested: '', assigned: ''} def:{{{node lstm_25_1/lstm_cell_58/recurrent_kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_25_1/lstm_cell_58/recurrent_kernel, lstm_25_1/lstm_cell_58/recurrent_kernel/Initializer/mul_1)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-13 23:50:06.028028: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_2_1/lstm_cell_35/bias/v/Assign' id:14570 op device:{requested: '', assigned: ''} def:{{{node lstm_2_1/lstm_cell_35/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_2_1/lstm_cell_35/bias/v, lstm_2_1/lstm_cell_35/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-13 23:50:07.018425: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_3_1/BiasAdd' id:13514 op device:{requested: '', assigned: ''} def:{{{node dense_3_1/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_3_1/MatMul, dense_3_1/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE:\n",
      "1.3734996906966694\n",
      "1.291463675969627\n"
     ]
    }
   ],
   "source": [
    "## Evaluation on test data\n",
    "import tensorflow as tf\n",
    "model = tf.keras.models.load_model(\"./checkpoints/Adaptive_prune_model_remove_structure_1.h5\")\n",
    "from feature_emotion import feature_extract, split_data, label_unique_tuples\n",
    "trace_wlk_num = label_unique_tuples(people_nums_all, walk_nums_all, trace_nums_all)\n",
    "trace_wlk_num = np.array(trace_wlk_num )\n",
    "\n",
    "y_pred = model.predict(X_test_normalized_new)\n",
    "print('Test MAE:')\n",
    "err = np.mean(np.abs(y_pred - y_test))\n",
    "print(err)\n",
    "## Evaluation: trace median vote\n",
    "trace_num_test = trace_wlk_num[test_idx]\n",
    "u = np.unique(trace_num_test)\n",
    "pred_trace = np.empty((0, 2))\n",
    "gt_trace = np.empty((0, 2))\n",
    "for i in u:\n",
    "  trace_idx = np.where(trace_num_test == i)[0]\n",
    "  y_pred_trace = y_pred[trace_idx,:]\n",
    "  pred = np.mean(y_pred_trace, axis = 0)\n",
    "  pred_trace = np.vstack((pred_trace, pred))\n",
    "  gt_t = y_test[trace_idx[0],:]\n",
    "  gt_trace = np.vstack((gt_trace, gt_t))\n",
    "print(np.mean(np.abs(pred_trace-gt_trace)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5eb35601-3dea-4587-96c0-75f414b20e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_16 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_17 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_18 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_19 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_20 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_22 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_23 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_24 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_25 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_26 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_27 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_28 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_29 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_30 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_31 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_32 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 23:50:26.595043: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_16_2/lstm_cell_82/kernel/Assign' id:18667 op device:{requested: '', assigned: ''} def:{{{node lstm_16_2/lstm_cell_82/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_16_2/lstm_cell_82/kernel, lstm_16_2/lstm_cell_82/kernel/Initializer/random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-13 23:50:27.833699: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_15_2/lstm_cell_81/bias/m/Assign' id:22190 op device:{requested: '', assigned: ''} def:{{{node lstm_15_2/lstm_cell_81/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_15_2/lstm_cell_81/bias/m, lstm_15_2/lstm_cell_81/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-13 23:50:28.955978: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_3_2/BiasAdd' id:21478 op device:{requested: '', assigned: ''} def:{{{node dense_3_2/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_3_2/MatMul, dense_3_2/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE:\n",
      "1.367240384279781\n",
      "1.2701901964770939\n"
     ]
    }
   ],
   "source": [
    "## Evaluation on test data\n",
    "import tensorflow as tf\n",
    "model = tf.keras.models.load_model(\"./checkpoints/Adaptive_prune_model_remove_structure_2.h5\")\n",
    "from feature_emotion import feature_extract, split_data, label_unique_tuples\n",
    "trace_wlk_num = label_unique_tuples(people_nums_all, walk_nums_all, trace_nums_all)\n",
    "trace_wlk_num = np.array(trace_wlk_num )\n",
    "\n",
    "y_pred = model.predict(X_test_normalized_new)\n",
    "print('Test MAE:')\n",
    "err = np.mean(np.abs(y_pred - y_test))\n",
    "print(err)\n",
    "## Evaluation: trace median vote\n",
    "trace_num_test = trace_wlk_num[test_idx]\n",
    "u = np.unique(trace_num_test)\n",
    "pred_trace = np.empty((0, 2))\n",
    "gt_trace = np.empty((0, 2))\n",
    "for i in u:\n",
    "  trace_idx = np.where(trace_num_test == i)[0]\n",
    "  y_pred_trace = y_pred[trace_idx,:]\n",
    "  pred = np.mean(y_pred_trace, axis = 0)\n",
    "  pred_trace = np.vstack((pred_trace, pred))\n",
    "  gt_t = y_test[trace_idx[0],:]\n",
    "  gt_trace = np.vstack((gt_trace, gt_t))\n",
    "print(np.mean(np.abs(pred_trace-gt_trace)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b7ce306-f1f3-4b0d-a7f4-1f2305723a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_16 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_17 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_18 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_19 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_20 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_22 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_23 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_24 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_25 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_26 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_27 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_28 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_29 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_30 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_31 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_32 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 23:50:48.820878: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_17_3/lstm_cell_116/bias/Assign' id:26820 op device:{requested: '', assigned: ''} def:{{{node lstm_17_3/lstm_cell_116/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_17_3/lstm_cell_116/bias, lstm_17_3/lstm_cell_116/bias/Initializer/concat)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-13 23:50:50.377784: W tensorflow/c/c_api.cc:304] Operation '{name:'learning_rate_3/Assign' id:29914 op device:{requested: '', assigned: ''} def:{{{node learning_rate_3/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](learning_rate_3, learning_rate_3/Initializer/initial_value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-13 23:50:51.793211: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_3_3/BiasAdd' id:29442 op device:{requested: '', assigned: ''} def:{{{node dense_3_3/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_3_3/MatMul, dense_3_3/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE:\n",
      "1.362470541816432\n",
      "1.2677945683303389\n"
     ]
    }
   ],
   "source": [
    "## Evaluation on test data\n",
    "import tensorflow as tf\n",
    "model = tf.keras.models.load_model(\"./checkpoints/Adaptive_prune_model_remove_structure_3.h5\")\n",
    "from feature_emotion import feature_extract, split_data, label_unique_tuples\n",
    "trace_wlk_num = label_unique_tuples(people_nums_all, walk_nums_all, trace_nums_all)\n",
    "trace_wlk_num = np.array(trace_wlk_num )\n",
    "\n",
    "y_pred = model.predict(X_test_normalized_new)\n",
    "print('Test MAE:')\n",
    "err = np.mean(np.abs(y_pred - y_test))\n",
    "print(err)\n",
    "## Evaluation: trace median vote\n",
    "trace_num_test = trace_wlk_num[test_idx]\n",
    "u = np.unique(trace_num_test)\n",
    "pred_trace = np.empty((0, 2))\n",
    "gt_trace = np.empty((0, 2))\n",
    "for i in u:\n",
    "  trace_idx = np.where(trace_num_test == i)[0]\n",
    "  y_pred_trace = y_pred[trace_idx,:]\n",
    "  pred = np.mean(y_pred_trace, axis = 0)\n",
    "  pred_trace = np.vstack((pred_trace, pred))\n",
    "  gt_t = y_test[trace_idx[0],:]\n",
    "  gt_trace = np.vstack((gt_trace, gt_t))\n",
    "print(np.mean(np.abs(pred_trace-gt_trace)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74ca0acb-30d1-49fc-87bb-8adb7ad770a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_16 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_17 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_18 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_19 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_20 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_22 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_23 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_24 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_25 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_26 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_27 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_28 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_29 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_30 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_31 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_32 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 23:51:11.448330: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_25_4/lstm_cell_157/bias/Assign' id:36064 op device:{requested: '', assigned: ''} def:{{{node lstm_25_4/lstm_cell_157/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_25_4/lstm_cell_157/bias, lstm_25_4/lstm_cell_157/bias/Initializer/concat)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-13 23:51:13.310387: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_10_4/lstm_cell_142/recurrent_kernel/m/Assign' id:38038 op device:{requested: '', assigned: ''} def:{{{node lstm_10_4/lstm_cell_142/recurrent_kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_10_4/lstm_cell_142/recurrent_kernel/m, lstm_10_4/lstm_cell_142/recurrent_kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-13 23:51:15.047608: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_3_4/BiasAdd' id:37406 op device:{requested: '', assigned: ''} def:{{{node dense_3_4/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_3_4/MatMul, dense_3_4/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE:\n",
      "1.380407188956601\n",
      "1.2837988047152276\n"
     ]
    }
   ],
   "source": [
    "## Evaluation on test data\n",
    "import tensorflow as tf\n",
    "model = tf.keras.models.load_model(\"./checkpoints/Adaptive_prune_model_remove_structure_4.h5\")\n",
    "from feature_emotion import feature_extract, split_data, label_unique_tuples\n",
    "trace_wlk_num = label_unique_tuples(people_nums_all, walk_nums_all, trace_nums_all)\n",
    "trace_wlk_num = np.array(trace_wlk_num )\n",
    "\n",
    "y_pred = model.predict(X_test_normalized_new)\n",
    "print('Test MAE:')\n",
    "err = np.mean(np.abs(y_pred - y_test))\n",
    "print(err)\n",
    "## Evaluation: trace median vote\n",
    "trace_num_test = trace_wlk_num[test_idx]\n",
    "u = np.unique(trace_num_test)\n",
    "pred_trace = np.empty((0, 2))\n",
    "gt_trace = np.empty((0, 2))\n",
    "for i in u:\n",
    "  trace_idx = np.where(trace_num_test == i)[0]\n",
    "  y_pred_trace = y_pred[trace_idx,:]\n",
    "  pred = np.mean(y_pred_trace, axis = 0)\n",
    "  pred_trace = np.vstack((pred_trace, pred))\n",
    "  gt_t = y_test[trace_idx[0],:]\n",
    "  gt_trace = np.vstack((gt_trace, gt_t))\n",
    "print(np.mean(np.abs(pred_trace-gt_trace)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47bdbd01-df81-4ac4-a009-b930eb31972f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_16 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_17 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_18 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_19 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_20 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_22 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_23 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_24 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_25 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_26 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_27 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_28 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_29 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_30 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_31 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_32 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 23:51:35.803839: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_2_5/bias/Assign' id:45344 op device:{requested: '', assigned: ''} def:{{{node dense_2_5/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_2_5/bias, dense_2_5/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-13 23:51:37.991230: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_12_5/lstm_cell_177/recurrent_kernel/m/Assign' id:46032 op device:{requested: '', assigned: ''} def:{{{node lstm_12_5/lstm_cell_177/recurrent_kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_12_5/lstm_cell_177/recurrent_kernel/m, lstm_12_5/lstm_cell_177/recurrent_kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-13 23:51:40.051003: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_3_5/BiasAdd' id:45370 op device:{requested: '', assigned: ''} def:{{{node dense_3_5/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_3_5/MatMul, dense_3_5/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE:\n",
      "1.3865868734560718\n",
      "1.2890272373520442\n"
     ]
    }
   ],
   "source": [
    "## Evaluation on test data\n",
    "import tensorflow as tf\n",
    "model = tf.keras.models.load_model(\"./checkpoints/Adaptive_prune_model_remove_structure_5.h5\")\n",
    "from feature_emotion import feature_extract, split_data, label_unique_tuples\n",
    "trace_wlk_num = label_unique_tuples(people_nums_all, walk_nums_all, trace_nums_all)\n",
    "trace_wlk_num = np.array(trace_wlk_num )\n",
    "\n",
    "y_pred = model.predict(X_test_normalized_new)\n",
    "print('Test MAE:')\n",
    "err = np.mean(np.abs(y_pred - y_test))\n",
    "print(err)\n",
    "## Evaluation: trace median vote\n",
    "trace_num_test = trace_wlk_num[test_idx]\n",
    "u = np.unique(trace_num_test)\n",
    "pred_trace = np.empty((0, 2))\n",
    "gt_trace = np.empty((0, 2))\n",
    "for i in u:\n",
    "  trace_idx = np.where(trace_num_test == i)[0]\n",
    "  y_pred_trace = y_pred[trace_idx,:]\n",
    "  pred = np.mean(y_pred_trace, axis = 0)\n",
    "  pred_trace = np.vstack((pred_trace, pred))\n",
    "  gt_t = y_test[trace_idx[0],:]\n",
    "  gt_trace = np.vstack((gt_trace, gt_t))\n",
    "print(np.mean(np.abs(pred_trace-gt_trace)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2273b6e0-0f02-486c-949b-486ad46e31e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_16 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_17 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_18 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_19 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_20 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_22 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_23 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_24 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_25 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_26 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_27 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_28 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_29 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_30 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_31 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_32 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 23:52:00.604280: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_26_6/lstm_cell_224/bias/Assign' id:52152 op device:{requested: '', assigned: ''} def:{{{node lstm_26_6/lstm_cell_224/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_26_6/lstm_cell_224/bias, lstm_26_6/lstm_cell_224/bias/Initializer/concat)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-13 23:52:03.116829: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_6_6/lstm_cell_204/kernel/v/Assign' id:54440 op device:{requested: '', assigned: ''} def:{{{node lstm_6_6/lstm_cell_204/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_6_6/lstm_cell_204/kernel/v, lstm_6_6/lstm_cell_204/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-13 23:52:05.543548: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_3_6/BiasAdd' id:53334 op device:{requested: '', assigned: ''} def:{{{node dense_3_6/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_3_6/MatMul, dense_3_6/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE:\n",
      "1.3819669904045586\n",
      "1.282882933053384\n"
     ]
    }
   ],
   "source": [
    "## Evaluation on test data\n",
    "import tensorflow as tf\n",
    "model = tf.keras.models.load_model(\"./checkpoints/Adaptive_prune_model_remove_structure_6.h5\")\n",
    "from feature_emotion import feature_extract, split_data, label_unique_tuples\n",
    "trace_wlk_num = label_unique_tuples(people_nums_all, walk_nums_all, trace_nums_all)\n",
    "trace_wlk_num = np.array(trace_wlk_num )\n",
    "\n",
    "y_pred = model.predict(X_test_normalized_new)\n",
    "print('Test MAE:')\n",
    "err = np.mean(np.abs(y_pred - y_test))\n",
    "print(err)\n",
    "## Evaluation: trace median vote\n",
    "trace_num_test = trace_wlk_num[test_idx]\n",
    "u = np.unique(trace_num_test)\n",
    "pred_trace = np.empty((0, 2))\n",
    "gt_trace = np.empty((0, 2))\n",
    "for i in u:\n",
    "  trace_idx = np.where(trace_num_test == i)[0]\n",
    "  y_pred_trace = y_pred[trace_idx,:]\n",
    "  pred = np.mean(y_pred_trace, axis = 0)\n",
    "  pred_trace = np.vstack((pred_trace, pred))\n",
    "  gt_t = y_test[trace_idx[0],:]\n",
    "  gt_trace = np.vstack((gt_trace, gt_t))\n",
    "print(np.mean(np.abs(pred_trace-gt_trace)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84172597-05f6-43f6-8b0f-255b41ee4b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_16 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_17 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_18 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_19 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_20 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_22 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_23 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_24 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_25 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_26 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_27 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_28 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_29 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_30 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_31 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_32 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 23:52:29.238051: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_19_7/lstm_cell_250/kernel/Assign' id:58967 op device:{requested: '', assigned: ''} def:{{{node lstm_19_7/lstm_cell_250/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_19_7/lstm_cell_250/kernel, lstm_19_7/lstm_cell_250/kernel/Initializer/random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-13 23:52:32.112088: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_17_7/lstm_cell_248/bias/m/Assign' id:62040 op device:{requested: '', assigned: ''} def:{{{node lstm_17_7/lstm_cell_248/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_17_7/lstm_cell_248/bias/m, lstm_17_7/lstm_cell_248/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-13 23:52:34.863248: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_3_7/BiasAdd' id:61298 op device:{requested: '', assigned: ''} def:{{{node dense_3_7/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_3_7/MatMul, dense_3_7/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE:\n",
      "1.3838351798150839\n",
      "1.281988346808165\n"
     ]
    }
   ],
   "source": [
    "## Evaluation on test data\n",
    "import tensorflow as tf\n",
    "model = tf.keras.models.load_model(\"./checkpoints/Adaptive_prune_model_remove_structure_7.h5\")\n",
    "from feature_emotion import feature_extract, split_data, label_unique_tuples\n",
    "trace_wlk_num = label_unique_tuples(people_nums_all, walk_nums_all, trace_nums_all)\n",
    "trace_wlk_num = np.array(trace_wlk_num )\n",
    "\n",
    "y_pred = model.predict(X_test_normalized_new)\n",
    "print('Test MAE:')\n",
    "err = np.mean(np.abs(y_pred - y_test))\n",
    "print(err)\n",
    "## Evaluation: trace median vote\n",
    "trace_num_test = trace_wlk_num[test_idx]\n",
    "u = np.unique(trace_num_test)\n",
    "pred_trace = np.empty((0, 2))\n",
    "gt_trace = np.empty((0, 2))\n",
    "for i in u:\n",
    "  trace_idx = np.where(trace_num_test == i)[0]\n",
    "  y_pred_trace = y_pred[trace_idx,:]\n",
    "  pred = np.mean(y_pred_trace, axis = 0)\n",
    "  pred_trace = np.vstack((pred_trace, pred))\n",
    "  gt_t = y_test[trace_idx[0],:]\n",
    "  gt_trace = np.vstack((gt_trace, gt_t))\n",
    "print(np.mean(np.abs(pred_trace-gt_trace)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15908504-090a-4a34-943b-4701d10c82c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_16 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_17 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_18 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_19 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_20 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_22 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_23 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_24 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_25 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_26 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_27 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_28 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_29 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_30 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_31 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_32 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 23:52:56.405388: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_30_8/lstm_cell_294/bias/Assign' id:68720 op device:{requested: '', assigned: ''} def:{{{node lstm_30_8/lstm_cell_294/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_30_8/lstm_cell_294/bias, lstm_30_8/lstm_cell_294/bias/Initializer/concat)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-13 23:52:59.631233: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_18_8/lstm_cell_282/kernel/v/Assign' id:70548 op device:{requested: '', assigned: ''} def:{{{node lstm_18_8/lstm_cell_282/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_18_8/lstm_cell_282/kernel/v, lstm_18_8/lstm_cell_282/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-13 23:53:02.726561: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_3_8/BiasAdd' id:69262 op device:{requested: '', assigned: ''} def:{{{node dense_3_8/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_3_8/MatMul, dense_3_8/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE:\n",
      "1.3859522966471995\n",
      "1.2864678630936879\n"
     ]
    }
   ],
   "source": [
    "## Evaluation on test data\n",
    "import tensorflow as tf\n",
    "model = tf.keras.models.load_model(\"./checkpoints/Adaptive_prune_model_remove_structure_8.h5\")\n",
    "from feature_emotion import feature_extract, split_data, label_unique_tuples\n",
    "trace_wlk_num = label_unique_tuples(people_nums_all, walk_nums_all, trace_nums_all)\n",
    "trace_wlk_num = np.array(trace_wlk_num )\n",
    "\n",
    "y_pred = model.predict(X_test_normalized_new)\n",
    "print('Test MAE:')\n",
    "err = np.mean(np.abs(y_pred - y_test))\n",
    "print(err)\n",
    "## Evaluation: trace median vote\n",
    "trace_num_test = trace_wlk_num[test_idx]\n",
    "u = np.unique(trace_num_test)\n",
    "pred_trace = np.empty((0, 2))\n",
    "gt_trace = np.empty((0, 2))\n",
    "for i in u:\n",
    "  trace_idx = np.where(trace_num_test == i)[0]\n",
    "  y_pred_trace = y_pred[trace_idx,:]\n",
    "  pred = np.mean(y_pred_trace, axis = 0)\n",
    "  pred_trace = np.vstack((pred_trace, pred))\n",
    "  gt_t = y_test[trace_idx[0],:]\n",
    "  gt_trace = np.vstack((gt_trace, gt_t))\n",
    "print(np.mean(np.abs(pred_trace-gt_trace)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e30ccff-4a4b-473c-99d6-0919463cadfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_16 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_17 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_18 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_19 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_20 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_22 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_23 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_24 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_25 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_26 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_27 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_28 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_29 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_30 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_31 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_32 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 23:53:30.638919: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_3_9/kernel/Assign' id:77216 op device:{requested: '', assigned: ''} def:{{{node dense_3_9/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_3_9/kernel, dense_3_9/kernel/Initializer/random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-13 23:53:34.212660: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_15_9/lstm_cell_312/kernel/v/Assign' id:78467 op device:{requested: '', assigned: ''} def:{{{node lstm_15_9/lstm_cell_312/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_15_9/lstm_cell_312/kernel/v, lstm_15_9/lstm_cell_312/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-13 23:53:37.704091: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_3_9/BiasAdd' id:77226 op device:{requested: '', assigned: ''} def:{{{node dense_3_9/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_3_9/MatMul, dense_3_9/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE:\n",
      "1.3670312915164202\n",
      "1.2914855369857985\n"
     ]
    }
   ],
   "source": [
    "## Evaluation on test data\n",
    "import tensorflow as tf\n",
    "model = tf.keras.models.load_model(\"./checkpoints/Adaptive_prune_model_remove_structure_9.h5\")\n",
    "from feature_emotion import feature_extract, split_data, label_unique_tuples\n",
    "trace_wlk_num = label_unique_tuples(people_nums_all, walk_nums_all, trace_nums_all)\n",
    "trace_wlk_num = np.array(trace_wlk_num )\n",
    "\n",
    "y_pred = model.predict(X_test_normalized_new)\n",
    "print('Test MAE:')\n",
    "err = np.mean(np.abs(y_pred - y_test))\n",
    "print(err)\n",
    "## Evaluation: trace median vote\n",
    "trace_num_test = trace_wlk_num[test_idx]\n",
    "u = np.unique(trace_num_test)\n",
    "pred_trace = np.empty((0, 2))\n",
    "gt_trace = np.empty((0, 2))\n",
    "for i in u:\n",
    "  trace_idx = np.where(trace_num_test == i)[0]\n",
    "  y_pred_trace = y_pred[trace_idx,:]\n",
    "  pred = np.mean(y_pred_trace, axis = 0)\n",
    "  pred_trace = np.vstack((pred_trace, pred))\n",
    "  gt_t = y_test[trace_idx[0],:]\n",
    "  gt_trace = np.vstack((gt_trace, gt_t))\n",
    "print(np.mean(np.abs(pred_trace-gt_trace)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabd295d-3e4d-437e-9751-02b67d6143f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
