{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c77f8da1-2837-4bf7-b3f4-dfcaa7ab42f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T02:51:28.585954Z",
     "iopub.status.busy": "2023-12-05T02:51:28.585843Z",
     "iopub.status.idle": "2023-12-05T02:51:28.677304Z",
     "shell.execute_reply": "2023-12-05T02:51:28.676711Z"
    }
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fe25376-b856-4296-ba02-5e3771ff9451",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T02:51:28.680048Z",
     "iopub.status.busy": "2023-12-05T02:51:28.679738Z",
     "iopub.status.idle": "2023-12-05T02:52:21.696992Z",
     "shell.execute_reply": "2023-12-05T02:52:21.696382Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 21:51:28.847131: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/lib/python3.11/site-packages/h5py/__init__.py:36: UserWarning: h5py is running against HDF5 1.14.2 when it was built against 1.14.1, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35309\n",
      "WARNING:tensorflow:From /usr/lib/python3.11/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "person_nums = [1,2,4,5,6,8,9,10,11,12,13,17,19,21,22,25,26,27,28,29]\n",
    "## Build the baseline model for emotion recognition with dropout layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, concatenate, SimpleRNN, LSTM, Conv2D, Flatten, MaxPooling2D, GRU, AveragePooling2D, Dropout, Lambda\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from feature_emotion import feature_extract, split_data, label_unique_tuples, split_data_unknown, split_data_few_shot\n",
    "gts, sensor_nums, walk_nums, trace_nums, people_nums, spe_centr, delta_spe_centr, spe_crest, delta_spe_crest, spe_decrease, delta_spe_decrease, spe_entropy, delta_spe_entropy, spe_flatness, delta_spe_flatness, spe_flux, delta_spe_flux, spe_kurtosis, delta_spe_kurtosis, spe_skewness, delta_spe_skewness, spe_rfp, delta_spe_rfp, spe_slope, delta_spe_slope, spe_spread, delta_spe_spread, wlk_fre, wlk_fres_trace, cwt_figs_all, cwt_sum_all_0, cwt_sum_all_1, cwt_sum_all_2, cwt_sum_all_3, high_fre_compos, pitchs, low_fre_compos, auto_corrs, real_hils, imag_hils, dur_time_1_alls, dur_time_2_alls, jitters, shimmers, jitter_rap, hrs, feature_combine, slope, zcrs, fft_features, energy_alls, log_energy_alls, smoothe_energy_alls, legendres, double_support_time, pdps_new, lpcs, ceps_features = feature_extract(person_nums)\n",
    "\n",
    "walk_nums_all = np.squeeze(walk_nums)\n",
    "trace_nums_all = np.squeeze(trace_nums)\n",
    "people_nums_all = np.squeeze(people_nums)\n",
    "ra = 0.1\n",
    "test_person_id = [28]\n",
    "flag_tr_val_te = split_data_few_shot(test_person_id, walk_nums_all, trace_nums_all, people_nums_all, person_nums, ratio = ra, rand_seed = 42)\n",
    "## Data Normalization before training ans testing\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_v2_behavior()\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, concatenate, LSTM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scalers = []\n",
    "X_train_normalized = []\n",
    "X_val_normalized = []\n",
    "X_test_normalized = []\n",
    "train_idx = np.where(flag_tr_val_te ==0)[0]\n",
    "np.random.shuffle(train_idx)\n",
    "val_idx = np.where(flag_tr_val_te ==1)[0]\n",
    "test_idx = np.where(flag_tr_val_te ==2)[0]\n",
    "\n",
    "for i, feature in enumerate([spe_centr, delta_spe_centr, spe_crest, delta_spe_crest, spe_decrease, delta_spe_decrease, spe_entropy, delta_spe_entropy, spe_flatness, delta_spe_flatness, spe_flux, delta_spe_flux, spe_kurtosis, delta_spe_kurtosis, spe_skewness, delta_spe_skewness, spe_rfp, delta_spe_rfp, spe_slope, delta_spe_slope, spe_spread, delta_spe_spread, wlk_fre, wlk_fres_trace, cwt_figs_all, cwt_sum_all_0, cwt_sum_all_1, cwt_sum_all_2, cwt_sum_all_3, high_fre_compos, pitchs, low_fre_compos, auto_corrs, real_hils, imag_hils, dur_time_1_alls, dur_time_2_alls, jitters, shimmers, jitter_rap, hrs, feature_combine, slope, zcrs, fft_features, energy_alls, log_energy_alls, smoothe_energy_alls, legendres, double_support_time, pdps_new, lpcs, ceps_features]):\n",
    "    scaler = StandardScaler()\n",
    "    if len(feature.shape)==2:\n",
    "        X_train_i = feature[train_idx,:]\n",
    "        X_val_i = feature[val_idx,:]\n",
    "        X_test_i = feature[test_idx,:]\n",
    "        X_train_normalized_i = scaler.fit_transform(X_train_i)\n",
    "        X_val_normalized_i = scaler.transform(X_val_i)\n",
    "        X_test_normalized_i = scaler.transform(X_test_i)\n",
    "        scalers.append(scaler)\n",
    "    else:\n",
    "        X_train_i = feature[train_idx,:,:]\n",
    "        X_val_i = feature[val_idx,:,:]\n",
    "        X_test_i = feature[test_idx,:,:]\n",
    "        X_train_normalized_i = scaler.fit_transform(X_train_i.reshape(X_train_i.shape[0], -1)).reshape(X_train_i.shape)\n",
    "        X_val_normalized_i = scaler.transform(X_val_i.reshape(X_val_i.shape[0], -1)).reshape(X_val_i.shape)\n",
    "        X_test_normalized_i = scaler.transform(X_test_i.reshape(X_test_i.shape[0], -1)).reshape(X_test_i.shape)\n",
    "        scalers.append(scaler)\n",
    "    X_train_normalized.append(X_train_normalized_i)\n",
    "    X_val_normalized.append(X_val_normalized_i)\n",
    "    X_test_normalized.append(X_test_normalized_i)\n",
    "y_train = gts[train_idx,:]\n",
    "y_val = gts[val_idx,:]\n",
    "y_test = gts[test_idx,:]\n",
    "X_train_normalized_new = []\n",
    "combined_feature = np.empty((len(X_train_normalized[0]),0))\n",
    "for feature in X_train_normalized:\n",
    "    if len(feature.shape) == 3:\n",
    "        X_train_normalized_new.append(feature)\n",
    "    elif feature.shape[1] <20:\n",
    "        combined_feature = np.hstack((combined_feature, feature))\n",
    "    else:\n",
    "        X_train_normalized_new.append(feature)\n",
    "X_train_normalized_new.append(combined_feature)\n",
    "\n",
    "X_val_normalized_new = []\n",
    "combined_feature = np.empty((len(X_val_normalized[0]),0))\n",
    "for feature in X_val_normalized:\n",
    "    if len(feature.shape) == 3:\n",
    "        X_val_normalized_new.append(feature)\n",
    "    elif feature.shape[1] <20:\n",
    "        combined_feature = np.hstack((combined_feature, feature))\n",
    "    else:\n",
    "        X_val_normalized_new.append(feature)\n",
    "X_val_normalized_new.append(combined_feature)\n",
    "\n",
    "X_test_normalized_new = []\n",
    "combined_feature = np.empty((len(X_test_normalized[0]),0))\n",
    "for feature in X_test_normalized:\n",
    "    if len(feature.shape) == 3:\n",
    "        X_test_normalized_new.append(feature)\n",
    "    elif feature.shape[1] <20:\n",
    "        combined_feature = np.hstack((combined_feature, feature))\n",
    "    else:\n",
    "        X_test_normalized_new.append(feature)\n",
    "X_test_normalized_new.append(combined_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0508e9a1-9544-41f8-af41-e3d3b02a8d80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T02:52:21.699398Z",
     "iopub.status.busy": "2023-12-05T02:52:21.698878Z",
     "iopub.status.idle": "2023-12-05T02:53:07.400280Z",
     "shell.execute_reply": "2023-12-05T02:53:07.399684Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python3.11/site-packages/tensorflow/python/ops/init_ops.py:94: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/lib/python3.11/site-packages/tensorflow/python/ops/init_ops.py:94: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/lib/python3.11/site-packages/tensorflow/python/ops/init_ops.py:94: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_16 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_17 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_18 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_19 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_20 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_22 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_23 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_24 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_25 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_26 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_27 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_28 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_29 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_30 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_31 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_32 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_33 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_34 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_35 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_36 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 21:52:51.821988: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-04 21:52:51.833599: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-04 21:52:51.833844: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-04 21:52:51.837406: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-04 21:52:51.837601: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-04 21:52:51.837774: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-04 21:52:51.919606: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-04 21:52:51.919817: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-04 21:52:51.919993: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-04 21:52:51.920135: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46608 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:2d:00.0, compute capability: 8.6\n",
      "2023-12-04 21:52:51.920493: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2023-12-04 21:52:53.782271: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-04 21:52:53.782559: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-04 21:52:53.782759: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-04 21:52:53.782989: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-04 21:52:53.783175: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-04 21:52:53.783319: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46608 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:2d:00.0, compute capability: 8.6\n",
      "2023-12-04 21:52:53.783353: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2023-12-04 21:52:53.831885: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n",
      "2023-12-04 21:52:54.082761: W tensorflow/c/c_api.cc:304] Operation '{name:'conv2d_3/bias/Assign' id:175 op device:{requested: '', assigned: ''} def:{{{node conv2d_3/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](conv2d_3/bias, conv2d_3/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-04 21:52:55.035350: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_33/lstm_cell_33/kernel/v/Assign' id:8106 op device:{requested: '', assigned: ''} def:{{{node lstm_33/lstm_cell_33/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_33/lstm_cell_33/kernel/v, lstm_33/lstm_cell_33/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "/usr/lib/python3.11/site-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "2023-12-04 21:52:55.799066: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_3/BiasAdd' id:6369 op device:{requested: '', assigned: ''} def:{{{node dense_3/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_3/MatMul, dense_3/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-04 21:52:56.360491: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-12-04 21:52:56.381014: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE:\n",
      "1.6283559065761295\n",
      "[1.46945725 1.69245263]\n",
      "1.5809549406065162\n"
     ]
    }
   ],
   "source": [
    "## Evaluation on test data\n",
    "import tensorflow as tf\n",
    "m_name = \"./checkpoints/unknown_person_few_shot_p\"+str(test_person_id[0])+'_30.h5'\n",
    "model = tf.keras.models.load_model(m_name)\n",
    "from feature_emotion import feature_extract, split_data, label_unique_tuples\n",
    "trace_wlk_num = label_unique_tuples(people_nums_all, walk_nums_all, trace_nums_all)\n",
    "trace_wlk_num = np.array(trace_wlk_num )\n",
    "\n",
    "y_pred = model.predict(X_test_normalized_new)\n",
    "print('Test MAE:')\n",
    "err = np.mean(np.abs(y_pred - y_test))\n",
    "print(err)\n",
    "## Evaluation: trace median vote\n",
    "trace_num_test = trace_wlk_num[test_idx]\n",
    "u = np.unique(trace_num_test)\n",
    "pred_trace = np.empty((0, 2))\n",
    "gt_trace = np.empty((0, 2))\n",
    "for i in u:\n",
    "  trace_idx = np.where(trace_num_test == i)[0]\n",
    "  y_pred_trace = y_pred[trace_idx,:]\n",
    "  pred = np.mean(y_pred_trace, axis = 0)\n",
    "  pred_trace = np.vstack((pred_trace, pred))\n",
    "  gt_t = y_test[trace_idx[0],:]\n",
    "  gt_trace = np.vstack((gt_trace, gt_t))\n",
    "print(np.mean(np.abs(pred_trace-gt_trace), axis = 0))\n",
    "print(np.mean(np.abs(pred_trace-gt_trace)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f71b59e4-6581-4ea8-994d-a02a6654ab6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T02:53:07.402180Z",
     "iopub.status.busy": "2023-12-05T02:53:07.402039Z",
     "iopub.status.idle": "2023-12-05T02:53:22.688737Z",
     "shell.execute_reply": "2023-12-05T02:53:22.688184Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_37 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_38 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_39 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_40 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_41 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_42 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_43 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_44 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_45 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_46 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_47 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_48 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_49 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_50 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_51 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_52 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_53 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_54 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_55 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_56 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_57 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_58 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_59 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_60 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_61 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_62 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_63 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_64 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_65 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_66 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_67 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_68 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_69 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_70 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_71 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_72 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_73 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 21:53:09.458266: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_49/lstm_cell_49/bias/Assign' id:11536 op device:{requested: '', assigned: ''} def:{{{node lstm_49/lstm_cell_49/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_49/lstm_cell_49/bias, lstm_49/lstm_cell_49/bias/Initializer/concat)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-04 21:53:10.791673: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_55/lstm_cell_55/kernel/m/Assign' id:16469 op device:{requested: '', assigned: ''} def:{{{node lstm_55/lstm_cell_55/kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_55/lstm_cell_55/kernel/m, lstm_55/lstm_cell_55/kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-04 21:53:11.804408: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_7/BiasAdd' id:15600 op device:{requested: '', assigned: ''} def:{{{node dense_7/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_7/MatMul, dense_7/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE:\n",
      "1.6093876049380493\n",
      "[1.42162782 1.70497491]\n",
      "1.5633013662717021\n"
     ]
    }
   ],
   "source": [
    "## Evaluation on test data\n",
    "import tensorflow as tf\n",
    "m_name = \"./checkpoints/unknown_person_few_shot_p\"+str(test_person_id[0])+'_31.h5'\n",
    "model = tf.keras.models.load_model(m_name)\n",
    "from feature_emotion import feature_extract, split_data, label_unique_tuples\n",
    "trace_wlk_num = label_unique_tuples(people_nums_all, walk_nums_all, trace_nums_all)\n",
    "trace_wlk_num = np.array(trace_wlk_num )\n",
    "\n",
    "y_pred = model.predict(X_test_normalized_new)\n",
    "print('Test MAE:')\n",
    "err = np.mean(np.abs(y_pred - y_test))\n",
    "print(err)\n",
    "## Evaluation: trace median vote\n",
    "trace_num_test = trace_wlk_num[test_idx]\n",
    "u = np.unique(trace_num_test)\n",
    "pred_trace = np.empty((0, 2))\n",
    "gt_trace = np.empty((0, 2))\n",
    "for i in u:\n",
    "  trace_idx = np.where(trace_num_test == i)[0]\n",
    "  y_pred_trace = y_pred[trace_idx,:]\n",
    "  pred = np.mean(y_pred_trace, axis = 0)\n",
    "  pred_trace = np.vstack((pred_trace, pred))\n",
    "  gt_t = y_test[trace_idx[0],:]\n",
    "  gt_trace = np.vstack((gt_trace, gt_t))\n",
    "print(np.mean(np.abs(pred_trace-gt_trace), axis = 0))\n",
    "print(np.mean(np.abs(pred_trace-gt_trace)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c06b7a97-b340-46f4-80a5-34e9e8446d85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T02:53:22.690770Z",
     "iopub.status.busy": "2023-12-05T02:53:22.690614Z",
     "iopub.status.idle": "2023-12-05T02:53:40.102627Z",
     "shell.execute_reply": "2023-12-05T02:53:40.102041Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_74 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_75 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_76 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_77 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_78 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_79 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_80 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_81 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_82 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_83 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_84 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_85 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_86 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_87 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_88 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_89 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_90 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_91 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_92 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_93 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_94 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_95 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_96 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_97 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_98 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_99 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_100 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_101 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_102 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_103 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_104 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_105 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_106 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_107 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_108 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_109 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_110 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 21:53:24.880738: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_91/lstm_cell_91/bias/Assign' id:21567 op device:{requested: '', assigned: ''} def:{{{node lstm_91/lstm_cell_91/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_91/lstm_cell_91/bias, lstm_91/lstm_cell_91/bias/Initializer/concat)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-04 21:53:26.373810: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_100/lstm_cell_100/bias/v/Assign' id:26473 op device:{requested: '', assigned: ''} def:{{{node lstm_100/lstm_cell_100/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_100/lstm_cell_100/bias/v, lstm_100/lstm_cell_100/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-04 21:53:27.709865: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_11/BiasAdd' id:24831 op device:{requested: '', assigned: ''} def:{{{node dense_11/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_11/MatMul, dense_11/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE:\n",
      "1.6228310815972358\n",
      "[1.44653385 1.71321228]\n",
      "1.5798730655764857\n"
     ]
    }
   ],
   "source": [
    "## Evaluation on test data\n",
    "import tensorflow as tf\n",
    "m_name = \"./checkpoints/unknown_person_few_shot_p\"+str(test_person_id[0])+'_32.h5'\n",
    "model = tf.keras.models.load_model(m_name)\n",
    "from feature_emotion import feature_extract, split_data, label_unique_tuples\n",
    "trace_wlk_num = label_unique_tuples(people_nums_all, walk_nums_all, trace_nums_all)\n",
    "trace_wlk_num = np.array(trace_wlk_num )\n",
    "\n",
    "y_pred = model.predict(X_test_normalized_new)\n",
    "print('Test MAE:')\n",
    "err = np.mean(np.abs(y_pred - y_test))\n",
    "print(err)\n",
    "## Evaluation: trace median vote\n",
    "trace_num_test = trace_wlk_num[test_idx]\n",
    "u = np.unique(trace_num_test)\n",
    "pred_trace = np.empty((0, 2))\n",
    "gt_trace = np.empty((0, 2))\n",
    "for i in u:\n",
    "  trace_idx = np.where(trace_num_test == i)[0]\n",
    "  y_pred_trace = y_pred[trace_idx,:]\n",
    "  pred = np.mean(y_pred_trace, axis = 0)\n",
    "  pred_trace = np.vstack((pred_trace, pred))\n",
    "  gt_t = y_test[trace_idx[0],:]\n",
    "  gt_trace = np.vstack((gt_trace, gt_t))\n",
    "print(np.mean(np.abs(pred_trace-gt_trace), axis = 0))\n",
    "print(np.mean(np.abs(pred_trace-gt_trace)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be46bcda-efc4-42fe-8dce-4440f02fbbda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T02:53:40.104582Z",
     "iopub.status.busy": "2023-12-05T02:53:40.104447Z",
     "iopub.status.idle": "2023-12-05T02:53:45.586476Z",
     "shell.execute_reply": "2023-12-05T02:53:45.585883Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35501\n"
     ]
    }
   ],
   "source": [
    "ra = 0.2\n",
    "\n",
    "flag_tr_val_te = split_data_few_shot(test_person_id, walk_nums_all, trace_nums_all, people_nums_all, person_nums, ratio = ra, rand_seed = 42)\n",
    "## Data Normalization before training ans testing\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_v2_behavior()\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, concatenate, LSTM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scalers = []\n",
    "X_train_normalized = []\n",
    "X_val_normalized = []\n",
    "X_test_normalized = []\n",
    "train_idx = np.where(flag_tr_val_te ==0)[0]\n",
    "np.random.shuffle(train_idx)\n",
    "val_idx = np.where(flag_tr_val_te ==1)[0]\n",
    "test_idx = np.where(flag_tr_val_te ==2)[0]\n",
    "\n",
    "for i, feature in enumerate([spe_centr, delta_spe_centr, spe_crest, delta_spe_crest, spe_decrease, delta_spe_decrease, spe_entropy, delta_spe_entropy, spe_flatness, delta_spe_flatness, spe_flux, delta_spe_flux, spe_kurtosis, delta_spe_kurtosis, spe_skewness, delta_spe_skewness, spe_rfp, delta_spe_rfp, spe_slope, delta_spe_slope, spe_spread, delta_spe_spread, wlk_fre, wlk_fres_trace, cwt_figs_all, cwt_sum_all_0, cwt_sum_all_1, cwt_sum_all_2, cwt_sum_all_3, high_fre_compos, pitchs, low_fre_compos, auto_corrs, real_hils, imag_hils, dur_time_1_alls, dur_time_2_alls, jitters, shimmers, jitter_rap, hrs, feature_combine, slope, zcrs, fft_features, energy_alls, log_energy_alls, smoothe_energy_alls, legendres, double_support_time, pdps_new, lpcs, ceps_features]):\n",
    "    scaler = StandardScaler()\n",
    "    if len(feature.shape)==2:\n",
    "        X_train_i = feature[train_idx,:]\n",
    "        X_val_i = feature[val_idx,:]\n",
    "        X_test_i = feature[test_idx,:]\n",
    "        X_train_normalized_i = scaler.fit_transform(X_train_i)\n",
    "        X_val_normalized_i = scaler.transform(X_val_i)\n",
    "        X_test_normalized_i = scaler.transform(X_test_i)\n",
    "        scalers.append(scaler)\n",
    "    else:\n",
    "        X_train_i = feature[train_idx,:,:]\n",
    "        X_val_i = feature[val_idx,:,:]\n",
    "        X_test_i = feature[test_idx,:,:]\n",
    "        X_train_normalized_i = scaler.fit_transform(X_train_i.reshape(X_train_i.shape[0], -1)).reshape(X_train_i.shape)\n",
    "        X_val_normalized_i = scaler.transform(X_val_i.reshape(X_val_i.shape[0], -1)).reshape(X_val_i.shape)\n",
    "        X_test_normalized_i = scaler.transform(X_test_i.reshape(X_test_i.shape[0], -1)).reshape(X_test_i.shape)\n",
    "        scalers.append(scaler)\n",
    "    X_train_normalized.append(X_train_normalized_i)\n",
    "    X_val_normalized.append(X_val_normalized_i)\n",
    "    X_test_normalized.append(X_test_normalized_i)\n",
    "y_train = gts[train_idx,:]\n",
    "y_val = gts[val_idx,:]\n",
    "y_test = gts[test_idx,:]\n",
    "X_train_normalized_new = []\n",
    "combined_feature = np.empty((len(X_train_normalized[0]),0))\n",
    "for feature in X_train_normalized:\n",
    "    if len(feature.shape) == 3:\n",
    "        X_train_normalized_new.append(feature)\n",
    "    elif feature.shape[1] <20:\n",
    "        combined_feature = np.hstack((combined_feature, feature))\n",
    "    else:\n",
    "        X_train_normalized_new.append(feature)\n",
    "X_train_normalized_new.append(combined_feature)\n",
    "\n",
    "X_val_normalized_new = []\n",
    "combined_feature = np.empty((len(X_val_normalized[0]),0))\n",
    "for feature in X_val_normalized:\n",
    "    if len(feature.shape) == 3:\n",
    "        X_val_normalized_new.append(feature)\n",
    "    elif feature.shape[1] <20:\n",
    "        combined_feature = np.hstack((combined_feature, feature))\n",
    "    else:\n",
    "        X_val_normalized_new.append(feature)\n",
    "X_val_normalized_new.append(combined_feature)\n",
    "\n",
    "X_test_normalized_new = []\n",
    "combined_feature = np.empty((len(X_test_normalized[0]),0))\n",
    "for feature in X_test_normalized:\n",
    "    if len(feature.shape) == 3:\n",
    "        X_test_normalized_new.append(feature)\n",
    "    elif feature.shape[1] <20:\n",
    "        combined_feature = np.hstack((combined_feature, feature))\n",
    "    else:\n",
    "        X_test_normalized_new.append(feature)\n",
    "X_test_normalized_new.append(combined_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1506c6f9-24f2-4023-99c0-bae8cee7bfd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T02:53:45.588756Z",
     "iopub.status.busy": "2023-12-05T02:53:45.588595Z",
     "iopub.status.idle": "2023-12-05T02:54:01.697362Z",
     "shell.execute_reply": "2023-12-05T02:54:01.696833Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_111 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_112 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_113 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_114 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_115 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_116 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_117 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_118 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_119 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_120 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_121 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_122 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_123 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_124 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_125 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_126 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_127 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_128 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_129 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_130 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_131 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_132 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_133 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_134 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_135 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_136 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_137 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_138 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_139 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_140 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_141 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_142 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_143 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_144 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_145 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_146 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_147 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 21:53:48.113389: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_124/lstm_cell_124/recurrent_kernel/Assign' id:30149 op device:{requested: '', assigned: ''} def:{{{node lstm_124/lstm_cell_124/recurrent_kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_124/lstm_cell_124/recurrent_kernel, lstm_124/lstm_cell_124/recurrent_kernel/Initializer/mul_1)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-04 21:53:49.970565: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_135/lstm_cell_135/recurrent_kernel/v/Assign' id:35669 op device:{requested: '', assigned: ''} def:{{{node lstm_135/lstm_cell_135/recurrent_kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_135/lstm_cell_135/recurrent_kernel/v, lstm_135/lstm_cell_135/recurrent_kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "/usr/lib/python3.11/site-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "2023-12-04 21:53:51.684166: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_15/BiasAdd' id:34062 op device:{requested: '', assigned: ''} def:{{{node dense_15/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_15/MatMul, dense_15/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE:\n",
      "1.462002485031883\n",
      "[1.35356784 1.41603599]\n",
      "1.3848019123077393\n"
     ]
    }
   ],
   "source": [
    "## Evaluation on test data\n",
    "import tensorflow as tf\n",
    "m_name = \"./checkpoints/unknown_person_few_shot_p\"+str(test_person_id[0])+'_33.h5'\n",
    "model = tf.keras.models.load_model(m_name)\n",
    "from feature_emotion import feature_extract, split_data, label_unique_tuples\n",
    "trace_wlk_num = label_unique_tuples(people_nums_all, walk_nums_all, trace_nums_all)\n",
    "trace_wlk_num = np.array(trace_wlk_num )\n",
    "\n",
    "y_pred = model.predict(X_test_normalized_new)\n",
    "print('Test MAE:')\n",
    "err = np.mean(np.abs(y_pred - y_test))\n",
    "print(err)\n",
    "## Evaluation: trace median vote\n",
    "trace_num_test = trace_wlk_num[test_idx]\n",
    "u = np.unique(trace_num_test)\n",
    "pred_trace = np.empty((0, 2))\n",
    "gt_trace = np.empty((0, 2))\n",
    "for i in u:\n",
    "  trace_idx = np.where(trace_num_test == i)[0]\n",
    "  y_pred_trace = y_pred[trace_idx,:]\n",
    "  pred = np.mean(y_pred_trace, axis = 0)\n",
    "  pred_trace = np.vstack((pred_trace, pred))\n",
    "  gt_t = y_test[trace_idx[0],:]\n",
    "  gt_trace = np.vstack((gt_trace, gt_t))\n",
    "print(np.mean(np.abs(pred_trace-gt_trace), axis = 0))\n",
    "print(np.mean(np.abs(pred_trace-gt_trace)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0ddf488-3353-4b99-8359-36c9d84031d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T02:54:01.699165Z",
     "iopub.status.busy": "2023-12-05T02:54:01.699032Z",
     "iopub.status.idle": "2023-12-05T02:54:18.381836Z",
     "shell.execute_reply": "2023-12-05T02:54:18.381289Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_148 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_149 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_150 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_151 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_152 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_153 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_154 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_155 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_156 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_157 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_158 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_159 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_160 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_161 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_162 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_163 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_164 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_165 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_166 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_167 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_168 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_169 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_170 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_171 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_172 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_173 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_174 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_175 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_176 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_177 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_178 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_179 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_180 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_181 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_182 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_183 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_184 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 21:54:04.224245: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_155/lstm_cell_155/kernel/Assign' id:38400 op device:{requested: '', assigned: ''} def:{{{node lstm_155/lstm_cell_155/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_155/lstm_cell_155/kernel, lstm_155/lstm_cell_155/kernel/Initializer/random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-04 21:54:06.462367: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_157/lstm_cell_157/recurrent_kernel/v/Assign' id:44675 op device:{requested: '', assigned: ''} def:{{{node lstm_157/lstm_cell_157/recurrent_kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_157/lstm_cell_157/recurrent_kernel/v, lstm_157/lstm_cell_157/recurrent_kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-04 21:54:08.759246: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_19/BiasAdd' id:43293 op device:{requested: '', assigned: ''} def:{{{node dense_19/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_19/MatMul, dense_19/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE:\n",
      "1.425591441253821\n",
      "[1.33220935 1.34255403]\n",
      "1.337381690979004\n"
     ]
    }
   ],
   "source": [
    "## Evaluation on test data\n",
    "import tensorflow as tf\n",
    "m_name = \"./checkpoints/unknown_person_few_shot_p\"+str(test_person_id[0])+'_34.h5'\n",
    "model = tf.keras.models.load_model(m_name)\n",
    "from feature_emotion import feature_extract, split_data, label_unique_tuples\n",
    "trace_wlk_num = label_unique_tuples(people_nums_all, walk_nums_all, trace_nums_all)\n",
    "trace_wlk_num = np.array(trace_wlk_num )\n",
    "\n",
    "y_pred = model.predict(X_test_normalized_new)\n",
    "print('Test MAE:')\n",
    "err = np.mean(np.abs(y_pred - y_test))\n",
    "print(err)\n",
    "## Evaluation: trace median vote\n",
    "trace_num_test = trace_wlk_num[test_idx]\n",
    "u = np.unique(trace_num_test)\n",
    "pred_trace = np.empty((0, 2))\n",
    "gt_trace = np.empty((0, 2))\n",
    "for i in u:\n",
    "  trace_idx = np.where(trace_num_test == i)[0]\n",
    "  y_pred_trace = y_pred[trace_idx,:]\n",
    "  pred = np.mean(y_pred_trace, axis = 0)\n",
    "  pred_trace = np.vstack((pred_trace, pred))\n",
    "  gt_t = y_test[trace_idx[0],:]\n",
    "  gt_trace = np.vstack((gt_trace, gt_t))\n",
    "print(np.mean(np.abs(pred_trace-gt_trace), axis = 0))\n",
    "print(np.mean(np.abs(pred_trace-gt_trace)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f305ce0-0b67-4e3e-847c-5caaa2203b05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T02:54:18.383728Z",
     "iopub.status.busy": "2023-12-05T02:54:18.383595Z",
     "iopub.status.idle": "2023-12-05T02:54:36.092095Z",
     "shell.execute_reply": "2023-12-05T02:54:36.091492Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_185 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_186 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_187 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_188 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_189 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_190 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_191 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_192 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_193 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_194 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_195 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_196 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_197 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_198 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_199 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_200 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_201 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_202 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_203 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_204 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_205 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_206 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_207 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_208 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_209 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_210 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_211 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_212 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_213 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_214 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_215 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_216 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_217 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_218 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_219 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_220 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_221 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 21:54:21.077027: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_195/lstm_cell_195/kernel/Assign' id:48111 op device:{requested: '', assigned: ''} def:{{{node lstm_195/lstm_cell_195/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_195/lstm_cell_195/kernel, lstm_195/lstm_cell_195/kernel/Initializer/random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-04 21:54:23.681923: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_214/lstm_cell_214/recurrent_kernel/v/Assign' id:54206 op device:{requested: '', assigned: ''} def:{{{node lstm_214/lstm_cell_214/recurrent_kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_214/lstm_cell_214/recurrent_kernel/v, lstm_214/lstm_cell_214/recurrent_kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-04 21:54:26.158206: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_23/BiasAdd' id:52524 op device:{requested: '', assigned: ''} def:{{{node dense_23/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_23/MatMul, dense_23/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE:\n",
      "1.4723916828483343\n",
      "[1.36083255 1.460352  ]\n",
      "1.410592276573181\n"
     ]
    }
   ],
   "source": [
    "## Evaluation on test data\n",
    "import tensorflow as tf\n",
    "m_name = \"./checkpoints/unknown_person_few_shot_p\"+str(test_person_id[0])+'_35.h5'\n",
    "model = tf.keras.models.load_model(m_name)\n",
    "from feature_emotion import feature_extract, split_data, label_unique_tuples\n",
    "trace_wlk_num = label_unique_tuples(people_nums_all, walk_nums_all, trace_nums_all)\n",
    "trace_wlk_num = np.array(trace_wlk_num )\n",
    "\n",
    "y_pred = model.predict(X_test_normalized_new)\n",
    "print('Test MAE:')\n",
    "err = np.mean(np.abs(y_pred - y_test))\n",
    "print(err)\n",
    "## Evaluation: trace median vote\n",
    "trace_num_test = trace_wlk_num[test_idx]\n",
    "u = np.unique(trace_num_test)\n",
    "pred_trace = np.empty((0, 2))\n",
    "gt_trace = np.empty((0, 2))\n",
    "for i in u:\n",
    "  trace_idx = np.where(trace_num_test == i)[0]\n",
    "  y_pred_trace = y_pred[trace_idx,:]\n",
    "  pred = np.mean(y_pred_trace, axis = 0)\n",
    "  pred_trace = np.vstack((pred_trace, pred))\n",
    "  gt_t = y_test[trace_idx[0],:]\n",
    "  gt_trace = np.vstack((gt_trace, gt_t))\n",
    "print(np.mean(np.abs(pred_trace-gt_trace), axis = 0))\n",
    "print(np.mean(np.abs(pred_trace-gt_trace)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c294d081-a1bf-4f04-b9e0-1d9c2ced400a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T02:54:36.094090Z",
     "iopub.status.busy": "2023-12-05T02:54:36.093938Z",
     "iopub.status.idle": "2023-12-05T02:54:41.603398Z",
     "shell.execute_reply": "2023-12-05T02:54:41.602808Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35681\n"
     ]
    }
   ],
   "source": [
    "ra = 0.3\n",
    "\n",
    "flag_tr_val_te = split_data_few_shot(test_person_id, walk_nums_all, trace_nums_all, people_nums_all, person_nums, ratio = ra, rand_seed = 42)\n",
    "## Data Normalization before training ans testing\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_v2_behavior()\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, concatenate, LSTM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scalers = []\n",
    "X_train_normalized = []\n",
    "X_val_normalized = []\n",
    "X_test_normalized = []\n",
    "train_idx = np.where(flag_tr_val_te ==0)[0]\n",
    "np.random.shuffle(train_idx)\n",
    "val_idx = np.where(flag_tr_val_te ==1)[0]\n",
    "test_idx = np.where(flag_tr_val_te ==2)[0]\n",
    "\n",
    "for i, feature in enumerate([spe_centr, delta_spe_centr, spe_crest, delta_spe_crest, spe_decrease, delta_spe_decrease, spe_entropy, delta_spe_entropy, spe_flatness, delta_spe_flatness, spe_flux, delta_spe_flux, spe_kurtosis, delta_spe_kurtosis, spe_skewness, delta_spe_skewness, spe_rfp, delta_spe_rfp, spe_slope, delta_spe_slope, spe_spread, delta_spe_spread, wlk_fre, wlk_fres_trace, cwt_figs_all, cwt_sum_all_0, cwt_sum_all_1, cwt_sum_all_2, cwt_sum_all_3, high_fre_compos, pitchs, low_fre_compos, auto_corrs, real_hils, imag_hils, dur_time_1_alls, dur_time_2_alls, jitters, shimmers, jitter_rap, hrs, feature_combine, slope, zcrs, fft_features, energy_alls, log_energy_alls, smoothe_energy_alls, legendres, double_support_time, pdps_new, lpcs, ceps_features]):\n",
    "    scaler = StandardScaler()\n",
    "    if len(feature.shape)==2:\n",
    "        X_train_i = feature[train_idx,:]\n",
    "        X_val_i = feature[val_idx,:]\n",
    "        X_test_i = feature[test_idx,:]\n",
    "        X_train_normalized_i = scaler.fit_transform(X_train_i)\n",
    "        X_val_normalized_i = scaler.transform(X_val_i)\n",
    "        X_test_normalized_i = scaler.transform(X_test_i)\n",
    "        scalers.append(scaler)\n",
    "    else:\n",
    "        X_train_i = feature[train_idx,:,:]\n",
    "        X_val_i = feature[val_idx,:,:]\n",
    "        X_test_i = feature[test_idx,:,:]\n",
    "        X_train_normalized_i = scaler.fit_transform(X_train_i.reshape(X_train_i.shape[0], -1)).reshape(X_train_i.shape)\n",
    "        X_val_normalized_i = scaler.transform(X_val_i.reshape(X_val_i.shape[0], -1)).reshape(X_val_i.shape)\n",
    "        X_test_normalized_i = scaler.transform(X_test_i.reshape(X_test_i.shape[0], -1)).reshape(X_test_i.shape)\n",
    "        scalers.append(scaler)\n",
    "    X_train_normalized.append(X_train_normalized_i)\n",
    "    X_val_normalized.append(X_val_normalized_i)\n",
    "    X_test_normalized.append(X_test_normalized_i)\n",
    "y_train = gts[train_idx,:]\n",
    "y_val = gts[val_idx,:]\n",
    "y_test = gts[test_idx,:]\n",
    "X_train_normalized_new = []\n",
    "combined_feature = np.empty((len(X_train_normalized[0]),0))\n",
    "for feature in X_train_normalized:\n",
    "    if len(feature.shape) == 3:\n",
    "        X_train_normalized_new.append(feature)\n",
    "    elif feature.shape[1] <20:\n",
    "        combined_feature = np.hstack((combined_feature, feature))\n",
    "    else:\n",
    "        X_train_normalized_new.append(feature)\n",
    "X_train_normalized_new.append(combined_feature)\n",
    "\n",
    "X_val_normalized_new = []\n",
    "combined_feature = np.empty((len(X_val_normalized[0]),0))\n",
    "for feature in X_val_normalized:\n",
    "    if len(feature.shape) == 3:\n",
    "        X_val_normalized_new.append(feature)\n",
    "    elif feature.shape[1] <20:\n",
    "        combined_feature = np.hstack((combined_feature, feature))\n",
    "    else:\n",
    "        X_val_normalized_new.append(feature)\n",
    "X_val_normalized_new.append(combined_feature)\n",
    "\n",
    "X_test_normalized_new = []\n",
    "combined_feature = np.empty((len(X_test_normalized[0]),0))\n",
    "for feature in X_test_normalized:\n",
    "    if len(feature.shape) == 3:\n",
    "        X_test_normalized_new.append(feature)\n",
    "    elif feature.shape[1] <20:\n",
    "        combined_feature = np.hstack((combined_feature, feature))\n",
    "    else:\n",
    "        X_test_normalized_new.append(feature)\n",
    "X_test_normalized_new.append(combined_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a77a1a30-917d-4776-924f-cfe653f146fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T02:54:41.605667Z",
     "iopub.status.busy": "2023-12-05T02:54:41.605451Z",
     "iopub.status.idle": "2023-12-05T02:55:01.091858Z",
     "shell.execute_reply": "2023-12-05T02:55:01.091301Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_222 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_223 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_224 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_225 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_226 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_227 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_228 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_229 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_230 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_231 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_232 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_233 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_234 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_235 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_236 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_237 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_238 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_239 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_240 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_241 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_242 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_243 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_244 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_245 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_246 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_247 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_248 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_249 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_250 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_251 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_252 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_253 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_254 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_255 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_256 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_257 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_258 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 21:54:44.485436: W tensorflow/c/c_api.cc:304] Operation '{name:'conv2d_27/bias/Assign' id:55561 op device:{requested: '', assigned: ''} def:{{{node conv2d_27/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](conv2d_27/bias, conv2d_27/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-04 21:54:47.523609: W tensorflow/c/c_api.cc:304] Operation '{name:'decay_6/Assign' id:62302 op device:{requested: '', assigned: ''} def:{{{node decay_6/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](decay_6, decay_6/Initializer/initial_value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "/usr/lib/python3.11/site-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "2023-12-04 21:54:50.362517: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_27/BiasAdd' id:61755 op device:{requested: '', assigned: ''} def:{{{node dense_27/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_27/MatMul, dense_27/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE:\n",
      "1.3161641824426074\n",
      "[1.09511859 1.41205132]\n",
      "1.2535849517041986\n"
     ]
    }
   ],
   "source": [
    "## Evaluation on test data\n",
    "import tensorflow as tf\n",
    "m_name = \"./checkpoints/unknown_person_few_shot_p\"+str(test_person_id[0])+'_36.h5'\n",
    "model = tf.keras.models.load_model(m_name)\n",
    "from feature_emotion import feature_extract, split_data, label_unique_tuples\n",
    "trace_wlk_num = label_unique_tuples(people_nums_all, walk_nums_all, trace_nums_all)\n",
    "trace_wlk_num = np.array(trace_wlk_num )\n",
    "\n",
    "y_pred = model.predict(X_test_normalized_new)\n",
    "print('Test MAE:')\n",
    "err = np.mean(np.abs(y_pred - y_test))\n",
    "print(err)\n",
    "## Evaluation: trace median vote\n",
    "trace_num_test = trace_wlk_num[test_idx]\n",
    "u = np.unique(trace_num_test)\n",
    "pred_trace = np.empty((0, 2))\n",
    "gt_trace = np.empty((0, 2))\n",
    "for i in u:\n",
    "  trace_idx = np.where(trace_num_test == i)[0]\n",
    "  y_pred_trace = y_pred[trace_idx,:]\n",
    "  pred = np.mean(y_pred_trace, axis = 0)\n",
    "  pred_trace = np.vstack((pred_trace, pred))\n",
    "  gt_t = y_test[trace_idx[0],:]\n",
    "  gt_trace = np.vstack((gt_trace, gt_t))\n",
    "print(np.mean(np.abs(pred_trace-gt_trace), axis = 0))\n",
    "print(np.mean(np.abs(pred_trace-gt_trace)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78e887e8-7eea-40ab-b60c-0fbec20b5cf7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T02:55:01.093765Z",
     "iopub.status.busy": "2023-12-05T02:55:01.093624Z",
     "iopub.status.idle": "2023-12-05T02:55:20.622213Z",
     "shell.execute_reply": "2023-12-05T02:55:20.621637Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_259 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_260 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_261 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_262 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_263 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_264 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_265 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_266 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_267 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_268 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_269 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_270 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_271 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_272 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_273 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_274 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_275 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_276 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_277 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_278 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_279 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_280 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_281 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_282 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_283 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_284 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_285 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_286 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_287 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_288 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_289 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_290 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_291 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_292 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_293 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_294 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_295 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 21:55:04.438828: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_271/lstm_cell_271/kernel/Assign' id:66893 op device:{requested: '', assigned: ''} def:{{{node lstm_271/lstm_cell_271/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_271/lstm_cell_271/kernel, lstm_271/lstm_cell_271/kernel/Initializer/random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-04 21:55:07.867493: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_288/lstm_cell_288/kernel/v/Assign' id:72663 op device:{requested: '', assigned: ''} def:{{{node lstm_288/lstm_cell_288/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_288/lstm_cell_288/kernel/v, lstm_288/lstm_cell_288/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-04 21:55:11.102081: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_31/BiasAdd' id:70986 op device:{requested: '', assigned: ''} def:{{{node dense_31/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_31/MatMul, dense_31/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE:\n",
      "1.37899251128688\n",
      "[1.16926468 1.50369638]\n",
      "1.3364805297418074\n"
     ]
    }
   ],
   "source": [
    "## Evaluation on test data\n",
    "import tensorflow as tf\n",
    "m_name = \"./checkpoints/unknown_person_few_shot_p\"+str(test_person_id[0])+'_37.h5'\n",
    "model = tf.keras.models.load_model(m_name)\n",
    "from feature_emotion import feature_extract, split_data, label_unique_tuples\n",
    "trace_wlk_num = label_unique_tuples(people_nums_all, walk_nums_all, trace_nums_all)\n",
    "trace_wlk_num = np.array(trace_wlk_num )\n",
    "\n",
    "y_pred = model.predict(X_test_normalized_new)\n",
    "print('Test MAE:')\n",
    "err = np.mean(np.abs(y_pred - y_test))\n",
    "print(err)\n",
    "## Evaluation: trace median vote\n",
    "trace_num_test = trace_wlk_num[test_idx]\n",
    "u = np.unique(trace_num_test)\n",
    "pred_trace = np.empty((0, 2))\n",
    "gt_trace = np.empty((0, 2))\n",
    "for i in u:\n",
    "  trace_idx = np.where(trace_num_test == i)[0]\n",
    "  y_pred_trace = y_pred[trace_idx,:]\n",
    "  pred = np.mean(y_pred_trace, axis = 0)\n",
    "  pred_trace = np.vstack((pred_trace, pred))\n",
    "  gt_t = y_test[trace_idx[0],:]\n",
    "  gt_trace = np.vstack((gt_trace, gt_t))\n",
    "print(np.mean(np.abs(pred_trace-gt_trace), axis = 0))\n",
    "print(np.mean(np.abs(pred_trace-gt_trace)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d25d6b2a-4d26-4dc0-9db7-bbc0f826c72f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T02:55:20.624067Z",
     "iopub.status.busy": "2023-12-05T02:55:20.623928Z",
     "iopub.status.idle": "2023-12-05T02:55:40.649857Z",
     "shell.execute_reply": "2023-12-05T02:55:40.649270Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_296 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_297 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_298 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_299 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_300 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_301 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_302 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_303 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_304 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_305 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_306 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_307 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_308 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_309 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_310 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_311 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_312 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_313 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_314 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_315 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_316 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_317 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_318 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_319 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_320 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_321 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_322 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_323 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_324 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_325 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_326 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_327 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_328 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_329 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_330 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_331 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_332 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 21:55:23.858407: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_316/lstm_cell_316/kernel/Assign' id:77404 op device:{requested: '', assigned: ''} def:{{{node lstm_316/lstm_cell_316/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_316/lstm_cell_316/kernel, lstm_316/lstm_cell_316/kernel/Initializer/random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-04 21:55:27.666403: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_318/lstm_cell_318/bias/m/Assign' id:81156 op device:{requested: '', assigned: ''} def:{{{node lstm_318/lstm_cell_318/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_318/lstm_cell_318/bias/m, lstm_318/lstm_cell_318/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-04 21:55:31.339330: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_35/BiasAdd' id:80217 op device:{requested: '', assigned: ''} def:{{{node dense_35/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_35/MatMul, dense_35/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE:\n",
      "1.3628660215572876\n",
      "[1.13918653 1.48185891]\n",
      "1.3105227221142162\n"
     ]
    }
   ],
   "source": [
    "## Evaluation on test data\n",
    "import tensorflow as tf\n",
    "m_name = \"./checkpoints/unknown_person_few_shot_p\"+str(test_person_id[0])+'_38.h5'\n",
    "model = tf.keras.models.load_model(m_name)\n",
    "from feature_emotion import feature_extract, split_data, label_unique_tuples\n",
    "trace_wlk_num = label_unique_tuples(people_nums_all, walk_nums_all, trace_nums_all)\n",
    "trace_wlk_num = np.array(trace_wlk_num )\n",
    "\n",
    "y_pred = model.predict(X_test_normalized_new)\n",
    "print('Test MAE:')\n",
    "err = np.mean(np.abs(y_pred - y_test))\n",
    "print(err)\n",
    "## Evaluation: trace median vote\n",
    "trace_num_test = trace_wlk_num[test_idx]\n",
    "u = np.unique(trace_num_test)\n",
    "pred_trace = np.empty((0, 2))\n",
    "gt_trace = np.empty((0, 2))\n",
    "for i in u:\n",
    "  trace_idx = np.where(trace_num_test == i)[0]\n",
    "  y_pred_trace = y_pred[trace_idx,:]\n",
    "  pred = np.mean(y_pred_trace, axis = 0)\n",
    "  pred_trace = np.vstack((pred_trace, pred))\n",
    "  gt_t = y_test[trace_idx[0],:]\n",
    "  gt_trace = np.vstack((gt_trace, gt_t))\n",
    "print(np.mean(np.abs(pred_trace-gt_trace), axis = 0))\n",
    "print(np.mean(np.abs(pred_trace-gt_trace)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5fdc1ec-b09c-4cd9-a202-cc114b9ce9fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T02:55:40.651629Z",
     "iopub.status.busy": "2023-12-05T02:55:40.651489Z",
     "iopub.status.idle": "2023-12-05T02:55:46.224582Z",
     "shell.execute_reply": "2023-12-05T02:55:46.223993Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35873\n"
     ]
    }
   ],
   "source": [
    "ra = 0.4\n",
    "\n",
    "flag_tr_val_te = split_data_few_shot(test_person_id, walk_nums_all, trace_nums_all, people_nums_all, person_nums, ratio = ra, rand_seed = 42)\n",
    "## Data Normalization before training ans testing\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_v2_behavior()\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, concatenate, LSTM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scalers = []\n",
    "X_train_normalized = []\n",
    "X_val_normalized = []\n",
    "X_test_normalized = []\n",
    "train_idx = np.where(flag_tr_val_te ==0)[0]\n",
    "np.random.shuffle(train_idx)\n",
    "val_idx = np.where(flag_tr_val_te ==1)[0]\n",
    "test_idx = np.where(flag_tr_val_te ==2)[0]\n",
    "\n",
    "for i, feature in enumerate([spe_centr, delta_spe_centr, spe_crest, delta_spe_crest, spe_decrease, delta_spe_decrease, spe_entropy, delta_spe_entropy, spe_flatness, delta_spe_flatness, spe_flux, delta_spe_flux, spe_kurtosis, delta_spe_kurtosis, spe_skewness, delta_spe_skewness, spe_rfp, delta_spe_rfp, spe_slope, delta_spe_slope, spe_spread, delta_spe_spread, wlk_fre, wlk_fres_trace, cwt_figs_all, cwt_sum_all_0, cwt_sum_all_1, cwt_sum_all_2, cwt_sum_all_3, high_fre_compos, pitchs, low_fre_compos, auto_corrs, real_hils, imag_hils, dur_time_1_alls, dur_time_2_alls, jitters, shimmers, jitter_rap, hrs, feature_combine, slope, zcrs, fft_features, energy_alls, log_energy_alls, smoothe_energy_alls, legendres, double_support_time, pdps_new, lpcs, ceps_features]):\n",
    "    scaler = StandardScaler()\n",
    "    if len(feature.shape)==2:\n",
    "        X_train_i = feature[train_idx,:]\n",
    "        X_val_i = feature[val_idx,:]\n",
    "        X_test_i = feature[test_idx,:]\n",
    "        X_train_normalized_i = scaler.fit_transform(X_train_i)\n",
    "        X_val_normalized_i = scaler.transform(X_val_i)\n",
    "        X_test_normalized_i = scaler.transform(X_test_i)\n",
    "        scalers.append(scaler)\n",
    "    else:\n",
    "        X_train_i = feature[train_idx,:,:]\n",
    "        X_val_i = feature[val_idx,:,:]\n",
    "        X_test_i = feature[test_idx,:,:]\n",
    "        X_train_normalized_i = scaler.fit_transform(X_train_i.reshape(X_train_i.shape[0], -1)).reshape(X_train_i.shape)\n",
    "        X_val_normalized_i = scaler.transform(X_val_i.reshape(X_val_i.shape[0], -1)).reshape(X_val_i.shape)\n",
    "        X_test_normalized_i = scaler.transform(X_test_i.reshape(X_test_i.shape[0], -1)).reshape(X_test_i.shape)\n",
    "        scalers.append(scaler)\n",
    "    X_train_normalized.append(X_train_normalized_i)\n",
    "    X_val_normalized.append(X_val_normalized_i)\n",
    "    X_test_normalized.append(X_test_normalized_i)\n",
    "y_train = gts[train_idx,:]\n",
    "y_val = gts[val_idx,:]\n",
    "y_test = gts[test_idx,:]\n",
    "X_train_normalized_new = []\n",
    "combined_feature = np.empty((len(X_train_normalized[0]),0))\n",
    "for feature in X_train_normalized:\n",
    "    if len(feature.shape) == 3:\n",
    "        X_train_normalized_new.append(feature)\n",
    "    elif feature.shape[1] <20:\n",
    "        combined_feature = np.hstack((combined_feature, feature))\n",
    "    else:\n",
    "        X_train_normalized_new.append(feature)\n",
    "X_train_normalized_new.append(combined_feature)\n",
    "\n",
    "X_val_normalized_new = []\n",
    "combined_feature = np.empty((len(X_val_normalized[0]),0))\n",
    "for feature in X_val_normalized:\n",
    "    if len(feature.shape) == 3:\n",
    "        X_val_normalized_new.append(feature)\n",
    "    elif feature.shape[1] <20:\n",
    "        combined_feature = np.hstack((combined_feature, feature))\n",
    "    else:\n",
    "        X_val_normalized_new.append(feature)\n",
    "X_val_normalized_new.append(combined_feature)\n",
    "\n",
    "X_test_normalized_new = []\n",
    "combined_feature = np.empty((len(X_test_normalized[0]),0))\n",
    "for feature in X_test_normalized:\n",
    "    if len(feature.shape) == 3:\n",
    "        X_test_normalized_new.append(feature)\n",
    "    elif feature.shape[1] <20:\n",
    "        combined_feature = np.hstack((combined_feature, feature))\n",
    "    else:\n",
    "        X_test_normalized_new.append(feature)\n",
    "X_test_normalized_new.append(combined_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21850b63-95df-4896-96a3-3b53ad3bf4bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T02:55:46.226383Z",
     "iopub.status.busy": "2023-12-05T02:55:46.226259Z",
     "iopub.status.idle": "2023-12-05T02:56:06.903993Z",
     "shell.execute_reply": "2023-12-05T02:56:06.903408Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_333 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_334 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_335 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_336 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_337 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_338 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_339 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_340 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_341 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_342 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_343 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_344 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_345 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_346 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_347 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_348 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_349 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_350 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_351 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_352 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_353 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_354 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_355 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_356 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_357 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_358 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_359 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_360 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_361 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_362 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_363 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_364 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_365 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_366 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_367 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_368 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_369 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 21:55:49.658173: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_338/lstm_cell_338/kernel/Assign' id:84235 op device:{requested: '', assigned: ''} def:{{{node lstm_338/lstm_cell_338/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_338/lstm_cell_338/kernel, lstm_338/lstm_cell_338/kernel/Initializer/random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-04 21:55:54.243129: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_359/lstm_cell_359/bias/m/Assign' id:90447 op device:{requested: '', assigned: ''} def:{{{node lstm_359/lstm_cell_359/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_359/lstm_cell_359/bias/m, lstm_359/lstm_cell_359/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "/usr/lib/python3.11/site-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "2023-12-04 21:55:58.367739: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_39/BiasAdd' id:89448 op device:{requested: '', assigned: ''} def:{{{node dense_39/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_39/MatMul, dense_39/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE:\n",
      "1.1974576183508896\n",
      "[0.9655326  1.24201356]\n",
      "1.1037730828244636\n"
     ]
    }
   ],
   "source": [
    "## Evaluation on test data\n",
    "import tensorflow as tf\n",
    "m_name = \"./checkpoints/unknown_person_few_shot_p\"+str(test_person_id[0])+'_39.h5'\n",
    "model = tf.keras.models.load_model(m_name)\n",
    "from feature_emotion import feature_extract, split_data, label_unique_tuples\n",
    "trace_wlk_num = label_unique_tuples(people_nums_all, walk_nums_all, trace_nums_all)\n",
    "trace_wlk_num = np.array(trace_wlk_num )\n",
    "\n",
    "y_pred = model.predict(X_test_normalized_new)\n",
    "print('Test MAE:')\n",
    "err = np.mean(np.abs(y_pred - y_test))\n",
    "print(err)\n",
    "## Evaluation: trace median vote\n",
    "trace_num_test = trace_wlk_num[test_idx]\n",
    "u = np.unique(trace_num_test)\n",
    "pred_trace = np.empty((0, 2))\n",
    "gt_trace = np.empty((0, 2))\n",
    "for i in u:\n",
    "  trace_idx = np.where(trace_num_test == i)[0]\n",
    "  y_pred_trace = y_pred[trace_idx,:]\n",
    "  pred = np.mean(y_pred_trace, axis = 0)\n",
    "  pred_trace = np.vstack((pred_trace, pred))\n",
    "  gt_t = y_test[trace_idx[0],:]\n",
    "  gt_trace = np.vstack((gt_trace, gt_t))\n",
    "print(np.mean(np.abs(pred_trace-gt_trace), axis = 0))\n",
    "print(np.mean(np.abs(pred_trace-gt_trace)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e975f2a-a3c7-466c-888c-df4d35f49fa1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T02:56:06.905894Z",
     "iopub.status.busy": "2023-12-05T02:56:06.905751Z",
     "iopub.status.idle": "2023-12-05T02:56:28.359003Z",
     "shell.execute_reply": "2023-12-05T02:56:28.358444Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_370 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_371 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_372 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_373 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_374 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_375 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_376 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_377 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_378 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_379 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_380 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_381 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_382 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_383 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_384 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_385 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_386 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_387 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_388 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_389 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_390 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_391 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_392 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_393 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_394 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_395 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_396 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_397 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_398 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_399 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_400 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_401 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_402 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_403 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_404 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_405 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_406 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 21:56:10.515294: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_370/lstm_cell_370/bias/Assign' id:92695 op device:{requested: '', assigned: ''} def:{{{node lstm_370/lstm_cell_370/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_370/lstm_cell_370/bias, lstm_370/lstm_cell_370/bias/Initializer/concat)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-04 21:56:15.119924: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_402/lstm_cell_402/recurrent_kernel/m/Assign' id:99763 op device:{requested: '', assigned: ''} def:{{{node lstm_402/lstm_cell_402/recurrent_kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_402/lstm_cell_402/recurrent_kernel/m, lstm_402/lstm_cell_402/recurrent_kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-04 21:56:19.613964: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_43/BiasAdd' id:98679 op device:{requested: '', assigned: ''} def:{{{node dense_43/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_43/MatMul, dense_43/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE:\n",
      "1.2523480690223106\n",
      "[1.01631807 1.34889685]\n",
      "1.1826074592610623\n"
     ]
    }
   ],
   "source": [
    "## Evaluation on test data\n",
    "import tensorflow as tf\n",
    "m_name = \"./checkpoints/unknown_person_few_shot_p\"+str(test_person_id[0])+'_40.h5'\n",
    "model = tf.keras.models.load_model(m_name)\n",
    "from feature_emotion import feature_extract, split_data, label_unique_tuples\n",
    "trace_wlk_num = label_unique_tuples(people_nums_all, walk_nums_all, trace_nums_all)\n",
    "trace_wlk_num = np.array(trace_wlk_num )\n",
    "\n",
    "y_pred = model.predict(X_test_normalized_new)\n",
    "print('Test MAE:')\n",
    "err = np.mean(np.abs(y_pred - y_test))\n",
    "print(err)\n",
    "## Evaluation: trace median vote\n",
    "trace_num_test = trace_wlk_num[test_idx]\n",
    "u = np.unique(trace_num_test)\n",
    "pred_trace = np.empty((0, 2))\n",
    "gt_trace = np.empty((0, 2))\n",
    "for i in u:\n",
    "  trace_idx = np.where(trace_num_test == i)[0]\n",
    "  y_pred_trace = y_pred[trace_idx,:]\n",
    "  pred = np.mean(y_pred_trace, axis = 0)\n",
    "  pred_trace = np.vstack((pred_trace, pred))\n",
    "  gt_t = y_test[trace_idx[0],:]\n",
    "  gt_trace = np.vstack((gt_trace, gt_t))\n",
    "print(np.mean(np.abs(pred_trace-gt_trace), axis = 0))\n",
    "print(np.mean(np.abs(pred_trace-gt_trace)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a37872d2-0d6b-43d0-84c6-3f1124a4d4d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T02:56:28.360789Z",
     "iopub.status.busy": "2023-12-05T02:56:28.360650Z",
     "iopub.status.idle": "2023-12-05T02:56:51.158528Z",
     "shell.execute_reply": "2023-12-05T02:56:51.157967Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_407 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_408 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_409 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_410 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_411 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_412 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_413 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_414 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_415 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_416 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_417 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_418 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_419 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_420 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_421 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_422 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_423 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_424 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_425 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_426 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_427 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_428 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_429 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_430 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_431 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_432 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_433 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_434 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_435 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_436 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_437 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_438 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_439 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_440 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_441 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_442 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_443 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 21:56:32.183341: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_434/lstm_cell_434/kernel/Assign' id:106219 op device:{requested: '', assigned: ''} def:{{{node lstm_434/lstm_cell_434/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_434/lstm_cell_434/kernel, lstm_434/lstm_cell_434/kernel/Initializer/random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-04 21:56:37.225509: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_419/lstm_cell_419/recurrent_kernel/m/Assign' id:108694 op device:{requested: '', assigned: ''} def:{{{node lstm_419/lstm_cell_419/recurrent_kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_419/lstm_cell_419/recurrent_kernel/m, lstm_419/lstm_cell_419/recurrent_kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-04 21:56:42.144572: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_47/BiasAdd' id:107910 op device:{requested: '', assigned: ''} def:{{{node dense_47/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_47/MatMul, dense_47/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE:\n",
      "1.221563294424233\n",
      "[1.00806897 1.27041036]\n",
      "1.1392396637733946\n"
     ]
    }
   ],
   "source": [
    "## Evaluation on test data\n",
    "import tensorflow as tf\n",
    "m_name = \"./checkpoints/unknown_person_few_shot_p\"+str(test_person_id[0])+'_41.h5'\n",
    "model = tf.keras.models.load_model(m_name)\n",
    "from feature_emotion import feature_extract, split_data, label_unique_tuples\n",
    "trace_wlk_num = label_unique_tuples(people_nums_all, walk_nums_all, trace_nums_all)\n",
    "trace_wlk_num = np.array(trace_wlk_num )\n",
    "\n",
    "y_pred = model.predict(X_test_normalized_new)\n",
    "print('Test MAE:')\n",
    "err = np.mean(np.abs(y_pred - y_test))\n",
    "print(err)\n",
    "## Evaluation: trace median vote\n",
    "trace_num_test = trace_wlk_num[test_idx]\n",
    "u = np.unique(trace_num_test)\n",
    "pred_trace = np.empty((0, 2))\n",
    "gt_trace = np.empty((0, 2))\n",
    "for i in u:\n",
    "  trace_idx = np.where(trace_num_test == i)[0]\n",
    "  y_pred_trace = y_pred[trace_idx,:]\n",
    "  pred = np.mean(y_pred_trace, axis = 0)\n",
    "  pred_trace = np.vstack((pred_trace, pred))\n",
    "  gt_t = y_test[trace_idx[0],:]\n",
    "  gt_trace = np.vstack((gt_trace, gt_t))\n",
    "print(np.mean(np.abs(pred_trace-gt_trace), axis = 0))\n",
    "print(np.mean(np.abs(pred_trace-gt_trace)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "917ef878-ec69-496f-805c-c87875f4b312",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T02:56:51.160453Z",
     "iopub.status.busy": "2023-12-05T02:56:51.160316Z",
     "iopub.status.idle": "2023-12-05T02:56:56.703129Z",
     "shell.execute_reply": "2023-12-05T02:56:56.702502Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36065\n"
     ]
    }
   ],
   "source": [
    "ra = 0.5\n",
    "\n",
    "flag_tr_val_te = split_data_few_shot(test_person_id, walk_nums_all, trace_nums_all, people_nums_all, person_nums, ratio = ra, rand_seed = 42)\n",
    "## Data Normalization before training ans testing\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_v2_behavior()\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, concatenate, LSTM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scalers = []\n",
    "X_train_normalized = []\n",
    "X_val_normalized = []\n",
    "X_test_normalized = []\n",
    "train_idx = np.where(flag_tr_val_te ==0)[0]\n",
    "np.random.shuffle(train_idx)\n",
    "val_idx = np.where(flag_tr_val_te ==1)[0]\n",
    "test_idx = np.where(flag_tr_val_te ==2)[0]\n",
    "\n",
    "for i, feature in enumerate([spe_centr, delta_spe_centr, spe_crest, delta_spe_crest, spe_decrease, delta_spe_decrease, spe_entropy, delta_spe_entropy, spe_flatness, delta_spe_flatness, spe_flux, delta_spe_flux, spe_kurtosis, delta_spe_kurtosis, spe_skewness, delta_spe_skewness, spe_rfp, delta_spe_rfp, spe_slope, delta_spe_slope, spe_spread, delta_spe_spread, wlk_fre, wlk_fres_trace, cwt_figs_all, cwt_sum_all_0, cwt_sum_all_1, cwt_sum_all_2, cwt_sum_all_3, high_fre_compos, pitchs, low_fre_compos, auto_corrs, real_hils, imag_hils, dur_time_1_alls, dur_time_2_alls, jitters, shimmers, jitter_rap, hrs, feature_combine, slope, zcrs, fft_features, energy_alls, log_energy_alls, smoothe_energy_alls, legendres, double_support_time, pdps_new, lpcs, ceps_features]):\n",
    "    scaler = StandardScaler()\n",
    "    if len(feature.shape)==2:\n",
    "        X_train_i = feature[train_idx,:]\n",
    "        X_val_i = feature[val_idx,:]\n",
    "        X_test_i = feature[test_idx,:]\n",
    "        X_train_normalized_i = scaler.fit_transform(X_train_i)\n",
    "        X_val_normalized_i = scaler.transform(X_val_i)\n",
    "        X_test_normalized_i = scaler.transform(X_test_i)\n",
    "        scalers.append(scaler)\n",
    "    else:\n",
    "        X_train_i = feature[train_idx,:,:]\n",
    "        X_val_i = feature[val_idx,:,:]\n",
    "        X_test_i = feature[test_idx,:,:]\n",
    "        X_train_normalized_i = scaler.fit_transform(X_train_i.reshape(X_train_i.shape[0], -1)).reshape(X_train_i.shape)\n",
    "        X_val_normalized_i = scaler.transform(X_val_i.reshape(X_val_i.shape[0], -1)).reshape(X_val_i.shape)\n",
    "        X_test_normalized_i = scaler.transform(X_test_i.reshape(X_test_i.shape[0], -1)).reshape(X_test_i.shape)\n",
    "        scalers.append(scaler)\n",
    "    X_train_normalized.append(X_train_normalized_i)\n",
    "    X_val_normalized.append(X_val_normalized_i)\n",
    "    X_test_normalized.append(X_test_normalized_i)\n",
    "y_train = gts[train_idx,:]\n",
    "y_val = gts[val_idx,:]\n",
    "y_test = gts[test_idx,:]\n",
    "X_train_normalized_new = []\n",
    "combined_feature = np.empty((len(X_train_normalized[0]),0))\n",
    "for feature in X_train_normalized:\n",
    "    if len(feature.shape) == 3:\n",
    "        X_train_normalized_new.append(feature)\n",
    "    elif feature.shape[1] <20:\n",
    "        combined_feature = np.hstack((combined_feature, feature))\n",
    "    else:\n",
    "        X_train_normalized_new.append(feature)\n",
    "X_train_normalized_new.append(combined_feature)\n",
    "\n",
    "X_val_normalized_new = []\n",
    "combined_feature = np.empty((len(X_val_normalized[0]),0))\n",
    "for feature in X_val_normalized:\n",
    "    if len(feature.shape) == 3:\n",
    "        X_val_normalized_new.append(feature)\n",
    "    elif feature.shape[1] <20:\n",
    "        combined_feature = np.hstack((combined_feature, feature))\n",
    "    else:\n",
    "        X_val_normalized_new.append(feature)\n",
    "X_val_normalized_new.append(combined_feature)\n",
    "\n",
    "X_test_normalized_new = []\n",
    "combined_feature = np.empty((len(X_test_normalized[0]),0))\n",
    "for feature in X_test_normalized:\n",
    "    if len(feature.shape) == 3:\n",
    "        X_test_normalized_new.append(feature)\n",
    "    elif feature.shape[1] <20:\n",
    "        combined_feature = np.hstack((combined_feature, feature))\n",
    "    else:\n",
    "        X_test_normalized_new.append(feature)\n",
    "X_test_normalized_new.append(combined_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c0baa69-1067-4c69-a8c3-4190ca4f5e5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T02:56:56.704958Z",
     "iopub.status.busy": "2023-12-05T02:56:56.704817Z",
     "iopub.status.idle": "2023-12-05T02:57:19.945491Z",
     "shell.execute_reply": "2023-12-05T02:57:19.944913Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_444 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_445 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_446 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_447 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_448 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_449 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_450 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_451 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_452 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_453 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_454 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_455 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_456 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_457 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_458 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_459 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_460 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_461 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_462 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_463 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_464 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_465 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_466 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_467 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_468 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_469 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_470 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_471 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_472 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_473 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_474 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_475 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_476 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_477 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_478 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_479 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_480 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 21:57:00.687775: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_471/lstm_cell_471/bias/Assign' id:115479 op device:{requested: '', assigned: ''} def:{{{node lstm_471/lstm_cell_471/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_471/lstm_cell_471/bias, lstm_471/lstm_cell_471/bias/Initializer/concat)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-04 21:57:06.603025: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_475/lstm_cell_475/bias/m/Assign' id:118215 op device:{requested: '', assigned: ''} def:{{{node lstm_475/lstm_cell_475/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_475/lstm_cell_475/bias/m, lstm_475/lstm_cell_475/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "/usr/lib/python3.11/site-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "2023-12-04 21:57:11.934093: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_51/BiasAdd' id:117141 op device:{requested: '', assigned: ''} def:{{{node dense_51/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_51/MatMul, dense_51/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE:\n",
      "1.2614957652667649\n",
      "[1.12303239 1.30884848]\n",
      "1.2159404357274373\n"
     ]
    }
   ],
   "source": [
    "## Evaluation on test data\n",
    "import tensorflow as tf\n",
    "m_name = \"./checkpoints/unknown_person_few_shot_p\"+str(test_person_id[0])+'_42.h5'\n",
    "model = tf.keras.models.load_model(m_name)\n",
    "from feature_emotion import feature_extract, split_data, label_unique_tuples\n",
    "trace_wlk_num = label_unique_tuples(people_nums_all, walk_nums_all, trace_nums_all)\n",
    "trace_wlk_num = np.array(trace_wlk_num )\n",
    "\n",
    "y_pred = model.predict(X_test_normalized_new)\n",
    "print('Test MAE:')\n",
    "err = np.mean(np.abs(y_pred - y_test))\n",
    "print(err)\n",
    "## Evaluation: trace median vote\n",
    "trace_num_test = trace_wlk_num[test_idx]\n",
    "u = np.unique(trace_num_test)\n",
    "pred_trace = np.empty((0, 2))\n",
    "gt_trace = np.empty((0, 2))\n",
    "for i in u:\n",
    "  trace_idx = np.where(trace_num_test == i)[0]\n",
    "  y_pred_trace = y_pred[trace_idx,:]\n",
    "  pred = np.mean(y_pred_trace, axis = 0)\n",
    "  pred_trace = np.vstack((pred_trace, pred))\n",
    "  gt_t = y_test[trace_idx[0],:]\n",
    "  gt_trace = np.vstack((gt_trace, gt_t))\n",
    "print(np.mean(np.abs(pred_trace-gt_trace), axis = 0))\n",
    "print(np.mean(np.abs(pred_trace-gt_trace)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b15922c7-69c8-4047-a2c9-97911d2f927f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T02:57:19.947399Z",
     "iopub.status.busy": "2023-12-05T02:57:19.947260Z",
     "iopub.status.idle": "2023-12-05T02:57:43.878908Z",
     "shell.execute_reply": "2023-12-05T02:57:43.878309Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_481 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_482 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_483 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_484 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_485 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_486 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_487 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_488 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_489 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_490 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_491 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_492 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_493 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_494 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_495 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_496 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_497 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_498 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_499 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_500 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_501 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_502 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_503 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_504 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_505 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_506 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_507 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_508 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_509 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_510 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_511 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_512 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_513 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_514 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_515 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_516 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_517 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 21:57:24.135636: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_506/lstm_cell_506/bias/Assign' id:124390 op device:{requested: '', assigned: ''} def:{{{node lstm_506/lstm_cell_506/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_506/lstm_cell_506/bias, lstm_506/lstm_cell_506/bias/Initializer/concat)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-04 21:57:30.030266: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_510/lstm_cell_510/bias/v/Assign' id:128059 op device:{requested: '', assigned: ''} def:{{{node lstm_510/lstm_cell_510/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_510/lstm_cell_510/bias/v, lstm_510/lstm_cell_510/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-04 21:57:35.766409: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_55/BiasAdd' id:126372 op device:{requested: '', assigned: ''} def:{{{node dense_55/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_55/MatMul, dense_55/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE:\n",
      "1.257107007325205\n",
      "[1.05514984 1.31219684]\n",
      "1.1836733390123417\n"
     ]
    }
   ],
   "source": [
    "## Evaluation on test data\n",
    "import tensorflow as tf\n",
    "m_name = \"./checkpoints/unknown_person_few_shot_p\"+str(test_person_id[0])+'_43.h5'\n",
    "model = tf.keras.models.load_model(m_name)\n",
    "from feature_emotion import feature_extract, split_data, label_unique_tuples\n",
    "trace_wlk_num = label_unique_tuples(people_nums_all, walk_nums_all, trace_nums_all)\n",
    "trace_wlk_num = np.array(trace_wlk_num )\n",
    "\n",
    "y_pred = model.predict(X_test_normalized_new)\n",
    "print('Test MAE:')\n",
    "err = np.mean(np.abs(y_pred - y_test))\n",
    "print(err)\n",
    "## Evaluation: trace median vote\n",
    "trace_num_test = trace_wlk_num[test_idx]\n",
    "u = np.unique(trace_num_test)\n",
    "pred_trace = np.empty((0, 2))\n",
    "gt_trace = np.empty((0, 2))\n",
    "for i in u:\n",
    "  trace_idx = np.where(trace_num_test == i)[0]\n",
    "  y_pred_trace = y_pred[trace_idx,:]\n",
    "  pred = np.mean(y_pred_trace, axis = 0)\n",
    "  pred_trace = np.vstack((pred_trace, pred))\n",
    "  gt_t = y_test[trace_idx[0],:]\n",
    "  gt_trace = np.vstack((gt_trace, gt_t))\n",
    "print(np.mean(np.abs(pred_trace-gt_trace), axis = 0))\n",
    "print(np.mean(np.abs(pred_trace-gt_trace)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a11d1f75-8bab-4080-9b9c-bbf2d092e39a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T02:57:43.880679Z",
     "iopub.status.busy": "2023-12-05T02:57:43.880533Z",
     "iopub.status.idle": "2023-12-05T02:58:09.010307Z",
     "shell.execute_reply": "2023-12-05T02:58:09.009736Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_518 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_519 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_520 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_521 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_522 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_523 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_524 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_525 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_526 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_527 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_528 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_529 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_530 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_531 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_532 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_533 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_534 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_535 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_536 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_537 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_538 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_539 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_540 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_541 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_542 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_543 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_544 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_545 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_546 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_547 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_548 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_549 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_550 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_551 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_552 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_553 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_554 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 21:57:48.245699: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_528/lstm_cell_528/bias/Assign' id:131219 op device:{requested: '', assigned: ''} def:{{{node lstm_528/lstm_cell_528/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_528/lstm_cell_528/bias, lstm_528/lstm_cell_528/bias/Initializer/concat)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-04 21:57:54.488560: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_550/lstm_cell_550/recurrent_kernel/m/Assign' id:136687 op device:{requested: '', assigned: ''} def:{{{node lstm_550/lstm_cell_550/recurrent_kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_550/lstm_cell_550/recurrent_kernel/m, lstm_550/lstm_cell_550/recurrent_kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-04 21:58:00.596997: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_59/BiasAdd' id:135603 op device:{requested: '', assigned: ''} def:{{{node dense_59/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_59/MatMul, dense_59/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE:\n",
      "1.226620446285631\n",
      "[1.07340285 1.25535883]\n",
      "1.1643808422944484\n"
     ]
    }
   ],
   "source": [
    "## Evaluation on test data\n",
    "import tensorflow as tf\n",
    "m_name = \"./checkpoints/unknown_person_few_shot_p\"+str(test_person_id[0])+'_44.h5'\n",
    "model = tf.keras.models.load_model(m_name)\n",
    "from feature_emotion import feature_extract, split_data, label_unique_tuples\n",
    "trace_wlk_num = label_unique_tuples(people_nums_all, walk_nums_all, trace_nums_all)\n",
    "trace_wlk_num = np.array(trace_wlk_num )\n",
    "\n",
    "y_pred = model.predict(X_test_normalized_new)\n",
    "print('Test MAE:')\n",
    "err = np.mean(np.abs(y_pred - y_test))\n",
    "print(err)\n",
    "## Evaluation: trace median vote\n",
    "trace_num_test = trace_wlk_num[test_idx]\n",
    "u = np.unique(trace_num_test)\n",
    "pred_trace = np.empty((0, 2))\n",
    "gt_trace = np.empty((0, 2))\n",
    "for i in u:\n",
    "  trace_idx = np.where(trace_num_test == i)[0]\n",
    "  y_pred_trace = y_pred[trace_idx,:]\n",
    "  pred = np.mean(y_pred_trace, axis = 0)\n",
    "  pred_trace = np.vstack((pred_trace, pred))\n",
    "  gt_t = y_test[trace_idx[0],:]\n",
    "  gt_trace = np.vstack((gt_trace, gt_t))\n",
    "print(np.mean(np.abs(pred_trace-gt_trace), axis = 0))\n",
    "print(np.mean(np.abs(pred_trace-gt_trace)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3a11653-95d3-4aec-83f7-ccfaf49c3a26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T02:58:09.012184Z",
     "iopub.status.busy": "2023-12-05T02:58:09.012042Z",
     "iopub.status.idle": "2023-12-05T02:58:14.661251Z",
     "shell.execute_reply": "2023-12-05T02:58:14.660655Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36245\n"
     ]
    }
   ],
   "source": [
    "ra = 0.6\n",
    "\n",
    "flag_tr_val_te = split_data_few_shot(test_person_id, walk_nums_all, trace_nums_all, people_nums_all, person_nums, ratio = ra, rand_seed = 42)\n",
    "## Data Normalization before training ans testing\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_v2_behavior()\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, concatenate, LSTM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scalers = []\n",
    "X_train_normalized = []\n",
    "X_val_normalized = []\n",
    "X_test_normalized = []\n",
    "train_idx = np.where(flag_tr_val_te ==0)[0]\n",
    "np.random.shuffle(train_idx)\n",
    "val_idx = np.where(flag_tr_val_te ==1)[0]\n",
    "test_idx = np.where(flag_tr_val_te ==2)[0]\n",
    "\n",
    "for i, feature in enumerate([spe_centr, delta_spe_centr, spe_crest, delta_spe_crest, spe_decrease, delta_spe_decrease, spe_entropy, delta_spe_entropy, spe_flatness, delta_spe_flatness, spe_flux, delta_spe_flux, spe_kurtosis, delta_spe_kurtosis, spe_skewness, delta_spe_skewness, spe_rfp, delta_spe_rfp, spe_slope, delta_spe_slope, spe_spread, delta_spe_spread, wlk_fre, wlk_fres_trace, cwt_figs_all, cwt_sum_all_0, cwt_sum_all_1, cwt_sum_all_2, cwt_sum_all_3, high_fre_compos, pitchs, low_fre_compos, auto_corrs, real_hils, imag_hils, dur_time_1_alls, dur_time_2_alls, jitters, shimmers, jitter_rap, hrs, feature_combine, slope, zcrs, fft_features, energy_alls, log_energy_alls, smoothe_energy_alls, legendres, double_support_time, pdps_new, lpcs, ceps_features]):\n",
    "    scaler = StandardScaler()\n",
    "    if len(feature.shape)==2:\n",
    "        X_train_i = feature[train_idx,:]\n",
    "        X_val_i = feature[val_idx,:]\n",
    "        X_test_i = feature[test_idx,:]\n",
    "        X_train_normalized_i = scaler.fit_transform(X_train_i)\n",
    "        X_val_normalized_i = scaler.transform(X_val_i)\n",
    "        X_test_normalized_i = scaler.transform(X_test_i)\n",
    "        scalers.append(scaler)\n",
    "    else:\n",
    "        X_train_i = feature[train_idx,:,:]\n",
    "        X_val_i = feature[val_idx,:,:]\n",
    "        X_test_i = feature[test_idx,:,:]\n",
    "        X_train_normalized_i = scaler.fit_transform(X_train_i.reshape(X_train_i.shape[0], -1)).reshape(X_train_i.shape)\n",
    "        X_val_normalized_i = scaler.transform(X_val_i.reshape(X_val_i.shape[0], -1)).reshape(X_val_i.shape)\n",
    "        X_test_normalized_i = scaler.transform(X_test_i.reshape(X_test_i.shape[0], -1)).reshape(X_test_i.shape)\n",
    "        scalers.append(scaler)\n",
    "    X_train_normalized.append(X_train_normalized_i)\n",
    "    X_val_normalized.append(X_val_normalized_i)\n",
    "    X_test_normalized.append(X_test_normalized_i)\n",
    "y_train = gts[train_idx,:]\n",
    "y_val = gts[val_idx,:]\n",
    "y_test = gts[test_idx,:]\n",
    "X_train_normalized_new = []\n",
    "combined_feature = np.empty((len(X_train_normalized[0]),0))\n",
    "for feature in X_train_normalized:\n",
    "    if len(feature.shape) == 3:\n",
    "        X_train_normalized_new.append(feature)\n",
    "    elif feature.shape[1] <20:\n",
    "        combined_feature = np.hstack((combined_feature, feature))\n",
    "    else:\n",
    "        X_train_normalized_new.append(feature)\n",
    "X_train_normalized_new.append(combined_feature)\n",
    "\n",
    "X_val_normalized_new = []\n",
    "combined_feature = np.empty((len(X_val_normalized[0]),0))\n",
    "for feature in X_val_normalized:\n",
    "    if len(feature.shape) == 3:\n",
    "        X_val_normalized_new.append(feature)\n",
    "    elif feature.shape[1] <20:\n",
    "        combined_feature = np.hstack((combined_feature, feature))\n",
    "    else:\n",
    "        X_val_normalized_new.append(feature)\n",
    "X_val_normalized_new.append(combined_feature)\n",
    "\n",
    "X_test_normalized_new = []\n",
    "combined_feature = np.empty((len(X_test_normalized[0]),0))\n",
    "for feature in X_test_normalized:\n",
    "    if len(feature.shape) == 3:\n",
    "        X_test_normalized_new.append(feature)\n",
    "    elif feature.shape[1] <20:\n",
    "        combined_feature = np.hstack((combined_feature, feature))\n",
    "    else:\n",
    "        X_test_normalized_new.append(feature)\n",
    "X_test_normalized_new.append(combined_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cac3469a-819d-4824-9d8c-75dfe11a299b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T02:58:14.663275Z",
     "iopub.status.busy": "2023-12-05T02:58:14.663138Z",
     "iopub.status.idle": "2023-12-05T02:58:39.936976Z",
     "shell.execute_reply": "2023-12-05T02:58:39.936421Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_555 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_556 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_557 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_558 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_559 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_560 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_561 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_562 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_563 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_564 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_565 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_566 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_567 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_568 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_569 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_570 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_571 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_572 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_573 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_574 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_575 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_576 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_577 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_578 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_579 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_580 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_581 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_582 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_583 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_584 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_585 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_586 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_587 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_588 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_589 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_590 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_591 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 21:58:19.190923: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_590/lstm_cell_590/kernel/Assign' id:144423 op device:{requested: '', assigned: ''} def:{{{node lstm_590/lstm_cell_590/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_590/lstm_cell_590/kernel, lstm_590/lstm_cell_590/kernel/Initializer/random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-04 21:58:25.824342: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_557/lstm_cell_557/recurrent_kernel/v/Assign' id:146111 op device:{requested: '', assigned: ''} def:{{{node lstm_557/lstm_cell_557/recurrent_kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_557/lstm_cell_557/recurrent_kernel/v, lstm_557/lstm_cell_557/recurrent_kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "/usr/lib/python3.11/site-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "2023-12-04 21:58:32.363223: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_63/BiasAdd' id:144834 op device:{requested: '', assigned: ''} def:{{{node dense_63/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_63/MatMul, dense_63/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE:\n",
      "1.1714587403589456\n",
      "[1.12702274 1.10667324]\n",
      "1.1168479900511483\n"
     ]
    }
   ],
   "source": [
    "## Evaluation on test data\n",
    "import tensorflow as tf\n",
    "m_name = \"./checkpoints/unknown_person_few_shot_p\"+str(test_person_id[0])+'_45.h5'\n",
    "model = tf.keras.models.load_model(m_name)\n",
    "from feature_emotion import feature_extract, split_data, label_unique_tuples\n",
    "trace_wlk_num = label_unique_tuples(people_nums_all, walk_nums_all, trace_nums_all)\n",
    "trace_wlk_num = np.array(trace_wlk_num )\n",
    "\n",
    "y_pred = model.predict(X_test_normalized_new)\n",
    "print('Test MAE:')\n",
    "err = np.mean(np.abs(y_pred - y_test))\n",
    "print(err)\n",
    "## Evaluation: trace median vote\n",
    "trace_num_test = trace_wlk_num[test_idx]\n",
    "u = np.unique(trace_num_test)\n",
    "pred_trace = np.empty((0, 2))\n",
    "gt_trace = np.empty((0, 2))\n",
    "for i in u:\n",
    "  trace_idx = np.where(trace_num_test == i)[0]\n",
    "  y_pred_trace = y_pred[trace_idx,:]\n",
    "  pred = np.mean(y_pred_trace, axis = 0)\n",
    "  pred_trace = np.vstack((pred_trace, pred))\n",
    "  gt_t = y_test[trace_idx[0],:]\n",
    "  gt_trace = np.vstack((gt_trace, gt_t))\n",
    "print(np.mean(np.abs(pred_trace-gt_trace), axis = 0))\n",
    "print(np.mean(np.abs(pred_trace-gt_trace)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40dd5f9f-94fd-4d5b-978f-8d5eb4373836",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T02:58:39.938790Z",
     "iopub.status.busy": "2023-12-05T02:58:39.938645Z",
     "iopub.status.idle": "2023-12-05T02:59:06.894535Z",
     "shell.execute_reply": "2023-12-05T02:59:06.893940Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_592 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_593 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_594 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_595 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_596 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_597 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_598 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_599 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_600 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_601 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_602 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_603 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_604 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_605 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_606 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_607 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_608 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_609 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_610 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_611 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_612 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_613 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_614 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_615 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_616 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_617 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_618 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_619 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_620 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_621 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_622 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_623 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_624 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_625 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_626 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_627 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_628 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 21:58:45.213076: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_624/lstm_cell_624/kernel/Assign' id:153174 op device:{requested: '', assigned: ''} def:{{{node lstm_624/lstm_cell_624/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_624/lstm_cell_624/kernel, lstm_624/lstm_cell_624/kernel/Initializer/random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-04 21:58:52.276520: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_612/lstm_cell_612/kernel/m/Assign' id:154964 op device:{requested: '', assigned: ''} def:{{{node lstm_612/lstm_cell_612/kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_612/lstm_cell_612/kernel/m, lstm_612/lstm_cell_612/kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-04 21:58:59.180749: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_67/BiasAdd' id:154065 op device:{requested: '', assigned: ''} def:{{{node dense_67/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_67/MatMul, dense_67/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE:\n",
      "1.208560192711139\n",
      "[1.10864413 1.17743578]\n",
      "1.1430399531409854\n"
     ]
    }
   ],
   "source": [
    "## Evaluation on test data\n",
    "import tensorflow as tf\n",
    "m_name = \"./checkpoints/unknown_person_few_shot_p\"+str(test_person_id[0])+'_46.h5'\n",
    "model = tf.keras.models.load_model(m_name)\n",
    "from feature_emotion import feature_extract, split_data, label_unique_tuples\n",
    "trace_wlk_num = label_unique_tuples(people_nums_all, walk_nums_all, trace_nums_all)\n",
    "trace_wlk_num = np.array(trace_wlk_num )\n",
    "\n",
    "y_pred = model.predict(X_test_normalized_new)\n",
    "print('Test MAE:')\n",
    "err = np.mean(np.abs(y_pred - y_test))\n",
    "print(err)\n",
    "## Evaluation: trace median vote\n",
    "trace_num_test = trace_wlk_num[test_idx]\n",
    "u = np.unique(trace_num_test)\n",
    "pred_trace = np.empty((0, 2))\n",
    "gt_trace = np.empty((0, 2))\n",
    "for i in u:\n",
    "  trace_idx = np.where(trace_num_test == i)[0]\n",
    "  y_pred_trace = y_pred[trace_idx,:]\n",
    "  pred = np.mean(y_pred_trace, axis = 0)\n",
    "  pred_trace = np.vstack((pred_trace, pred))\n",
    "  gt_t = y_test[trace_idx[0],:]\n",
    "  gt_trace = np.vstack((gt_trace, gt_t))\n",
    "print(np.mean(np.abs(pred_trace-gt_trace), axis = 0))\n",
    "print(np.mean(np.abs(pred_trace-gt_trace)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a71b7739-d59e-4ee5-9e5a-5031460d5214",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T02:59:06.896395Z",
     "iopub.status.busy": "2023-12-05T02:59:06.896253Z",
     "iopub.status.idle": "2023-12-05T02:59:34.554617Z",
     "shell.execute_reply": "2023-12-05T02:59:34.554048Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_629 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_630 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_631 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_632 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_633 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_634 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_635 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_636 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_637 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_638 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_639 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_640 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_641 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_642 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_643 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_644 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_645 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_646 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_647 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_648 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_649 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_650 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_651 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_652 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_653 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_654 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_655 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_656 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_657 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_658 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_659 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_660 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_661 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_662 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_663 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_664 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_665 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 21:59:11.803827: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_654/lstm_cell_654/kernel/Assign' id:161285 op device:{requested: '', assigned: ''} def:{{{node lstm_654/lstm_cell_654/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_654/lstm_cell_654/kernel, lstm_654/lstm_cell_654/kernel/Initializer/random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-04 21:59:19.230084: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_655/lstm_cell_655/recurrent_kernel/m/Assign' id:164290 op device:{requested: '', assigned: ''} def:{{{node lstm_655/lstm_cell_655/recurrent_kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_655/lstm_cell_655/recurrent_kernel/m, lstm_655/lstm_cell_655/recurrent_kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-04 21:59:26.594165: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_71/BiasAdd' id:163296 op device:{requested: '', assigned: ''} def:{{{node dense_71/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_71/MatMul, dense_71/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE:\n",
      "1.1724089733349583\n",
      "[1.07522558 1.1271018 ]\n",
      "1.1011636900523352\n"
     ]
    }
   ],
   "source": [
    "## Evaluation on test data\n",
    "import tensorflow as tf\n",
    "m_name = \"./checkpoints/unknown_person_few_shot_p\"+str(test_person_id[0])+'_47.h5'\n",
    "model = tf.keras.models.load_model(m_name)\n",
    "from feature_emotion import feature_extract, split_data, label_unique_tuples\n",
    "trace_wlk_num = label_unique_tuples(people_nums_all, walk_nums_all, trace_nums_all)\n",
    "trace_wlk_num = np.array(trace_wlk_num )\n",
    "\n",
    "y_pred = model.predict(X_test_normalized_new)\n",
    "print('Test MAE:')\n",
    "err = np.mean(np.abs(y_pred - y_test))\n",
    "print(err)\n",
    "## Evaluation: trace median vote\n",
    "trace_num_test = trace_wlk_num[test_idx]\n",
    "u = np.unique(trace_num_test)\n",
    "pred_trace = np.empty((0, 2))\n",
    "gt_trace = np.empty((0, 2))\n",
    "for i in u:\n",
    "  trace_idx = np.where(trace_num_test == i)[0]\n",
    "  y_pred_trace = y_pred[trace_idx,:]\n",
    "  pred = np.mean(y_pred_trace, axis = 0)\n",
    "  pred_trace = np.vstack((pred_trace, pred))\n",
    "  gt_t = y_test[trace_idx[0],:]\n",
    "  gt_trace = np.vstack((gt_trace, gt_t))\n",
    "print(np.mean(np.abs(pred_trace-gt_trace), axis = 0))\n",
    "print(np.mean(np.abs(pred_trace-gt_trace)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "138e04f2-2533-4de5-8767-30a2758a3f71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T02:59:34.556557Z",
     "iopub.status.busy": "2023-12-05T02:59:34.556415Z",
     "iopub.status.idle": "2023-12-05T03:00:25.842061Z",
     "shell.execute_reply": "2023-12-05T03:00:25.841471Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35129\n"
     ]
    }
   ],
   "source": [
    "from feature_emotion import feature_extract, split_data, label_unique_tuples, split_data_unknown\n",
    "gts, sensor_nums, walk_nums, trace_nums, people_nums, spe_centr, delta_spe_centr, spe_crest, delta_spe_crest, spe_decrease, delta_spe_decrease, spe_entropy, delta_spe_entropy, spe_flatness, delta_spe_flatness, spe_flux, delta_spe_flux, spe_kurtosis, delta_spe_kurtosis, spe_skewness, delta_spe_skewness, spe_rfp, delta_spe_rfp, spe_slope, delta_spe_slope, spe_spread, delta_spe_spread, wlk_fre, wlk_fres_trace, cwt_figs_all, cwt_sum_all_0, cwt_sum_all_1, cwt_sum_all_2, cwt_sum_all_3, high_fre_compos, pitchs, low_fre_compos, auto_corrs, real_hils, imag_hils, dur_time_1_alls, dur_time_2_alls, jitters, shimmers, jitter_rap, hrs, feature, slope, zcrs, fft_features, energy_alls, log_energy_alls, smoothe_energy_alls, legendres, double_support_time, pdps_new, lpcs, ceps_features = feature_extract(person_nums)\n",
    "\n",
    "walk_nums_all = np.squeeze(walk_nums)\n",
    "trace_nums_all = np.squeeze(trace_nums)\n",
    "people_nums_all = np.squeeze(people_nums)\n",
    "\n",
    "flag_tr_val_te = split_data_unknown(test_person_id, walk_nums_all, trace_nums_all, people_nums_all, person_nums, rand_seed = 42)\n",
    "## Data Normalization before training ans testing\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_v2_behavior()\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, concatenate, LSTM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scalers = []\n",
    "X_train_normalized = []\n",
    "X_val_normalized = []\n",
    "X_test_normalized = []\n",
    "train_idx = np.where(flag_tr_val_te ==0)[0]\n",
    "np.random.shuffle(train_idx)\n",
    "val_idx = np.where(flag_tr_val_te ==1)[0]\n",
    "test_idx = np.where(flag_tr_val_te ==2)[0]\n",
    "\n",
    "for i, feature in enumerate([spe_centr, delta_spe_centr, spe_crest, delta_spe_crest, spe_decrease, delta_spe_decrease, spe_entropy, delta_spe_entropy, spe_flatness, delta_spe_flatness, spe_flux, delta_spe_flux, spe_kurtosis, delta_spe_kurtosis, spe_skewness, delta_spe_skewness, spe_rfp, delta_spe_rfp, spe_slope, delta_spe_slope, spe_spread, delta_spe_spread, wlk_fre, wlk_fres_trace, cwt_figs_all, cwt_sum_all_0, cwt_sum_all_1, cwt_sum_all_2, cwt_sum_all_3, high_fre_compos, pitchs, low_fre_compos, auto_corrs, real_hils, imag_hils, dur_time_1_alls, dur_time_2_alls, jitters, shimmers, jitter_rap, hrs, feature, slope, zcrs, fft_features, energy_alls, log_energy_alls, smoothe_energy_alls, legendres, double_support_time, pdps_new, lpcs, ceps_features]):\n",
    "    scaler = StandardScaler()\n",
    "    if len(feature.shape)==2:\n",
    "        X_train_i = feature[train_idx,:]\n",
    "        X_val_i = feature[val_idx,:]\n",
    "        X_test_i = feature[test_idx,:]\n",
    "        X_train_normalized_i = scaler.fit_transform(X_train_i)\n",
    "        X_val_normalized_i = scaler.transform(X_val_i)\n",
    "        X_test_normalized_i = scaler.transform(X_test_i)\n",
    "        scalers.append(scaler)\n",
    "    else:\n",
    "        X_train_i = feature[train_idx,:,:]\n",
    "        X_val_i = feature[val_idx,:,:]\n",
    "        X_test_i = feature[test_idx,:,:]\n",
    "        X_train_normalized_i = scaler.fit_transform(X_train_i.reshape(X_train_i.shape[0], -1)).reshape(X_train_i.shape)\n",
    "        X_val_normalized_i = scaler.transform(X_val_i.reshape(X_val_i.shape[0], -1)).reshape(X_val_i.shape)\n",
    "        X_test_normalized_i = scaler.transform(X_test_i.reshape(X_test_i.shape[0], -1)).reshape(X_test_i.shape)\n",
    "        scalers.append(scaler)\n",
    "    X_train_normalized.append(X_train_normalized_i)\n",
    "    X_val_normalized.append(X_val_normalized_i)\n",
    "    X_test_normalized.append(X_test_normalized_i)\n",
    "y_train = gts[train_idx,:]\n",
    "y_val = gts[val_idx,:]\n",
    "y_test = gts[test_idx,:]\n",
    "X_train_normalized_new = []\n",
    "combined_feature = np.empty((len(X_train_normalized[0]),0))\n",
    "for feature in X_train_normalized:\n",
    "    if len(feature.shape) == 3:\n",
    "        X_train_normalized_new.append(feature)\n",
    "    elif feature.shape[1] <20:\n",
    "        combined_feature = np.hstack((combined_feature, feature))\n",
    "    else:\n",
    "        X_train_normalized_new.append(feature)\n",
    "X_train_normalized_new.append(combined_feature)\n",
    "\n",
    "X_val_normalized_new = []\n",
    "combined_feature = np.empty((len(X_val_normalized[0]),0))\n",
    "for feature in X_val_normalized:\n",
    "    if len(feature.shape) == 3:\n",
    "        X_val_normalized_new.append(feature)\n",
    "    elif feature.shape[1] <20:\n",
    "        combined_feature = np.hstack((combined_feature, feature))\n",
    "    else:\n",
    "        X_val_normalized_new.append(feature)\n",
    "X_val_normalized_new.append(combined_feature)\n",
    "\n",
    "X_test_normalized_new = []\n",
    "combined_feature = np.empty((len(X_test_normalized[0]),0))\n",
    "for feature in X_test_normalized:\n",
    "    if len(feature.shape) == 3:\n",
    "        X_test_normalized_new.append(feature)\n",
    "    elif feature.shape[1] <20:\n",
    "        combined_feature = np.hstack((combined_feature, feature))\n",
    "    else:\n",
    "        X_test_normalized_new.append(feature)\n",
    "X_test_normalized_new.append(combined_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b42e7166-8c0c-44ab-8fe9-fb96d9f8555d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T03:00:25.844084Z",
     "iopub.status.busy": "2023-12-05T03:00:25.843956Z",
     "iopub.status.idle": "2023-12-05T03:01:01.132342Z",
     "shell.execute_reply": "2023-12-05T03:01:01.131730Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_16 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_17 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_18 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_19 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_20 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_22 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_23 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_24 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_25 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_26 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_27 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_28 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_29 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_30 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_31 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_32 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_33 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_34 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_35 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_36 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 22:00:30.948442: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_1_1/lstm_cell_667/kernel/Assign' id:166674 op device:{requested: '', assigned: ''} def:{{{node lstm_1_1/lstm_cell_667/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_1_1/lstm_cell_667/kernel, lstm_1_1/lstm_cell_667/kernel/Initializer/random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-04 22:00:38.829416: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_25_1/lstm_cell_691/recurrent_kernel/v/Assign' id:174149 op device:{requested: '', assigned: ''} def:{{{node lstm_25_1/lstm_cell_691/recurrent_kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_25_1/lstm_cell_691/recurrent_kernel/v, lstm_25_1/lstm_cell_691/recurrent_kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "/usr/lib/python3.11/site-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "2023-12-04 22:00:46.549198: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_3_1/BiasAdd' id:172527 op device:{requested: '', assigned: ''} def:{{{node dense_3_1/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_3_1/MatMul, dense_3_1/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE:\n",
      "1.9946155191486716\n",
      "[1.70527405 2.14309874]\n",
      "1.9241863932365026\n"
     ]
    }
   ],
   "source": [
    "## Evaluation on test data\n",
    "import tensorflow as tf\n",
    "m_name = \"./checkpoints/unknown_person_p\"+str(test_person_id[0])+'_20.h5'\n",
    "model = tf.keras.models.load_model(m_name)\n",
    "from feature_emotion import feature_extract, split_data, label_unique_tuples\n",
    "trace_wlk_num = label_unique_tuples(people_nums_all, walk_nums_all, trace_nums_all)\n",
    "trace_wlk_num = np.array(trace_wlk_num )\n",
    "\n",
    "y_pred = model.predict(X_test_normalized_new)\n",
    "print('Test MAE:')\n",
    "err = np.mean(np.abs(y_pred - y_test))\n",
    "print(err)\n",
    "## Evaluation: trace median vote\n",
    "trace_num_test = trace_wlk_num[test_idx]\n",
    "u = np.unique(trace_num_test)\n",
    "pred_trace = np.empty((0, 2))\n",
    "gt_trace = np.empty((0, 2))\n",
    "for i in u:\n",
    "  trace_idx = np.where(trace_num_test == i)[0]\n",
    "  y_pred_trace = y_pred[trace_idx,:]\n",
    "  pred = np.mean(y_pred_trace, axis = 0)\n",
    "  pred_trace = np.vstack((pred_trace, pred))\n",
    "  gt_t = y_test[trace_idx[0],:]\n",
    "  gt_trace = np.vstack((gt_trace, gt_t))\n",
    "print(np.mean(np.abs(pred_trace-gt_trace), axis = 0))\n",
    "print(np.mean(np.abs(pred_trace-gt_trace)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dac2fb56-f564-4cf3-b0ce-9320ddbc0fc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T03:01:01.134201Z",
     "iopub.status.busy": "2023-12-05T03:01:01.134061Z",
     "iopub.status.idle": "2023-12-05T03:01:37.705474Z",
     "shell.execute_reply": "2023-12-05T03:01:37.704929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_16 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_17 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_18 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_19 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_20 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_22 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_23 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_24 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_25 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_26 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_27 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_28 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_29 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_30 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_31 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_32 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_33 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_34 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_35 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_36 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 22:01:06.398979: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_4_2/lstm_cell_707/kernel/Assign' id:176385 op device:{requested: '', assigned: ''} def:{{{node lstm_4_2/lstm_cell_707/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_4_2/lstm_cell_707/kernel, lstm_4_2/lstm_cell_707/kernel/Initializer/random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-04 22:01:14.645405: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_4_2/lstm_cell_707/recurrent_kernel/v/Assign' id:183065 op device:{requested: '', assigned: ''} def:{{{node lstm_4_2/lstm_cell_707/recurrent_kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_4_2/lstm_cell_707/recurrent_kernel/v, lstm_4_2/lstm_cell_707/recurrent_kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-04 22:01:22.798695: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_3_2/BiasAdd' id:181758 op device:{requested: '', assigned: ''} def:{{{node dense_3_2/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_3_2/MatMul, dense_3_2/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE:\n",
      "1.9390991712864647\n",
      "[1.64383088 2.08170892]\n",
      "1.8627698994599855\n"
     ]
    }
   ],
   "source": [
    "## Evaluation on test data\n",
    "import tensorflow as tf\n",
    "m_name = \"./checkpoints/unknown_person_p\"+str(test_person_id[0])+'_21.h5'\n",
    "model = tf.keras.models.load_model(m_name)\n",
    "from feature_emotion import feature_extract, split_data, label_unique_tuples\n",
    "trace_wlk_num = label_unique_tuples(people_nums_all, walk_nums_all, trace_nums_all)\n",
    "trace_wlk_num = np.array(trace_wlk_num )\n",
    "\n",
    "y_pred = model.predict(X_test_normalized_new)\n",
    "print('Test MAE:')\n",
    "err = np.mean(np.abs(y_pred - y_test))\n",
    "print(err)\n",
    "## Evaluation: trace median vote\n",
    "trace_num_test = trace_wlk_num[test_idx]\n",
    "u = np.unique(trace_num_test)\n",
    "pred_trace = np.empty((0, 2))\n",
    "gt_trace = np.empty((0, 2))\n",
    "for i in u:\n",
    "  trace_idx = np.where(trace_num_test == i)[0]\n",
    "  y_pred_trace = y_pred[trace_idx,:]\n",
    "  pred = np.mean(y_pred_trace, axis = 0)\n",
    "  pred_trace = np.vstack((pred_trace, pred))\n",
    "  gt_t = y_test[trace_idx[0],:]\n",
    "  gt_trace = np.vstack((gt_trace, gt_t))\n",
    "print(np.mean(np.abs(pred_trace-gt_trace), axis = 0))\n",
    "print(np.mean(np.abs(pred_trace-gt_trace)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b49329c-9390-46e3-a0ba-8e4d8236de89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T03:01:37.707454Z",
     "iopub.status.busy": "2023-12-05T03:01:37.707314Z",
     "iopub.status.idle": "2023-12-05T03:02:14.965661Z",
     "shell.execute_reply": "2023-12-05T03:02:14.965082Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_16 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_17 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_18 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_19 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_20 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_22 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_23 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_24 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_25 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_26 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_27 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_28 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_29 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_30 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_31 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_32 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_33 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_34 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_35 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_36 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 22:01:43.145843: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_18_3/lstm_cell_758/recurrent_kernel/Assign' id:187876 op device:{requested: '', assigned: ''} def:{{{node lstm_18_3/lstm_cell_758/recurrent_kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_18_3/lstm_cell_758/recurrent_kernel, lstm_18_3/lstm_cell_758/recurrent_kernel/Initializer/mul_1)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-04 22:01:51.784570: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_6_3/lstm_cell_746/kernel/m/Assign' id:191678 op device:{requested: '', assigned: ''} def:{{{node lstm_6_3/lstm_cell_746/kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_6_3/lstm_cell_746/kernel/m, lstm_6_3/lstm_cell_746/kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-12-04 22:02:00.318058: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_3_3/BiasAdd' id:190989 op device:{requested: '', assigned: ''} def:{{{node dense_3_3/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_3_3/MatMul, dense_3_3/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE:\n",
      "1.908119241332906\n",
      "[1.62187691 2.0544264 ]\n",
      "1.8381516528435242\n"
     ]
    }
   ],
   "source": [
    "## Evaluation on test data\n",
    "import tensorflow as tf\n",
    "m_name = \"./checkpoints/unknown_person_p\"+str(test_person_id[0])+'_22.h5'\n",
    "model = tf.keras.models.load_model(m_name)\n",
    "from feature_emotion import feature_extract, split_data, label_unique_tuples\n",
    "trace_wlk_num = label_unique_tuples(people_nums_all, walk_nums_all, trace_nums_all)\n",
    "trace_wlk_num = np.array(trace_wlk_num )\n",
    "\n",
    "y_pred = model.predict(X_test_normalized_new)\n",
    "print('Test MAE:')\n",
    "err = np.mean(np.abs(y_pred - y_test))\n",
    "print(err)\n",
    "## Evaluation: trace median vote\n",
    "trace_num_test = trace_wlk_num[test_idx]\n",
    "u = np.unique(trace_num_test)\n",
    "pred_trace = np.empty((0, 2))\n",
    "gt_trace = np.empty((0, 2))\n",
    "for i in u:\n",
    "  trace_idx = np.where(trace_num_test == i)[0]\n",
    "  y_pred_trace = y_pred[trace_idx,:]\n",
    "  pred = np.mean(y_pred_trace, axis = 0)\n",
    "  pred_trace = np.vstack((pred_trace, pred))\n",
    "  gt_t = y_test[trace_idx[0],:]\n",
    "  gt_trace = np.vstack((gt_trace, gt_t))\n",
    "print(np.mean(np.abs(pred_trace-gt_trace), axis = 0))\n",
    "print(np.mean(np.abs(pred_trace-gt_trace)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
