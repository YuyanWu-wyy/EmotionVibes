nohup: ignoring input
Converting unknown_baseline_p1_few_shot.ipynb...
[NbConvertApp] Converting notebook unknown_baseline_p1_few_shot.ipynb to notebook
0.00s - Debugger warning: It seems that frozen modules are being used, which may
0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off
0.00s - to python to disable frozen modules.
0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.
0.00s - Debugger warning: It seems that frozen modules are being used, which may
0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off
0.00s - to python to disable frozen modules.
0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.
2023-11-15 16:58:38.006646: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-11-15 17:00:02.162200: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-11-15 17:00:02.174073: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-11-15 17:00:02.269358: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-11-15 17:00:02.273324: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-11-15 17:00:32.277311: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-11-15 17:00:32.277802: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-11-15 17:00:32.370230: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-11-15 17:00:32.370432: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-11-15 17:00:32.370596: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-11-15 17:00:32.370737: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46586 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:2d:00.0, compute capability: 8.6
2023-11-15 17:00:32.370993: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
2023-11-15 17:00:33.314149: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-11-15 17:00:33.314389: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-11-15 17:00:33.314562: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-11-15 17:00:33.314771: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-11-15 17:00:33.315017: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-11-15 17:00:33.315215: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46586 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:2d:00.0, compute capability: 8.6
2023-11-15 17:00:33.315247: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
2023-11-15 17:00:33.337342: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled
2023-11-15 17:00:33.555020: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_21/lstm_cell_21/bias/Assign' id:3510 op device:{requested: '', assigned: ''} def:{{{node lstm_21/lstm_cell_21/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_21/lstm_cell_21/bias, lstm_21/lstm_cell_21/bias/Initializer/concat)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
[NbConvertApp] ERROR | Kernel died while waiting for execute reply.
Traceback (most recent call last):
  File "/usr/bin/jupyter-nbconvert", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/usr/lib/python3.11/site-packages/jupyter_core/application.py", line 285, in launch_instance
    return super().launch_instance(argv=argv, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/site-packages/traitlets/config/application.py", line 1043, in launch_instance
    app.start()
  File "/usr/lib/python3.11/site-packages/nbconvert/nbconvertapp.py", line 410, in start
    self.convert_notebooks()
  File "/usr/lib/python3.11/site-packages/nbconvert/nbconvertapp.py", line 585, in convert_notebooks
    self.convert_single_notebook(notebook_filename)
  File "/usr/lib/python3.11/site-packages/nbconvert/nbconvertapp.py", line 551, in convert_single_notebook
    output, resources = self.export_single_notebook(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/site-packages/nbconvert/nbconvertapp.py", line 477, in export_single_notebook
    output, resources = self.exporter.from_filename(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/site-packages/nbconvert/exporters/exporter.py", line 201, in from_filename
    return self.from_file(f, resources=resources, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/site-packages/nbconvert/exporters/exporter.py", line 220, in from_file
    return self.from_notebook_node(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/site-packages/nbconvert/exporters/notebook.py", line 36, in from_notebook_node
    nb_copy, resources = super().from_notebook_node(nb, resources, **kw)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/site-packages/nbconvert/exporters/exporter.py", line 154, in from_notebook_node
    nb_copy, resources = self._preprocess(nb_copy, resources)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/site-packages/nbconvert/exporters/exporter.py", line 352, in _preprocess
    nbc, resc = preprocessor(nbc, resc)
                ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/site-packages/nbconvert/preprocessors/base.py", line 48, in __call__
    return self.preprocess(nb, resources)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/site-packages/nbconvert/preprocessors/execute.py", line 100, in preprocess
    self.preprocess_cell(cell, resources, index)
  File "/usr/lib/python3.11/site-packages/nbconvert/preprocessors/execute.py", line 121, in preprocess_cell
    cell = self.execute_cell(cell, index, store_history=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/site-packages/jupyter_core/utils/__init__.py", line 166, in wrapped
    return loop.run_until_complete(inner)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/asyncio/base_events.py", line 653, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/site-packages/nbclient/client.py", line 1005, in async_execute_cell
    raise DeadKernelError("Kernel died") from None
nbclient.exceptions.DeadKernelError: Kernel died
Error converting unknown_baseline_p1_few_shot.ipynb. Exiting.
nohup: ignoring input
Converting unknown_baseline_p1_few_shot.ipynb...
[NbConvertApp] Converting notebook unknown_baseline_p1_few_shot.ipynb to notebook
0.00s - Debugger warning: It seems that frozen modules are being used, which may
0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off
0.00s - to python to disable frozen modules.
0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.
0.00s - Debugger warning: It seems that frozen modules are being used, which may
0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off
0.00s - to python to disable frozen modules.
0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.
2023-11-15 17:01:47.703810: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-11-15 17:03:13.824619: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-11-15 17:03:13.837212: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-11-15 17:03:13.837466: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-11-15 17:03:13.841437: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-11-15 17:03:13.841774: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-11-15 17:03:13.841950: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-11-15 17:04:13.959710: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-11-15 17:04:13.962637: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-11-15 17:04:13.963477: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-11-15 17:04:13.964638: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46594 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:2d:00.0, compute capability: 8.6
2023-11-15 17:04:13.965192: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
2023-11-15 17:04:14.932732: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-11-15 17:04:14.933039: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-11-15 17:04:14.933230: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-11-15 17:04:14.933450: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-11-15 17:04:14.933628: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-11-15 17:04:14.933765: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46594 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:2d:00.0, compute capability: 8.6
2023-11-15 17:04:14.933797: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
2023-11-15 17:04:14.962473: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled
2023-11-15 17:04:15.187938: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_21/lstm_cell_21/bias/Assign' id:3510 op device:{requested: '', assigned: ''} def:{{{node lstm_21/lstm_cell_21/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_21/lstm_cell_21/bias, lstm_21/lstm_cell_21/bias/Initializer/concat)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
2023-11-15 17:04:15.328349: W tensorflow/c/c_api.cc:304] Operation '{name:'strided_slice/stack_1' id:3634 op device:{requested: '', assigned: ''} def:{{{node strided_slice/stack_1}} = Const[_has_manual_control_dependencies=true, dtype=DT_INT32, value=Tensor<type: int32 shape: [4] values: 0 0 0...>]()}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
2023-11-15 17:04:15.363697: W tensorflow/c/c_api.cc:304] Operation '{name:'strided_slice/stack_2' id:3635 op device:{requested: '', assigned: ''} def:{{{node strided_slice/stack_2}} = Const[_has_manual_control_dependencies=true, dtype=DT_INT32, value=Tensor<type: int32 shape: [4] values: 1 1 1...>]()}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
2023-11-15 17:04:19.926611: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_24/lstm_cell_24/bias/Assign' id:4236 op device:{requested: '', assigned: ''} def:{{{node lstm_24/lstm_cell_24/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_24/lstm_cell_24/bias, lstm_24/lstm_cell_24/bias/Initializer/concat)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
2023-11-15 17:04:24.252268: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape indropout/cond/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer
2023-11-15 17:04:26.796917: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8902
2023-11-15 17:04:27.036557: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
2023-11-15 17:04:47.256112: W tensorflow/c/c_api.cc:304] Operation '{name:'loss_1/mul' id:6477 op device:{requested: '', assigned: ''} def:{{{node loss_1/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_1/mul/x, loss_1/dense_3_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
2023-11-15 17:21:57.100633: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_27_1/lstm_cell_64/recurrent_kernel/Assign' id:22982 op device:{requested: '', assigned: ''} def:{{{node lstm_27_1/lstm_cell_64/recurrent_kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_27_1/lstm_cell_64/recurrent_kernel, lstm_27_1/lstm_cell_64/recurrent_kernel/Initializer/mul_1)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
2023-11-15 17:21:58.824861: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_30_1/lstm_cell_67/recurrent_kernel/v/Assign' id:26350 op device:{requested: '', assigned: ''} def:{{{node lstm_30_1/lstm_cell_67/recurrent_kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_30_1/lstm_cell_67/recurrent_kernel/v, lstm_30_1/lstm_cell_67/recurrent_kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
2023-11-15 17:22:00.206675: W tensorflow/c/c_api.cc:304] Operation '{name:'dropout_4_1/cond/Merge' id:24591 op device:{requested: '', assigned: ''} def:{{{node dropout_4_1/cond/Merge}} = Merge[N=2, T=DT_FLOAT, _has_manual_control_dependencies=true](dropout_4_1/cond/Identity, dropout_4_1/cond/dropout/SelectV2)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Traceback (most recent call last):
  File "/usr/bin/jupyter-nbconvert", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/usr/lib/python3.11/site-packages/jupyter_core/application.py", line 285, in launch_instance
    return super().launch_instance(argv=argv, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/site-packages/traitlets/config/application.py", line 1043, in launch_instance
    app.start()
  File "/usr/lib/python3.11/site-packages/nbconvert/nbconvertapp.py", line 410, in start
    self.convert_notebooks()
  File "/usr/lib/python3.11/site-packages/nbconvert/nbconvertapp.py", line 585, in convert_notebooks
    self.convert_single_notebook(notebook_filename)
  File "/usr/lib/python3.11/site-packages/nbconvert/nbconvertapp.py", line 551, in convert_single_notebook
    output, resources = self.export_single_notebook(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/site-packages/nbconvert/nbconvertapp.py", line 477, in export_single_notebook
    output, resources = self.exporter.from_filename(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/site-packages/nbconvert/exporters/exporter.py", line 201, in from_filename
    return self.from_file(f, resources=resources, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/site-packages/nbconvert/exporters/exporter.py", line 220, in from_file
    return self.from_notebook_node(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/site-packages/nbconvert/exporters/notebook.py", line 36, in from_notebook_node
    nb_copy, resources = super().from_notebook_node(nb, resources, **kw)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/site-packages/nbconvert/exporters/exporter.py", line 154, in from_notebook_node
    nb_copy, resources = self._preprocess(nb_copy, resources)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/site-packages/nbconvert/exporters/exporter.py", line 352, in _preprocess
    nbc, resc = preprocessor(nbc, resc)
                ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/site-packages/nbconvert/preprocessors/base.py", line 48, in __call__
    return self.preprocess(nb, resources)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/site-packages/nbconvert/preprocessors/execute.py", line 100, in preprocess
    self.preprocess_cell(cell, resources, index)
  File "/usr/lib/python3.11/site-packages/nbconvert/preprocessors/execute.py", line 121, in preprocess_cell
    cell = self.execute_cell(cell, index, store_history=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/site-packages/jupyter_core/utils/__init__.py", line 166, in wrapped
    return loop.run_until_complete(inner)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/asyncio/base_events.py", line 653, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/site-packages/nbclient/client.py", line 1058, in async_execute_cell
    await self._check_raise_for_error(cell, cell_index, exec_reply)
  File "/usr/lib/python3.11/site-packages/nbclient/client.py", line 914, in _check_raise_for_error
    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
------------------
person_nums = [1,2,4,5,6,8,9,10,11,12,13,17,19,21,22,25,26,27,28,29]
## Build the baseline model for emotion recognition with dropout layers
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, concatenate, SimpleRNN, LSTM, Conv2D, Flatten, MaxPooling2D, GRU, AveragePooling2D, Dropout, Lambda
from tensorflow.keras.callbacks import ModelCheckpoint
from sklearn.preprocessing import StandardScaler
def individual_model(features_list):
    input_layers = []
    hidden_layers = []
    combined_feature = np.empty((len(features_list[0]),0))
    for i, feature in enumerate(features_list):
        
        if len(feature.shape) == 3:
            input_i = Input(shape=(feature.shape[1], feature.shape[2]))
            input_layers.append(input_i)

            hidden_i = input_i[:,:,:,None]
            hidden_i = Conv2D(32, 3, activation='relu')(hidden_i)
            hidden_i = AveragePooling2D((3, 3))(hidden_i)
            hidden_i = Dropout(0.5)(hidden_i)
            hidden_i = Conv2D(16, 3, activation='relu')(hidden_i)
            hidden_i = AveragePooling2D((3, 3))(hidden_i)
            hidden_i = Dropout(0.5)(hidden_i)
            hidden_i = Conv2D(8, 1, activation='relu')(hidden_i)
            hidden_i = AveragePooling2D((2, 2))(hidden_i)
            hidden_i = Dropout(0.5)(hidden_i)
            hidden_i = Conv2D(4, 1, activation='relu')(hidden_i)
            hidden_i = AveragePooling2D((2, 2))(hidden_i)
            hidden_i = Dropout(0.5)(hidden_i)
            hidden_i = Flatten()(hidden_i)

            hidden_layers.append(hidden_i)
        elif feature.shape[1] <20:
            combined_feature = np.hstack((combined_feature, feature))
            
        else:  # For series features
            input_i = Input(shape=(feature.shape[1],))
            input_layers.append(input_i)
            hidden_i = Lambda(lambda x: x[:, :, None])(input_i)  # Add a new dimension
            hidden_i = LSTM(4)(hidden_i)
            hidden_layers.append(hidden_i)
    input_i = Input(shape=(combined_feature.shape[1],))
    input_layers.append(input_i)
    dense_num = np.max((1, int(combined_feature.shape[1]/2)))
    hidden_i = Dense(dense_num, activation='relu')(input_i)
    hidden_layers.append(hidden_i)
    print(combined_feature.shape)
    concat_layer = concatenate(hidden_layers)
    h = Dropout(0.2)(concat_layer)
    h = Dense(64, activation='relu')(h)
    h = Dense(32, activation='relu')(h)
    output_layer = Dense(2)(h)
    model = Model(inputs=input_layers, outputs=output_layer)
    model.compile(loss='mean_absolute_error', optimizer='adam')
    return model

import tensorflow as tf
from tensorflow.keras.callbacks import Callback
import numpy as np

class PruningCallback(tf.keras.callbacks.Callback):
    def __init__(self, initial_pruning_factor=0.1, final_pruning_factor=0.5, start_epoch=0, end_epoch=None, frequency=1):
        super(PruningCallback, self).__init__()
        self.initial_pruning_factor = initial_pruning_factor
        self.final_pruning_factor = final_pruning_factor
        self.start_epoch = start_epoch
        self.end_epoch = end_epoch if end_epoch is not None else np.inf
        self.frequency = frequency
        self.pruned_weights = {}
        self.layer_importance = {}

    def get_pruning_factor(self, epoch):
        if epoch < self.start_epoch:
            return 0
        if epoch > self.end_epoch:
            return self.final_pruning_factor
        return self.initial_pruning_factor + (self.final_pruning_factor - self.initial_pruning_factor) * (epoch - self.start_epoch) / (self.end_epoch - self.start_epoch)

    def on_train_begin(self, logs=None):
        total_weight_magnitude = 0
        for layer in self.model.layers:
            if hasattr(layer, 'get_weights'):
                weights = layer.get_weights()
                layer_norm = sum(np.linalg.norm(w) for w in weights)
                total_weight_magnitude += layer_norm
                self.layer_importance[layer.name] = layer_norm
    
        # Normalize the layer importance values so they sum up to 1
        for layer_name in self.layer_importance:
            self.layer_importance[layer_name] /= total_weight_magnitude
    # def on_train_begin(self, logs=None):
    #     total_weight_magnitude = sum([np.linalg.norm(layer.get_weights()) for layer in self.model.layers if hasattr(layer, 'get_weights')])
    #     for layer in self.model.layers:
    #         if hasattr(layer, 'get_weights'):
    #             self.layer_importance[layer.name] = np.linalg.norm(layer.get_weights()) / total_weight_magnitude

    def get_layer_pruning_factor(self, layer_name, global_pruning_factor):
        if layer_name in self.layer_importance:
            importance = self.layer_importance[layer_name]
            adjusted_pruning_factor = global_pruning_factor * (1 - importance)
            return min(adjusted_pruning_factor, 1)  # Ensure the pruning factor is not greater than 1
        return global_pruning_factor
    def prune_weights(self, layer, global_pruning_factor):
        
        weights = layer.get_weights()
        layer_name = layer.name
        pruning_factor = self.get_layer_pruning_factor(layer_name, global_pruning_factor)

        if layer_name not in self.pruned_weights:
            self.pruned_weights[layer_name] = [np.zeros_like(w, dtype=bool) for w in weights]
        
        for i in range(len(weights)):
            weight = weights[i]
            # print(weight.shape)
            # print(weight.size)
            if weight.ndim > 1:  # Only prune dense or convolutional layers
                unpruned_weights = np.logical_not(self.pruned_weights[layer_name][i])
                num_unpruned = np.sum(unpruned_weights)
                num_pruning = min(num_unpruned, int(weight.size * pruning_factor) - np.sum(self.pruned_weights[layer_name][i]))
                num_pruning = int(weight.size * pruning_factor) - np.sum(self.pruned_weights[layer_name][i])
                if num_pruning > 0:
                    unpruned_flat_indices = np.flatnonzero(unpruned_weights)
                    abs_unpruned_weights = np.abs(weight[unpruned_weights])
                    pruning_flat_indices = np.argpartition(abs_unpruned_weights, num_pruning)[:num_pruning]
                    
                    indices = np.unravel_index(pruning_flat_indices, weight.shape)
                    self.pruned_weights[layer_name][i][indices] = True

                weights[i] = weights[i]*(~self.pruned_weights[layer_name][i])
                
        layer.set_weights(weights)

    def on_epoch_end(self, epoch, logs=None):
        if (epoch - self.start_epoch) % self.frequency != 0:
            return

        pruning_factor = self.get_pruning_factor(epoch)
        for layer in self.model.layers:
            if isinstance(layer, (tf.keras.layers.Dense, tf.keras.layers.Conv2D)):
                self.prune_weights(layer, pruning_factor)
from feature_emotion import feature_extract, split_data, label_unique_tuples, split_data_unknown, split_data_few_shot
gts, sensor_nums, walk_nums, trace_nums, people_nums, spe_centr, delta_spe_centr, spe_crest, delta_spe_crest, spe_decrease, delta_spe_decrease, spe_entropy, delta_spe_entropy, spe_flatness, delta_spe_flatness, spe_flux, delta_spe_flux, spe_kurtosis, delta_spe_kurtosis, spe_skewness, delta_spe_skewness, spe_rfp, delta_spe_rfp, spe_slope, delta_spe_slope, spe_spread, delta_spe_spread, wlk_fre, wlk_fres_trace, cwt_figs_all, cwt_sum_all_0, cwt_sum_all_1, cwt_sum_all_2, cwt_sum_all_3, high_fre_compos, pitchs, low_fre_compos, auto_corrs, real_hils, imag_hils, dur_time_1_alls, dur_time_2_alls, jitters, shimmers, jitter_rap, hrs, feature, slope, zcrs, fft_features, energy_alls, log_energy_alls, smoothe_energy_alls, legendres, double_support_time, pdps_new, lpcs, ceps_features = feature_extract(person_nums)

walk_nums_all = np.squeeze(walk_nums)
trace_nums_all = np.squeeze(trace_nums)
people_nums_all = np.squeeze(people_nums)

## 0: train, 1: validation 2: test

test_person_id = [1]
ra_all = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]
idx = 30
for ra in ra_all:
    flag_tr_val_te = split_data_few_shot(test_person_id, walk_nums_all, trace_nums_all, people_nums_all, person_nums, ratio = ra, rand_seed = 42)
    ## Data Normalization before training ans testing
    import tensorflow as tf
    tf.compat.v1.disable_v2_behavior()
    from tensorflow.keras.models import Model
    from tensorflow.keras.layers import Input, Dense, concatenate, LSTM
    from sklearn.preprocessing import StandardScaler
    scalers = []
    X_train_normalized = []
    X_val_normalized = []
    X_test_normalized = []
    train_idx = np.where(flag_tr_val_te ==0)[0]
    np.random.shuffle(train_idx)
    val_idx = np.where(flag_tr_val_te ==1)[0]
    test_idx = np.where(flag_tr_val_te ==2)[0]
    
    for i, feature in enumerate([spe_centr, delta_spe_centr, spe_crest, delta_spe_crest, spe_decrease, delta_spe_decrease, spe_entropy, delta_spe_entropy, spe_flatness, delta_spe_flatness, spe_flux, delta_spe_flux, spe_kurtosis, delta_spe_kurtosis, spe_skewness, delta_spe_skewness, spe_rfp, delta_spe_rfp, spe_slope, delta_spe_slope, spe_spread, delta_spe_spread, wlk_fre, wlk_fres_trace, cwt_figs_all, cwt_sum_all_0, cwt_sum_all_1, cwt_sum_all_2, cwt_sum_all_3, high_fre_compos, pitchs, low_fre_compos, auto_corrs, real_hils, imag_hils, dur_time_1_alls, dur_time_2_alls, jitters, shimmers, jitter_rap, hrs, feature, slope, zcrs, fft_features, energy_alls, log_energy_alls, smoothe_energy_alls, legendres, double_support_time, pdps_new, lpcs, ceps_features]):
        scaler = StandardScaler()
        if len(feature.shape)==2:
            X_train_i = feature[train_idx,:]
            X_val_i = feature[val_idx,:]
            X_test_i = feature[test_idx,:]
            X_train_normalized_i = scaler.fit_transform(X_train_i)
            X_val_normalized_i = scaler.transform(X_val_i)
            X_test_normalized_i = scaler.transform(X_test_i)
            scalers.append(scaler)
        else:
            X_train_i = feature[train_idx,:,:]
            X_val_i = feature[val_idx,:,:]
            X_test_i = feature[test_idx,:,:]
            X_train_normalized_i = scaler.fit_transform(X_train_i.reshape(X_train_i.shape[0], -1)).reshape(X_train_i.shape)
            X_val_normalized_i = scaler.transform(X_val_i.reshape(X_val_i.shape[0], -1)).reshape(X_val_i.shape)
            X_test_normalized_i = scaler.transform(X_test_i.reshape(X_test_i.shape[0], -1)).reshape(X_test_i.shape)
            scalers.append(scaler)
        X_train_normalized.append(X_train_normalized_i)
        X_val_normalized.append(X_val_normalized_i)
        X_test_normalized.append(X_test_normalized_i)
    y_train = gts[train_idx,:]
    y_val = gts[val_idx,:]
    y_test = gts[test_idx,:]
    X_train_normalized_new = []
    combined_feature = np.empty((len(X_train_normalized[0]),0))
    for feature in X_train_normalized:
        if len(feature.shape) == 3:
            X_train_normalized_new.append(feature)
        elif feature.shape[1] <20:
            combined_feature = np.hstack((combined_feature, feature))
        else:
            X_train_normalized_new.append(feature)
    X_train_normalized_new.append(combined_feature)
    
    X_val_normalized_new = []
    combined_feature = np.empty((len(X_val_normalized[0]),0))
    for feature in X_val_normalized:
        if len(feature.shape) == 3:
            X_val_normalized_new.append(feature)
        elif feature.shape[1] <20:
            combined_feature = np.hstack((combined_feature, feature))
        else:
            X_val_normalized_new.append(feature)
    X_val_normalized_new.append(combined_feature)
    
    X_test_normalized_new = []
    combined_feature = np.empty((len(X_test_normalized[0]),0))
    for feature in X_test_normalized:
        if len(feature.shape) == 3:
            X_test_normalized_new.append(feature)
        elif feature.shape[1] <20:
            combined_feature = np.hstack((combined_feature, feature))
        else:
            X_test_normalized_new.append(feature)
    X_test_normalized_new.append(combined_feature)
    
    
    num_epochs = 50
    
    
    # Gradually prune weights in dense and convolutional layers from 10% to 50% over the course of training, starting from epoch 0.
    
    
    rates = [0.4, 0.5, 0.6]
    
    for r in rates:
        model = individual_model(X_train_normalized)
        model.compile(loss='mean_absolute_error', optimizer='adam')
        model_name = './checkpoints/unknown_person_few_shot_baseline_p'+str(test_person_id[0])+'_'+str(idx)+'.h5'
        checkpoint = ModelCheckpoint(model_name, monitor='val_loss', mode='min', save_best_only=True, verbose=1)
    
        pruning_callback = PruningCallback(initial_pruning_factor=0.01, final_pruning_factor=r, start_epoch=5, end_epoch=20, frequency=1)
        model.fit(X_train_normalized_new, y_train, epochs=num_epochs, batch_size=1024, validation_data=(X_val_normalized_new, y_val), callbacks=[pruning_callback, checkpoint])
        import tensorflow as tf
        model = tf.keras.models.load_model(model_name)
        from tensorflow.keras.models import Model
        layers = model.layers
        second_last_layer_output = layers[-4].output
        feature_extractor_model = Model(inputs=model.input, outputs=second_last_layer_output)
        train_features = feature_extractor_model.predict(X_train_normalized_new)
        test_features = feature_extractor_model.predict(X_test_normalized_new)
        
        p_train = people_nums[train_idx,:]
        p_val = people_nums[val_idx,:]
        p_test = people_nums[test_idx,:]
        ## Calculate the distance between test person and training person
        def euclidean_distance(a, b):
            return np.sqrt(np.sum((a - b) ** 2, axis=1))
        
        distance_dict = {}
        for ii in range(len(person_nums)):
            if person_nums[ii] == test_person_id[0]:
                continue
            else:
                
                ind = np.where(p_train ==person_nums[ii])[0]
                tmp_train_features = train_features[ind, :]
                distances = np.array([euclidean_distance(train_sample, test_features) for train_sample in tmp_train_features])
                print(distances.shape)
                average_distances = np.mean(distances, axis=1)
        
                # Step 4: Find the overall average distance
                overall_average_distance = np.mean(average_distances)
                distance_dict[person_nums[ii]] = overall_average_distance
        
        
        def normalize_to_weights(distance_dict):
            distances = np.array(list(distance_dict.values()))
            # Handle the case where a distance is zero to avoid division by zero
            distances = np.clip(distances, a_min=1e-10, a_max=None)
            weights = 1 / distances
            normalized_weights = weights
            # normalized_weights = weights / sum(weights)
            # print(sum(weights))
            # print(sum(normalized_weights))
            # Assign the normalized weights back to the dictionary
            normalized_weight_dict = dict(zip(distance_dict.keys(), normalized_weights))
            return normalized_weight_dict
        def scale_dict_values(my_dict):
            scaled_dict = my_dict.copy()
            min_val = min(scaled_dict.values())
            max_val = max(scaled_dict.values())
            
            for key in scaled_dict:
                scaled_dict[key] = 1 + 9 * (scaled_dict[key] - min_val) / (max_val - min_val)
            
            return scaled_dict
        weights_dict = normalize_to_weights(distance_dict)
        weights_dict = scale_dict_values(weights_dict)
        print(weights_dict)
        
        w_train = np.zeros_like(p_train)
        for i in range(len(w_train)):
            w_train[i] = weights_dict[int(p_train[i])]
        
        w_train = np.squeeze(w_train)
        
        import sys
        sys.path.append('..')
        # for layer in model.layers[-4:]:
        #     layer.trainable = False
        # model = individual_model.individual_model(X_train_normalized)
        model_name = './checkpoints/unknown_person_few_shot_p'+str(test_person_id[0])+'_'+str(idx)+'.h5'
        checkpoint = ModelCheckpoint(model_name, monitor='val_loss', mode='min', save_best_only=True, verbose=1)
        pruning_callback = PruningCallback(initial_pruning_factor=r, final_pruning_factor=r, start_epoch=1, end_epoch=20, frequency=1)
        
        history = model.fit(x=X_train_normalized_new, y=y_train,sample_weight= w_train,epochs=20, batch_size=1024, validation_data=(X_val_normalized_new, y_val), callbacks=[pruning_callback, checkpoint])
        import sys
        sys.path.append('..')
        # for layer in model.layers[-4:]:
        #     layer.trainable = False
        # model = individual_model.individual_model(X_train_normalized)
        model_name = './checkpoints/unknown_person_few_shot_baseline_p'+str(test_person_id[0])+'_'+str(idx)+'.h5'
        model = tf.keras.models.load_model(model_name)
        model_name = './checkpoints/unknown_person_few_shot_baseline_2_p'+str(test_person_id[0])+'_'+str(idx)+'.h5'
        idx += 1
        checkpoint = ModelCheckpoint(model_name, monitor='val_loss', mode='min', save_best_only=True, verbose=1)
        pruning_callback = PruningCallback(initial_pruning_factor=r, final_pruning_factor=r, start_epoch=1, end_epoch=20, frequency=1)
        
        history = model.fit(x=X_train_normalized_new, y=y_train,epochs=20, batch_size=1024, validation_data=(X_val_normalized_new, y_val), callbacks=[pruning_callback, checkpoint])
------------------

----- stderr -----
2023-11-15 17:01:47.703810: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
----- stderr -----
/usr/lib/python3.11/site-packages/h5py/__init__.py:36: UserWarning: h5py is running against HDF5 1.14.2 when it was built against 1.14.1, this may cause problems
  _warn(("h5py is running against HDF5 {0} when it was built against {1}, "
----- stdout -----
35564
----- stdout -----
WARNING:tensorflow:From /usr/lib/python3.11/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
----- stdout -----
WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stderr -----
2023-11-15 17:03:13.824619: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-11-15 17:03:13.837212: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-11-15 17:03:13.837466: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-11-15 17:03:13.841437: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-11-15 17:03:13.841774: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-11-15 17:03:13.841950: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-11-15 17:04:13.959710: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-11-15 17:04:13.962637: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-11-15 17:04:13.963477: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-11-15 17:04:13.964638: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46594 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:2d:00.0, compute capability: 8.6
2023-11-15 17:04:13.965192: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
----- stdout -----
WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_16 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_17 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_18 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_19 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_20 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stderr -----
2023-11-15 17:04:14.932732: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-11-15 17:04:14.933039: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-11-15 17:04:14.933230: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-11-15 17:04:14.933450: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-11-15 17:04:14.933628: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-11-15 17:04:14.933765: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46594 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:2d:00.0, compute capability: 8.6
2023-11-15 17:04:14.933797: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
2023-11-15 17:04:14.962473: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled
----- stderr -----
2023-11-15 17:04:15.187938: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_21/lstm_cell_21/bias/Assign' id:3510 op device:{requested: '', assigned: ''} def:{{{node lstm_21/lstm_cell_21/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_21/lstm_cell_21/bias, lstm_21/lstm_cell_21/bias/Initializer/concat)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
2023-11-15 17:04:15.328349: W tensorflow/c/c_api.cc:304] Operation '{name:'strided_slice/stack_1' id:3634 op device:{requested: '', assigned: ''} def:{{{node strided_slice/stack_1}} = Const[_has_manual_control_dependencies=true, dtype=DT_INT32, value=Tensor<type: int32 shape: [4] values: 0 0 0...>]()}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
2023-11-15 17:04:15.363697: W tensorflow/c/c_api.cc:304] Operation '{name:'strided_slice/stack_2' id:3635 op device:{requested: '', assigned: ''} def:{{{node strided_slice/stack_2}} = Const[_has_manual_control_dependencies=true, dtype=DT_INT32, value=Tensor<type: int32 shape: [4] values: 1 1 1...>]()}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
----- stdout -----
WARNING:tensorflow:Layer lstm_22 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_23 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_24 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_25 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_26 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_27 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_28 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_29 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_30 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_31 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_32 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_33 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_34 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_35 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_36 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
(32022, 95)
----- stdout -----
Train on 32022 samples, validate on 3542 samples
----- stderr -----
2023-11-15 17:04:19.926611: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_24/lstm_cell_24/bias/Assign' id:4236 op device:{requested: '', assigned: ''} def:{{{node lstm_24/lstm_cell_24/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_24/lstm_cell_24/bias, lstm_24/lstm_cell_24/bias/Initializer/concat)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
----- stdout -----
Epoch 1/50
----- stderr -----
2023-11-15 17:04:24.252268: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape indropout/cond/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer
----- stderr -----
2023-11-15 17:04:26.796917: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8902
----- stderr -----
2023-11-15 17:04:27.036557: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
----- stdout -----
 1024/32022 [..............................] - ETA: 2:41 - loss: 4.9835
----- stdout -----
 2048/32022 [>.............................] - ETA: 1:26 - loss: 4.9413
----- stdout -----
 3072/32022 [=>............................] - ETA: 1:01 - loss: 4.8768
----- stdout -----
 4096/32022 [==>...........................] - ETA: 48s - loss: 4.8270
----- stdout -----
 5120/32022 [===>..........................] - ETA: 40s - loss: 4.7635
----- stdout -----
 6144/32022 [====>.........................] - ETA: 34s - loss: 4.7034
----- stdout -----
 7168/32022 [=====>........................] - ETA: 30s - loss: 4.6389
----- stdout -----
 8192/32022 [======>.......................] - ETA: 27s - loss: 4.5655
----- stdout -----
 9216/32022 [=======>......................] - ETA: 24s - loss: 4.4889
----- stdout -----
10240/32022 [========>.....................] - ETA: 22s - loss: 4.4139
----- stdout -----
11264/32022 [=========>....................] - ETA: 20s - loss: 4.3412
----- stdout -----
12288/32022 [==========>...................] - ETA: 19s - loss: 4.2725
----- stdout -----
13312/32022 [===========>..................] - ETA: 17s - loss: 4.2014
----- stdout -----
14336/32022 [============>.................] - ETA: 16s - loss: 4.1295
----- stdout -----
15360/32022 [=============>................] - ETA: 14s - loss: 4.0597
----- stdout -----
16384/32022 [==============>...............] - ETA: 13s - loss: 3.9852
----- stdout -----
17408/32022 [===============>..............] - ETA: 12s - loss: 3.9124
----- stdout -----
18432/32022 [================>.............] - ETA: 11s - loss: 3.8453
----- stdout -----
19456/32022 [=================>............] - ETA: 10s - loss: 3.7778
----- stdout -----
20480/32022 [==================>...........] - ETA: 9s - loss: 3.7102
----- stdout -----
21504/32022 [===================>..........] - ETA: 8s - loss: 3.6496
----- stdout -----
22528/32022 [====================>.........] - ETA: 7s - loss: 3.5891
----- stdout -----
23552/32022 [=====================>........] - ETA: 6s - loss: 3.5346
----- stdout -----
24576/32022 [======================>.......] - ETA: 5s - loss: 3.4794
----- stdout -----
25600/32022 [======================>.......] - ETA: 4s - loss: 3.4282
----- stdout -----
26624/32022 [=======================>......] - ETA: 4s - loss: 3.3824
----- stdout -----
27648/32022 [========================>.....] - ETA: 3s - loss: 3.3339
----- stdout -----
28672/32022 [=========================>....] - ETA: 2s - loss: 3.2915
----- stdout -----
29696/32022 [==========================>...] - ETA: 1s - loss: 3.2510
----- stdout -----
30720/32022 [===========================>..] - ETA: 0s - loss: 3.2094
----- stdout -----
31744/32022 [============================>.] - ETA: 0s - loss: 3.1691
----- stdout -----
32022/32022 [==============================] - ETA: 0s - loss: 3.1583
----- stderr -----
/usr/lib/python3.11/site-packages/keras/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-11-15 17:04:47.256112: W tensorflow/c/c_api.cc:304] Operation '{name:'loss_1/mul' id:6477 op device:{requested: '', assigned: ''} def:{{{node loss_1/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_1/mul/x, loss_1/dense_3_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
----- stdout -----

Epoch 1: val_loss improved from inf to 1.86303, saving model to ./checkpoints/unknown_person_few_shot_baseline_p1_30.h5
----- stdout -----
32022/32022 [==============================] - 28s 877us/sample - loss: 3.1583 - val_loss: 1.8630
----- stdout -----
Epoch 2/50
----- stdout -----
 1024/32022 [..............................] - ETA: 18s - loss: 1.9958
----- stdout -----
 2048/32022 [>.............................] - ETA: 17s - loss: 1.9502
----- stdout -----
 3072/32022 [=>............................] - ETA: 16s - loss: 1.9336
----- stdout -----
 4096/32022 [==>...........................] - ETA: 15s - loss: 1.9353
----- stdout -----
 5120/32022 [===>..........................] - ETA: 15s - loss: 1.9200
----- stdout -----
 6144/32022 [====>.........................] - ETA: 14s - loss: 1.9113
----- stdout -----
 7168/32022 [=====>........................] - ETA: 14s - loss: 1.9027
----- stdout -----
 8192/32022 [======>.......................] - ETA: 13s - loss: 1.8916
----- stdout -----
 9216/32022 [=======>......................] - ETA: 13s - loss: 1.8900
----- stdout -----
10240/32022 [========>.....................] - ETA: 12s - loss: 1.8788
----- stdout -----
11264/32022 [=========>....................] - ETA: 11s - loss: 1.8710
----- stdout -----
12288/32022 [==========>...................] - ETA: 11s - loss: 1.8605
----- stdout -----
13312/32022 [===========>..................] - ETA: 10s - loss: 1.8545
----- stdout -----
14336/32022 [============>.................] - ETA: 10s - loss: 1.8437
----- stdout -----
15360/32022 [=============>................] - ETA: 9s - loss: 1.8346
----- stdout -----
16384/32022 [==============>...............] - ETA: 8s - loss: 1.8255
----- stdout -----
17408/32022 [===============>..............] - ETA: 8s - loss: 1.8177
----- stdout -----
18432/32022 [================>.............] - ETA: 7s - loss: 1.8151
----- stdout -----
19456/32022 [=================>............] - ETA: 7s - loss: 1.8091
----- stdout -----
20480/32022 [==================>...........] - ETA: 6s - loss: 1.8031
----- stdout -----
21504/32022 [===================>..........] - ETA: 6s - loss: 1.7987
----- stdout -----
22528/32022 [====================>.........] - ETA: 5s - loss: 1.7935
----- stdout -----
23552/32022 [=====================>........] - ETA: 4s - loss: 1.7898
----- stdout -----
24576/32022 [======================>.......] - ETA: 4s - loss: 1.7832
----- stdout -----
25600/32022 [======================>.......] - ETA: 3s - loss: 1.7767
----- stdout -----
26624/32022 [=======================>......] - ETA: 3s - loss: 1.7704
----- stdout -----
27648/32022 [========================>.....] - ETA: 2s - loss: 1.7641
----- stdout -----
28672/32022 [=========================>....] - ETA: 1s - loss: 1.7591
----- stdout -----
29696/32022 [==========================>...] - ETA: 1s - loss: 1.7541
----- stdout -----
30720/32022 [===========================>..] - ETA: 0s - loss: 1.7507
----- stdout -----
31744/32022 [============================>.] - ETA: 0s - loss: 1.7477
----- stdout -----
32022/32022 [==============================] - ETA: 0s - loss: 1.7465
----- stdout -----

Epoch 2: val_loss improved from 1.86303 to 1.53186, saving model to ./checkpoints/unknown_person_few_shot_baseline_p1_30.h5
----- stdout -----
32022/32022 [==============================] - 20s 618us/sample - loss: 1.7465 - val_loss: 1.5319
----- stdout -----
Epoch 3/50
----- stdout -----
 1024/32022 [..............................] - ETA: 17s - loss: 1.5949
----- stdout -----
 2048/32022 [>.............................] - ETA: 17s - loss: 1.6011
----- stdout -----
 3072/32022 [=>............................] - ETA: 16s - loss: 1.6122
----- stdout -----
 4096/32022 [==>...........................] - ETA: 16s - loss: 1.6124
----- stdout -----
 5120/32022 [===>..........................] - ETA: 15s - loss: 1.6094
----- stdout -----
 6144/32022 [====>.........................] - ETA: 14s - loss: 1.6044
----- stdout -----
 7168/32022 [=====>........................] - ETA: 14s - loss: 1.5995
----- stdout -----
 8192/32022 [======>.......................] - ETA: 13s - loss: 1.5970
----- stdout -----
 9216/32022 [=======>......................] - ETA: 13s - loss: 1.6005
----- stdout -----
10240/32022 [========>.....................] - ETA: 12s - loss: 1.5935
----- stdout -----
11264/32022 [=========>....................] - ETA: 11s - loss: 1.5911
----- stdout -----
12288/32022 [==========>...................] - ETA: 11s - loss: 1.5963
----- stdout -----
13312/32022 [===========>..................] - ETA: 10s - loss: 1.5975
----- stdout -----
14336/32022 [============>.................] - ETA: 10s - loss: 1.5936
----- stdout -----
15360/32022 [=============>................] - ETA: 9s - loss: 1.5935
----- stdout -----
16384/32022 [==============>...............] - ETA: 8s - loss: 1.5931
----- stdout -----
17408/32022 [===============>..............] - ETA: 8s - loss: 1.5890
----- stdout -----
18432/32022 [================>.............] - ETA: 7s - loss: 1.5858
----- stdout -----
19456/32022 [=================>............] - ETA: 7s - loss: 1.5830
----- stdout -----
20480/32022 [==================>...........] - ETA: 6s - loss: 1.5832
----- stdout -----
21504/32022 [===================>..........] - ETA: 5s - loss: 1.5801
----- stdout -----
22528/32022 [====================>.........] - ETA: 5s - loss: 1.5777
----- stdout -----
23552/32022 [=====================>........] - ETA: 4s - loss: 1.5767
----- stdout -----
24576/32022 [======================>.......] - ETA: 4s - loss: 1.5782
----- stdout -----
25600/32022 [======================>.......] - ETA: 3s - loss: 1.5764
----- stdout -----
26624/32022 [=======================>......] - ETA: 3s - loss: 1.5749
----- stdout -----
27648/32022 [========================>.....] - ETA: 2s - loss: 1.5723
----- stdout -----
28672/32022 [=========================>....] - ETA: 1s - loss: 1.5717
----- stdout -----
29696/32022 [==========================>...] - ETA: 1s - loss: 1.5707
----- stdout -----
30720/32022 [===========================>..] - ETA: 0s - loss: 1.5701
----- stdout -----
31744/32022 [============================>.] - ETA: 0s - loss: 1.5690
----- stdout -----
32022/32022 [==============================] - ETA: 0s - loss: 1.5687
----- stdout -----

Epoch 3: val_loss improved from 1.53186 to 1.45893, saving model to ./checkpoints/unknown_person_few_shot_baseline_p1_30.h5
----- stdout -----
32022/32022 [==============================] - 20s 615us/sample - loss: 1.5687 - val_loss: 1.4589
----- stdout -----
Epoch 4/50
----- stdout -----
 1024/32022 [..............................] - ETA: 18s - loss: 1.5305
----- stdout -----
 2048/32022 [>.............................] - ETA: 17s - loss: 1.5329
----- stdout -----
 3072/32022 [=>............................] - ETA: 17s - loss: 1.5250
----- stdout -----
 4096/32022 [==>...........................] - ETA: 17s - loss: 1.5365
----- stdout -----
 5120/32022 [===>..........................] - ETA: 16s - loss: 1.5359
----- stdout -----
 6144/32022 [====>.........................] - ETA: 15s - loss: 1.5335
----- stdout -----
 7168/32022 [=====>........................] - ETA: 14s - loss: 1.5305
----- stdout -----
 8192/32022 [======>.......................] - ETA: 14s - loss: 1.5315
----- stdout -----
 9216/32022 [=======>......................] - ETA: 13s - loss: 1.5300
----- stdout -----
10240/32022 [========>.....................] - ETA: 13s - loss: 1.5310
----- stdout -----
11264/32022 [=========>....................] - ETA: 12s - loss: 1.5316
----- stdout -----
12288/32022 [==========>...................] - ETA: 11s - loss: 1.5331
----- stdout -----
13312/32022 [===========>..................] - ETA: 11s - loss: 1.5324
----- stdout -----
14336/32022 [============>.................] - ETA: 10s - loss: 1.5309
----- stdout -----
15360/32022 [=============>................] - ETA: 9s - loss: 1.5313
----- stdout -----
16384/32022 [==============>...............] - ETA: 9s - loss: 1.5301
----- stdout -----
17408/32022 [===============>..............] - ETA: 8s - loss: 1.5300
----- stdout -----
18432/32022 [================>.............] - ETA: 8s - loss: 1.5291
----- stdout -----
19456/32022 [=================>............] - ETA: 7s - loss: 1.5278
----- stdout -----
20480/32022 [==================>...........] - ETA: 6s - loss: 1.5269
----- stdout -----
21504/32022 [===================>..........] - ETA: 6s - loss: 1.5281
----- stdout -----
22528/32022 [====================>.........] - ETA: 5s - loss: 1.5302
----- stdout -----
23552/32022 [=====================>........] - ETA: 4s - loss: 1.5302
----- stdout -----
24576/32022 [======================>.......] - ETA: 4s - loss: 1.5297
----- stdout -----
25600/32022 [======================>.......] - ETA: 3s - loss: 1.5282
----- stdout -----
26624/32022 [=======================>......] - ETA: 3s - loss: 1.5267
----- stdout -----
27648/32022 [========================>.....] - ETA: 2s - loss: 1.5250
----- stdout -----
28672/32022 [=========================>....] - ETA: 1s - loss: 1.5259
----- stdout -----
29696/32022 [==========================>...] - ETA: 1s - loss: 1.5250
----- stdout -----
30720/32022 [===========================>..] - ETA: 0s - loss: 1.5243
----- stdout -----
31744/32022 [============================>.] - ETA: 0s - loss: 1.5225
----- stdout -----
32022/32022 [==============================] - ETA: 0s - loss: 1.5228
----- stdout -----

Epoch 4: val_loss improved from 1.45893 to 1.43288, saving model to ./checkpoints/unknown_person_few_shot_baseline_p1_30.h5
----- stdout -----
32022/32022 [==============================] - 20s 629us/sample - loss: 1.5228 - val_loss: 1.4329
----- stdout -----
Epoch 5/50
----- stdout -----
 1024/32022 [..............................] - ETA: 18s - loss: 1.5264
----- stdout -----
 2048/32022 [>.............................] - ETA: 17s - loss: 1.5269
----- stdout -----
 3072/32022 [=>............................] - ETA: 17s - loss: 1.5254
----- stdout -----
 4096/32022 [==>...........................] - ETA: 17s - loss: 1.5220
----- stdout -----
 5120/32022 [===>..........................] - ETA: 16s - loss: 1.5200
----- stdout -----
 6144/32022 [====>.........................] - ETA: 15s - loss: 1.5214
----- stdout -----
 7168/32022 [=====>........................] - ETA: 15s - loss: 1.5195
----- stdout -----
 8192/32022 [======>.......................] - ETA: 14s - loss: 1.5114
----- stdout -----
 9216/32022 [=======>......................] - ETA: 13s - loss: 1.5101
----- stdout -----
10240/32022 [========>.....................] - ETA: 13s - loss: 1.5130
----- stdout -----
11264/32022 [=========>....................] - ETA: 12s - loss: 1.5146
----- stdout -----
12288/32022 [==========>...................] - ETA: 11s - loss: 1.5124
----- stdout -----
13312/32022 [===========>..................] - ETA: 11s - loss: 1.5091
----- stdout -----
14336/32022 [============>.................] - ETA: 10s - loss: 1.5079
----- stdout -----
15360/32022 [=============>................] - ETA: 9s - loss: 1.5076
----- stdout -----
16384/32022 [==============>...............] - ETA: 9s - loss: 1.5058
----- stdout -----
17408/32022 [===============>..............] - ETA: 8s - loss: 1.5072
----- stdout -----
18432/32022 [================>.............] - ETA: 8s - loss: 1.5066
----- stdout -----
19456/32022 [=================>............] - ETA: 7s - loss: 1.5043
----- stdout -----
20480/32022 [==================>...........] - ETA: 6s - loss: 1.5049
----- stdout -----
21504/32022 [===================>..........] - ETA: 6s - loss: 1.5060
----- stdout -----
22528/32022 [====================>.........] - ETA: 5s - loss: 1.5066
----- stdout -----
23552/32022 [=====================>........] - ETA: 5s - loss: 1.5060
----- stdout -----
24576/32022 [======================>.......] - ETA: 4s - loss: 1.5073
----- stdout -----
25600/32022 [======================>.......] - ETA: 3s - loss: 1.5064
----- stdout -----
26624/32022 [=======================>......] - ETA: 3s - loss: 1.5055
----- stdout -----
27648/32022 [========================>.....] - ETA: 2s - loss: 1.5052
----- stdout -----
28672/32022 [=========================>....] - ETA: 2s - loss: 1.5051
----- stdout -----
29696/32022 [==========================>...] - ETA: 1s - loss: 1.5044
----- stdout -----
30720/32022 [===========================>..] - ETA: 0s - loss: 1.5038
----- stdout -----
31744/32022 [============================>.] - ETA: 0s - loss: 1.5026
----- stdout -----
32022/32022 [==============================] - ETA: 0s - loss: 1.5030
----- stdout -----

Epoch 5: val_loss improved from 1.43288 to 1.41766, saving model to ./checkpoints/unknown_person_few_shot_baseline_p1_30.h5
----- stdout -----
32022/32022 [==============================] - 21s 665us/sample - loss: 1.5030 - val_loss: 1.4177
----- stdout -----
Epoch 6/50
----- stdout -----
 1024/32022 [..............................] - ETA: 20s - loss: 1.4695
----- stdout -----
 2048/32022 [>.............................] - ETA: 19s - loss: 1.4716
----- stdout -----
 3072/32022 [=>............................] - ETA: 19s - loss: 1.4724
----- stdout -----
 4096/32022 [==>...........................] - ETA: 18s - loss: 1.4740
----- stdout -----
 5120/32022 [===>..........................] - ETA: 17s - loss: 1.4812
----- stdout -----
 6144/32022 [====>.........................] - ETA: 17s - loss: 1.4801
----- stdout -----
 7168/32022 [=====>........................] - ETA: 16s - loss: 1.4874
----- stdout -----
 8192/32022 [======>.......................] - ETA: 15s - loss: 1.4850
----- stdout -----
 9216/32022 [=======>......................] - ETA: 15s - loss: 1.4861
----- stdout -----
10240/32022 [========>.....................] - ETA: 14s - loss: 1.4879
----- stdout -----
11264/32022 [=========>....................] - ETA: 13s - loss: 1.4925
----- stdout -----
12288/32022 [==========>...................] - ETA: 13s - loss: 1.4934
----- stdout -----
13312/32022 [===========>..................] - ETA: 12s - loss: 1.4924
----- stdout -----
14336/32022 [============>.................] - ETA: 11s - loss: 1.4932
----- stdout -----
15360/32022 [=============>................] - ETA: 11s - loss: 1.4916
----- stdout -----
16384/32022 [==============>...............] - ETA: 10s - loss: 1.4900
----- stdout -----
17408/32022 [===============>..............] - ETA: 9s - loss: 1.4896
----- stdout -----
18432/32022 [================>.............] - ETA: 9s - loss: 1.4922
----- stdout -----
19456/32022 [=================>............] - ETA: 8s - loss: 1.4938
----- stdout -----
20480/32022 [==================>...........] - ETA: 7s - loss: 1.4936
----- stdout -----
21504/32022 [===================>..........] - ETA: 6s - loss: 1.4930
----- stdout -----
22528/32022 [====================>.........] - ETA: 6s - loss: 1.4918
----- stdout -----
23552/32022 [=====================>........] - ETA: 5s - loss: 1.4922
----- stdout -----
24576/32022 [======================>.......] - ETA: 4s - loss: 1.4907
----- stdout -----
25600/32022 [======================>.......] - ETA: 4s - loss: 1.4902
----- stdout -----
26624/32022 [=======================>......] - ETA: 3s - loss: 1.4908
----- stdout -----
27648/32022 [========================>.....] - ETA: 2s - loss: 1.4897
----- stdout -----
28672/32022 [=========================>....] - ETA: 2s - loss: 1.4881
----- stdout -----
29696/32022 [==========================>...] - ETA: 1s - loss: 1.4879
----- stdout -----
30720/32022 [===========================>..] - ETA: 0s - loss: 1.4887
----- stdout -----
31744/32022 [============================>.] - ETA: 0s - loss: 1.4888
----- stdout -----
32022/32022 [==============================] - ETA: 0s - loss: 1.4881
----- stdout -----

Epoch 6: val_loss improved from 1.41766 to 1.40690, saving model to ./checkpoints/unknown_person_few_shot_baseline_p1_30.h5
----- stdout -----
32022/32022 [==============================] - 22s 688us/sample - loss: 1.4881 - val_loss: 1.4069
----- stdout -----
Epoch 7/50
----- stdout -----
 1024/32022 [..............................] - ETA: 21s - loss: 1.4689
----- stdout -----
 2048/32022 [>.............................] - ETA: 20s - loss: 1.4692
----- stdout -----
 3072/32022 [=>............................] - ETA: 19s - loss: 1.4798
----- stdout -----
 4096/32022 [==>...........................] - ETA: 18s - loss: 1.4721
----- stdout -----
 5120/32022 [===>..........................] - ETA: 18s - loss: 1.4724
----- stdout -----
 6144/32022 [====>.........................] - ETA: 17s - loss: 1.4669
----- stdout -----
 7168/32022 [=====>........................] - ETA: 16s - loss: 1.4668
----- stdout -----
 8192/32022 [======>.......................] - ETA: 15s - loss: 1.4670
----- stdout -----
 9216/32022 [=======>......................] - ETA: 15s - loss: 1.4659
----- stdout -----
10240/32022 [========>.....................] - ETA: 14s - loss: 1.4703
----- stdout -----
11264/32022 [=========>....................] - ETA: 13s - loss: 1.4717
----- stdout -----
12288/32022 [==========>...................] - ETA: 13s - loss: 1.4728
----- stdout -----
13312/32022 [===========>..................] - ETA: 12s - loss: 1.4737
----- stdout -----
14336/32022 [============>.................] - ETA: 11s - loss: 1.4737
----- stdout -----
15360/32022 [=============>................] - ETA: 11s - loss: 1.4744
----- stdout -----
16384/32022 [==============>...............] - ETA: 10s - loss: 1.4742
----- stdout -----
17408/32022 [===============>..............] - ETA: 9s - loss: 1.4755
----- stdout -----
18432/32022 [================>.............] - ETA: 9s - loss: 1.4743
----- stdout -----
19456/32022 [=================>............] - ETA: 8s - loss: 1.4748
----- stdout -----
20480/32022 [==================>...........] - ETA: 7s - loss: 1.4728
----- stdout -----
21504/32022 [===================>..........] - ETA: 7s - loss: 1.4733
----- stdout -----
22528/32022 [====================>.........] - ETA: 6s - loss: 1.4737
----- stdout -----
23552/32022 [=====================>........] - ETA: 5s - loss: 1.4743
----- stdout -----
24576/32022 [======================>.......] - ETA: 4s - loss: 1.4732
----- stdout -----
25600/32022 [======================>.......] - ETA: 4s - loss: 1.4747
----- stdout -----
26624/32022 [=======================>......] - ETA: 3s - loss: 1.4745
----- stdout -----
27648/32022 [========================>.....] - ETA: 2s - loss: 1.4758
----- stdout -----
28672/32022 [=========================>....] - ETA: 2s - loss: 1.4751
----- stdout -----
29696/32022 [==========================>...] - ETA: 1s - loss: 1.4753
----- stdout -----
30720/32022 [===========================>..] - ETA: 0s - loss: 1.4757
----- stdout -----
31744/32022 [============================>.] - ETA: 0s - loss: 1.4750
----- stdout -----
32022/32022 [==============================] - ETA: 0s - loss: 1.4749
----- stdout -----

Epoch 7: val_loss improved from 1.40690 to 1.40194, saving model to ./checkpoints/unknown_person_few_shot_baseline_p1_30.h5
----- stdout -----
32022/32022 [==============================] - 23s 718us/sample - loss: 1.4749 - val_loss: 1.4019
----- stdout -----
Epoch 8/50
----- stdout -----
 1024/32022 [..............................] - ETA: 18s - loss: 1.4586
----- stdout -----
 2048/32022 [>.............................] - ETA: 18s - loss: 1.4773
----- stdout -----
 3072/32022 [=>............................] - ETA: 18s - loss: 1.4778
----- stdout -----
 4096/32022 [==>...........................] - ETA: 18s - loss: 1.4731
----- stdout -----
 5120/32022 [===>..........................] - ETA: 17s - loss: 1.4719
----- stdout -----
 6144/32022 [====>.........................] - ETA: 16s - loss: 1.4753
----- stdout -----
 7168/32022 [=====>........................] - ETA: 16s - loss: 1.4818
----- stdout -----
 8192/32022 [======>.......................] - ETA: 15s - loss: 1.4796
----- stdout -----
 9216/32022 [=======>......................] - ETA: 14s - loss: 1.4758
----- stdout -----
10240/32022 [========>.....................] - ETA: 14s - loss: 1.4710
----- stdout -----
11264/32022 [=========>....................] - ETA: 13s - loss: 1.4733
----- stdout -----
12288/32022 [==========>...................] - ETA: 12s - loss: 1.4750
----- stdout -----
13312/32022 [===========>..................] - ETA: 11s - loss: 1.4743
----- stdout -----
14336/32022 [============>.................] - ETA: 11s - loss: 1.4747
----- stdout -----
15360/32022 [=============>................] - ETA: 10s - loss: 1.4773
----- stdout -----
16384/32022 [==============>...............] - ETA: 9s - loss: 1.4759
----- stdout -----
17408/32022 [===============>..............] - ETA: 9s - loss: 1.4755
----- stdout -----
18432/32022 [================>.............] - ETA: 8s - loss: 1.4768
----- stdout -----
19456/32022 [=================>............] - ETA: 7s - loss: 1.4778
----- stdout -----
20480/32022 [==================>...........] - ETA: 7s - loss: 1.4781
----- stdout -----
21504/32022 [===================>..........] - ETA: 6s - loss: 1.4790
----- stdout -----
22528/32022 [====================>.........] - ETA: 5s - loss: 1.4803
----- stdout -----
23552/32022 [=====================>........] - ETA: 5s - loss: 1.4802
----- stdout -----
24576/32022 [======================>.......] - ETA: 4s - loss: 1.4796
----- stdout -----
25600/32022 [======================>.......] - ETA: 3s - loss: 1.4784
----- stdout -----
26624/32022 [=======================>......] - ETA: 3s - loss: 1.4785
----- stdout -----
27648/32022 [========================>.....] - ETA: 2s - loss: 1.4780
----- stdout -----
28672/32022 [=========================>....] - ETA: 2s - loss: 1.4780
----- stdout -----
29696/32022 [==========================>...] - ETA: 1s - loss: 1.4767
----- stdout -----
30720/32022 [===========================>..] - ETA: 0s - loss: 1.4761
----- stdout -----
31744/32022 [============================>.] - ETA: 0s - loss: 1.4752
----- stdout -----
32022/32022 [==============================] - ETA: 0s - loss: 1.4755
----- stdout -----

Epoch 8: val_loss improved from 1.40194 to 1.40185, saving model to ./checkpoints/unknown_person_few_shot_baseline_p1_30.h5
----- stdout -----
32022/32022 [==============================] - 21s 649us/sample - loss: 1.4755 - val_loss: 1.4018
----- stdout -----
Epoch 9/50
----- stdout -----
 1024/32022 [..............................] - ETA: 18s - loss: 1.4962
----- stdout -----
 2048/32022 [>.............................] - ETA: 17s - loss: 1.5100
----- stdout -----
 3072/32022 [=>............................] - ETA: 16s - loss: 1.5112
----- stdout -----
 4096/32022 [==>...........................] - ETA: 16s - loss: 1.4999
----- stdout -----
 5120/32022 [===>..........................] - ETA: 16s - loss: 1.4951
----- stdout -----
 6144/32022 [====>.........................] - ETA: 15s - loss: 1.4862
----- stdout -----
 7168/32022 [=====>........................] - ETA: 15s - loss: 1.4827
----- stdout -----
 8192/32022 [======>.......................] - ETA: 14s - loss: 1.4839
----- stdout -----
 9216/32022 [=======>......................] - ETA: 13s - loss: 1.4851
----- stdout -----
10240/32022 [========>.....................] - ETA: 12s - loss: 1.4825
----- stdout -----
11264/32022 [=========>....................] - ETA: 12s - loss: 1.4855
----- stdout -----
12288/32022 [==========>...................] - ETA: 11s - loss: 1.4829
----- stdout -----
13312/32022 [===========>..................] - ETA: 11s - loss: 1.4825
----- stdout -----
14336/32022 [============>.................] - ETA: 10s - loss: 1.4791
----- stdout -----
15360/32022 [=============>................] - ETA: 9s - loss: 1.4765
----- stdout -----
16384/32022 [==============>...............] - ETA: 9s - loss: 1.4717
----- stdout -----
17408/32022 [===============>..............] - ETA: 8s - loss: 1.4710
----- stdout -----
18432/32022 [================>.............] - ETA: 8s - loss: 1.4703
----- stdout -----
19456/32022 [=================>............] - ETA: 7s - loss: 1.4683
----- stdout -----
20480/32022 [==================>...........] - ETA: 6s - loss: 1.4683
----- stdout -----
21504/32022 [===================>..........] - ETA: 6s - loss: 1.4685
----- stdout -----
22528/32022 [====================>.........] - ETA: 5s - loss: 1.4684
----- stdout -----
23552/32022 [=====================>........] - ETA: 5s - loss: 1.4682
----- stdout -----
24576/32022 [======================>.......] - ETA: 4s - loss: 1.4696
----- stdout -----
25600/32022 [======================>.......] - ETA: 3s - loss: 1.4701
----- stdout -----
26624/32022 [=======================>......] - ETA: 3s - loss: 1.4698
----- stdout -----
27648/32022 [========================>.....] - ETA: 2s - loss: 1.4685
----- stdout -----
28672/32022 [=========================>....] - ETA: 1s - loss: 1.4687
----- stdout -----
29696/32022 [==========================>...] - ETA: 1s - loss: 1.4680
----- stdout -----
30720/32022 [===========================>..] - ETA: 0s - loss: 1.4684
----- stdout -----
31744/32022 [============================>.] - ETA: 0s - loss: 1.4694
----- stdout -----
32022/32022 [==============================] - ETA: 0s - loss: 1.4699
----- stdout -----

Epoch 9: val_loss improved from 1.40185 to 1.39745, saving model to ./checkpoints/unknown_person_few_shot_baseline_p1_30.h5
----- stdout -----
32022/32022 [==============================] - 20s 636us/sample - loss: 1.4699 - val_loss: 1.3974
----- stdout -----
Epoch 10/50
----- stdout -----
 1024/32022 [..............................] - ETA: 18s - loss: 1.4759
----- stdout -----
 2048/32022 [>.............................] - ETA: 17s - loss: 1.4752
----- stdout -----
 3072/32022 [=>............................] - ETA: 17s - loss: 1.4733
----- stdout -----
 4096/32022 [==>...........................] - ETA: 16s - loss: 1.4806
----- stdout -----
 5120/32022 [===>..........................] - ETA: 16s - loss: 1.4809
----- stdout -----
 6144/32022 [====>.........................] - ETA: 15s - loss: 1.4823
----- stdout -----
 7168/32022 [=====>........................] - ETA: 14s - loss: 1.4811
----- stdout -----
 8192/32022 [======>.......................] - ETA: 14s - loss: 1.4789
----- stdout -----
 9216/32022 [=======>......................] - ETA: 13s - loss: 1.4745
----- stdout -----
10240/32022 [========>.....................] - ETA: 12s - loss: 1.4700
----- stdout -----
11264/32022 [=========>....................] - ETA: 12s - loss: 1.4663
----- stdout -----
12288/32022 [==========>...................] - ETA: 11s - loss: 1.4673
----- stdout -----
13312/32022 [===========>..................] - ETA: 10s - loss: 1.4693
----- stdout -----
14336/32022 [============>.................] - ETA: 10s - loss: 1.4690
----- stdout -----
15360/32022 [=============>................] - ETA: 9s - loss: 1.4707
----- stdout -----
16384/32022 [==============>...............] - ETA: 9s - loss: 1.4719
----- stdout -----
17408/32022 [===============>..............] - ETA: 8s - loss: 1.4734
----- stdout -----
18432/32022 [================>.............] - ETA: 7s - loss: 1.4725
----- stdout -----
19456/32022 [=================>............] - ETA: 7s - loss: 1.4705
----- stdout -----
20480/32022 [==================>...........] - ETA: 6s - loss: 1.4696
----- stdout -----
21504/32022 [===================>..........] - ETA: 6s - loss: 1.4662
----- stdout -----
22528/32022 [====================>.........] - ETA: 5s - loss: 1.4658
----- stdout -----
23552/32022 [=====================>........] - ETA: 4s - loss: 1.4667
----- stdout -----
24576/32022 [======================>.......] - ETA: 4s - loss: 1.4666
----- stdout -----
25600/32022 [======================>.......] - ETA: 3s - loss: 1.4671
----- stdout -----
26624/32022 [=======================>......] - ETA: 3s - loss: 1.4681
----- stdout -----
27648/32022 [========================>.....] - ETA: 2s - loss: 1.4662
----- stdout -----
28672/32022 [=========================>....] - ETA: 2s - loss: 1.4662
----- stdout -----
29696/32022 [==========================>...] - ETA: 1s - loss: 1.4667
----- stdout -----
30720/32022 [===========================>..] - ETA: 0s - loss: 1.4668
----- stdout -----
31744/32022 [============================>.] - ETA: 0s - loss: 1.4667
----- stdout -----
32022/32022 [==============================] - ETA: 0s - loss: 1.4661
----- stdout -----

Epoch 10: val_loss improved from 1.39745 to 1.39168, saving model to ./checkpoints/unknown_person_few_shot_baseline_p1_30.h5
----- stdout -----
32022/32022 [==============================] - 21s 662us/sample - loss: 1.4661 - val_loss: 1.3917
----- stdout -----
Epoch 11/50
----- stdout -----
 1024/32022 [..............................] - ETA: 20s - loss: 1.4365
----- stdout -----
 2048/32022 [>.............................] - ETA: 20s - loss: 1.4562
----- stdout -----
 3072/32022 [=>............................] - ETA: 19s - loss: 1.4506
----- stdout -----
 4096/32022 [==>...........................] - ETA: 18s - loss: 1.4691
----- stdout -----
 5120/32022 [===>..........................] - ETA: 18s - loss: 1.4648
----- stdout -----
 6144/32022 [====>.........................] - ETA: 17s - loss: 1.4641
----- stdout -----
 7168/32022 [=====>........................] - ETA: 16s - loss: 1.4722
----- stdout -----
 8192/32022 [======>.......................] - ETA: 16s - loss: 1.4699
----- stdout -----
 9216/32022 [=======>......................] - ETA: 15s - loss: 1.4641
----- stdout -----
10240/32022 [========>.....................] - ETA: 14s - loss: 1.4637
----- stdout -----
11264/32022 [=========>....................] - ETA: 14s - loss: 1.4684
----- stdout -----
12288/32022 [==========>...................] - ETA: 13s - loss: 1.4659
----- stdout -----
13312/32022 [===========>..................] - ETA: 12s - loss: 1.4659
----- stdout -----
14336/32022 [============>.................] - ETA: 12s - loss: 1.4653
----- stdout -----
15360/32022 [=============>................] - ETA: 11s - loss: 1.4668
----- stdout -----
16384/32022 [==============>...............] - ETA: 10s - loss: 1.4664
----- stdout -----
17408/32022 [===============>..............] - ETA: 9s - loss: 1.4647
----- stdout -----
18432/32022 [================>.............] - ETA: 9s - loss: 1.4636
----- stdout -----
19456/32022 [=================>............] - ETA: 8s - loss: 1.4621
----- stdout -----
20480/32022 [==================>...........] - ETA: 7s - loss: 1.4640
----- stdout -----
21504/32022 [===================>..........] - ETA: 7s - loss: 1.4649
----- stdout -----
22528/32022 [====================>.........] - ETA: 6s - loss: 1.4644
----- stdout -----
23552/32022 [=====================>........] - ETA: 5s - loss: 1.4641
----- stdout -----
24576/32022 [======================>.......] - ETA: 5s - loss: 1.4645
----- stdout -----
25600/32022 [======================>.......] - ETA: 4s - loss: 1.4644
----- stdout -----
26624/32022 [=======================>......] - ETA: 3s - loss: 1.4629
----- stdout -----
27648/32022 [========================>.....] - ETA: 2s - loss: 1.4613
----- stdout -----
28672/32022 [=========================>....] - ETA: 2s - loss: 1.4608
----- stdout -----
29696/32022 [==========================>...] - ETA: 1s - loss: 1.4610
----- stdout -----
30720/32022 [===========================>..] - ETA: 0s - loss: 1.4612
----- stdout -----
31744/32022 [============================>.] - ETA: 0s - loss: 1.4608
----- stdout -----
32022/32022 [==============================] - ETA: 0s - loss: 1.4611
----- stdout -----

Epoch 11: val_loss improved from 1.39168 to 1.38880, saving model to ./checkpoints/unknown_person_few_shot_baseline_p1_30.h5
----- stdout -----
32022/32022 [==============================] - 23s 723us/sample - loss: 1.4611 - val_loss: 1.3888
----- stdout -----
Epoch 12/50
----- stdout -----
 1024/32022 [..............................] - ETA: 20s - loss: 1.4618
----- stdout -----
 2048/32022 [>.............................] - ETA: 18s - loss: 1.4601
----- stdout -----
 3072/32022 [=>............................] - ETA: 17s - loss: 1.4688
----- stdout -----
 4096/32022 [==>...........................] - ETA: 16s - loss: 1.4698
----- stdout -----
 5120/32022 [===>..........................] - ETA: 16s - loss: 1.4730
----- stdout -----
 6144/32022 [====>.........................] - ETA: 15s - loss: 1.4772
----- stdout -----
 7168/32022 [=====>........................] - ETA: 14s - loss: 1.4748
----- stdout -----
 8192/32022 [======>.......................] - ETA: 14s - loss: 1.4755
----- stdout -----
 9216/32022 [=======>......................] - ETA: 13s - loss: 1.4727
----- stdout -----
10240/32022 [========>.....................] - ETA: 12s - loss: 1.4737
----- stdout -----
11264/32022 [=========>....................] - ETA: 12s - loss: 1.4783
----- stdout -----
12288/32022 [==========>...................] - ETA: 11s - loss: 1.4731
----- stdout -----
13312/32022 [===========>..................] - ETA: 10s - loss: 1.4707
----- stdout -----
14336/32022 [============>.................] - ETA: 10s - loss: 1.4678
----- stdout -----
15360/32022 [=============>................] - ETA: 9s - loss: 1.4660
----- stdout -----
16384/32022 [==============>...............] - ETA: 9s - loss: 1.4656
----- stdout -----
17408/32022 [===============>..............] - ETA: 8s - loss: 1.4629
----- stdout -----
18432/32022 [================>.............] - ETA: 8s - loss: 1.4624
----- stdout -----
19456/32022 [=================>............] - ETA: 7s - loss: 1.4620
----- stdout -----
20480/32022 [==================>...........] - ETA: 7s - loss: 1.4604
----- stdout -----
21504/32022 [===================>..........] - ETA: 6s - loss: 1.4607
----- stdout -----
22528/32022 [====================>.........] - ETA: 5s - loss: 1.4615
----- stdout -----
23552/32022 [=====================>........] - ETA: 5s - loss: 1.4621
----- stdout -----
24576/32022 [======================>.......] - ETA: 4s - loss: 1.4598
----- stdout -----
25600/32022 [======================>.......] - ETA: 3s - loss: 1.4579
----- stdout -----
26624/32022 [=======================>......] - ETA: 3s - loss: 1.4575
----- stdout -----
27648/32022 [========================>.....] - ETA: 2s - loss: 1.4582
----- stdout -----
28672/32022 [=========================>....] - ETA: 2s - loss: 1.4588
----- stdout -----
29696/32022 [==========================>...] - ETA: 1s - loss: 1.4574
----- stdout -----
30720/32022 [===========================>..] - ETA: 0s - loss: 1.4570
----- stdout -----
31744/32022 [============================>.] - ETA: 0s - loss: 1.4573
----- stdout -----
32022/32022 [==============================] - ETA: 0s - loss: 1.4576
----- stdout -----

Epoch 12: val_loss improved from 1.38880 to 1.38685, saving model to ./checkpoints/unknown_person_few_shot_baseline_p1_30.h5
----- stdout -----
32022/32022 [==============================] - 22s 678us/sample - loss: 1.4576 - val_loss: 1.3868
----- stdout -----
Epoch 13/50
----- stdout -----
 1024/32022 [..............................] - ETA: 20s - loss: 1.4400
----- stdout -----
 2048/32022 [>.............................] - ETA: 19s - loss: 1.4517
----- stdout -----
 3072/32022 [=>............................] - ETA: 18s - loss: 1.4361
----- stdout -----
 4096/32022 [==>...........................] - ETA: 18s - loss: 1.4417
----- stdout -----
 5120/32022 [===>..........................] - ETA: 17s - loss: 1.4520
----- stdout -----
 6144/32022 [====>.........................] - ETA: 16s - loss: 1.4589
----- stdout -----
 7168/32022 [=====>........................] - ETA: 16s - loss: 1.4601
----- stdout -----
 8192/32022 [======>.......................] - ETA: 15s - loss: 1.4577
----- stdout -----
 9216/32022 [=======>......................] - ETA: 14s - loss: 1.4570
----- stdout -----
10240/32022 [========>.....................] - ETA: 14s - loss: 1.4534
----- stdout -----
11264/32022 [=========>....................] - ETA: 13s - loss: 1.4491
----- stdout -----
12288/32022 [==========>...................] - ETA: 13s - loss: 1.4466
----- stdout -----
13312/32022 [===========>..................] - ETA: 12s - loss: 1.4498
----- stdout -----
14336/32022 [============>.................] - ETA: 11s - loss: 1.4514
----- stdout -----
15360/32022 [=============>................] - ETA: 11s - loss: 1.4522
----- stdout -----
16384/32022 [==============>...............] - ETA: 10s - loss: 1.4514
----- stdout -----
17408/32022 [===============>..............] - ETA: 9s - loss: 1.4520
----- stdout -----
18432/32022 [================>.............] - ETA: 8s - loss: 1.4513
----- stdout -----
19456/32022 [=================>............] - ETA: 8s - loss: 1.4498
----- stdout -----
20480/32022 [==================>...........] - ETA: 7s - loss: 1.4497
----- stdout -----
21504/32022 [===================>..........] - ETA: 6s - loss: 1.4493
----- stdout -----
22528/32022 [====================>.........] - ETA: 6s - loss: 1.4507
----- stdout -----
23552/32022 [=====================>........] - ETA: 5s - loss: 1.4504
----- stdout -----
24576/32022 [======================>.......] - ETA: 4s - loss: 1.4512
----- stdout -----
25600/32022 [======================>.......] - ETA: 4s - loss: 1.4509
----- stdout -----
26624/32022 [=======================>......] - ETA: 3s - loss: 1.4498
----- stdout -----
27648/32022 [========================>.....] - ETA: 2s - loss: 1.4510
----- stdout -----
28672/32022 [=========================>....] - ETA: 2s - loss: 1.4509
----- stdout -----
29696/32022 [==========================>...] - ETA: 1s - loss: 1.4515
----- stdout -----
30720/32022 [===========================>..] - ETA: 0s - loss: 1.4521
----- stdout -----
31744/32022 [============================>.] - ETA: 0s - loss: 1.4519
----- stdout -----
32022/32022 [==============================] - ETA: 0s - loss: 1.4523
----- stdout -----

Epoch 13: val_loss improved from 1.38685 to 1.38201, saving model to ./checkpoints/unknown_person_few_shot_baseline_p1_30.h5
----- stdout -----
32022/32022 [==============================] - 22s 680us/sample - loss: 1.4523 - val_loss: 1.3820
----- stdout -----
Epoch 14/50
----- stdout -----
 1024/32022 [..............................] - ETA: 17s - loss: 1.4830
----- stdout -----
 2048/32022 [>.............................] - ETA: 16s - loss: 1.4700
----- stdout -----
 3072/32022 [=>............................] - ETA: 16s - loss: 1.4606
----- stdout -----
 4096/32022 [==>...........................] - ETA: 16s - loss: 1.4590
----- stdout -----
 5120/32022 [===>..........................] - ETA: 15s - loss: 1.4515
----- stdout -----
 6144/32022 [====>.........................] - ETA: 14s - loss: 1.4480
----- stdout -----
 7168/32022 [=====>........................] - ETA: 14s - loss: 1.4506
----- stdout -----
 8192/32022 [======>.......................] - ETA: 13s - loss: 1.4514
----- stdout -----
 9216/32022 [=======>......................] - ETA: 13s - loss: 1.4558
----- stdout -----
10240/32022 [========>.....................] - ETA: 12s - loss: 1.4547
----- stdout -----
11264/32022 [=========>....................] - ETA: 11s - loss: 1.4545
----- stdout -----
12288/32022 [==========>...................] - ETA: 11s - loss: 1.4542
----- stdout -----
13312/32022 [===========>..................] - ETA: 10s - loss: 1.4552
----- stdout -----
14336/32022 [============>.................] - ETA: 10s - loss: 1.4565
----- stdout -----
15360/32022 [=============>................] - ETA: 9s - loss: 1.4565
----- stdout -----
16384/32022 [==============>...............] - ETA: 8s - loss: 1.4567
----- stdout -----
17408/32022 [===============>..............] - ETA: 8s - loss: 1.4565
----- stdout -----
18432/32022 [================>.............] - ETA: 7s - loss: 1.4553
----- stdout -----
19456/32022 [=================>............] - ETA: 7s - loss: 1.4542
----- stdout -----
20480/32022 [==================>...........] - ETA: 6s - loss: 1.4552
----- stdout -----
21504/32022 [===================>..........] - ETA: 6s - loss: 1.4539
----- stdout -----
22528/32022 [====================>.........] - ETA: 5s - loss: 1.4554
----- stdout -----
23552/32022 [=====================>........] - ETA: 4s - loss: 1.4554
----- stdout -----
24576/32022 [======================>.......] - ETA: 4s - loss: 1.4552
----- stdout -----
25600/32022 [======================>.......] - ETA: 3s - loss: 1.4548
----- stdout -----
26624/32022 [=======================>......] - ETA: 3s - loss: 1.4542
----- stdout -----
27648/32022 [========================>.....] - ETA: 2s - loss: 1.4528
----- stdout -----
28672/32022 [=========================>....] - ETA: 1s - loss: 1.4523
----- stdout -----
29696/32022 [==========================>...] - ETA: 1s - loss: 1.4519
----- stdout -----
30720/32022 [===========================>..] - ETA: 0s - loss: 1.4521
----- stdout -----
31744/32022 [============================>.] - ETA: 0s - loss: 1.4522
----- stdout -----
32022/32022 [==============================] - ETA: 0s - loss: 1.4520
----- stdout -----

Epoch 14: val_loss improved from 1.38201 to 1.38037, saving model to ./checkpoints/unknown_person_few_shot_baseline_p1_30.h5
----- stdout -----
32022/32022 [==============================] - 20s 621us/sample - loss: 1.4520 - val_loss: 1.3804
----- stdout -----
Epoch 15/50
----- stdout -----
 1024/32022 [..............................] - ETA: 17s - loss: 1.5096
----- stdout -----
 2048/32022 [>.............................] - ETA: 17s - loss: 1.4884
----- stdout -----
 3072/32022 [=>............................] - ETA: 17s - loss: 1.4754
----- stdout -----
 4096/32022 [==>...........................] - ETA: 16s - loss: 1.4553
----- stdout -----
 5120/32022 [===>..........................] - ETA: 15s - loss: 1.4555
----- stdout -----
 6144/32022 [====>.........................] - ETA: 15s - loss: 1.4531
----- stdout -----
 7168/32022 [=====>........................] - ETA: 14s - loss: 1.4551
----- stdout -----
 8192/32022 [======>.......................] - ETA: 13s - loss: 1.4519
----- stdout -----
 9216/32022 [=======>......................] - ETA: 13s - loss: 1.4521
----- stdout -----
10240/32022 [========>.....................] - ETA: 12s - loss: 1.4511
----- stdout -----
11264/32022 [=========>....................] - ETA: 12s - loss: 1.4558
----- stdout -----
12288/32022 [==========>...................] - ETA: 11s - loss: 1.4546
----- stdout -----
13312/32022 [===========>..................] - ETA: 10s - loss: 1.4534
----- stdout -----
14336/32022 [============>.................] - ETA: 10s - loss: 1.4560
----- stdout -----
15360/32022 [=============>................] - ETA: 9s - loss: 1.4561
----- stdout -----
16384/32022 [==============>...............] - ETA: 9s - loss: 1.4563
----- stdout -----
17408/32022 [===============>..............] - ETA: 8s - loss: 1.4547
----- stdout -----
18432/32022 [================>.............] - ETA: 7s - loss: 1.4547
----- stdout -----
19456/32022 [=================>............] - ETA: 7s - loss: 1.4527
----- stdout -----
20480/32022 [==================>...........] - ETA: 6s - loss: 1.4529
----- stdout -----
21504/32022 [===================>..........] - ETA: 6s - loss: 1.4536
----- stdout -----
22528/32022 [====================>.........] - ETA: 5s - loss: 1.4533
----- stdout -----
23552/32022 [=====================>........] - ETA: 4s - loss: 1.4539
----- stdout -----
24576/32022 [======================>.......] - ETA: 4s - loss: 1.4551
----- stdout -----
25600/32022 [======================>.......] - ETA: 3s - loss: 1.4548
----- stdout -----
26624/32022 [=======================>......] - ETA: 3s - loss: 1.4533
----- stdout -----
27648/32022 [========================>.....] - ETA: 2s - loss: 1.4525
----- stdout -----
28672/32022 [=========================>....] - ETA: 1s - loss: 1.4518
----- stdout -----
29696/32022 [==========================>...] - ETA: 1s - loss: 1.4515
----- stdout -----
30720/32022 [===========================>..] - ETA: 0s - loss: 1.4510
----- stdout -----
31744/32022 [============================>.] - ETA: 0s - loss: 1.4516
----- stdout -----
32022/32022 [==============================] - ETA: 0s - loss: 1.4519
----- stdout -----

Epoch 15: val_loss improved from 1.38037 to 1.37697, saving model to ./checkpoints/unknown_person_few_shot_baseline_p1_30.h5
----- stdout -----
32022/32022 [==============================] - 20s 622us/sample - loss: 1.4519 - val_loss: 1.3770
----- stdout -----
Epoch 16/50
----- stdout -----
 1024/32022 [..............................] - ETA: 17s - loss: 1.5037
----- stdout -----
 2048/32022 [>.............................] - ETA: 16s - loss: 1.4747
----- stdout -----
 3072/32022 [=>............................] - ETA: 16s - loss: 1.4789
----- stdout -----
 4096/32022 [==>...........................] - ETA: 15s - loss: 1.4791
----- stdout -----
 5120/32022 [===>..........................] - ETA: 15s - loss: 1.4780
----- stdout -----
 6144/32022 [====>.........................] - ETA: 14s - loss: 1.4787
----- stdout -----
 7168/32022 [=====>........................] - ETA: 14s - loss: 1.4693
----- stdout -----
 8192/32022 [======>.......................] - ETA: 13s - loss: 1.4718
----- stdout -----
 9216/32022 [=======>......................] - ETA: 12s - loss: 1.4705
----- stdout -----
10240/32022 [========>.....................] - ETA: 12s - loss: 1.4693
----- stdout -----
11264/32022 [=========>....................] - ETA: 11s - loss: 1.4695
----- stdout -----
12288/32022 [==========>...................] - ETA: 11s - loss: 1.4674
----- stdout -----
13312/32022 [===========>..................] - ETA: 10s - loss: 1.4684
----- stdout -----
14336/32022 [============>.................] - ETA: 10s - loss: 1.4646
----- stdout -----
15360/32022 [=============>................] - ETA: 9s - loss: 1.4644
----- stdout -----
16384/32022 [==============>...............] - ETA: 9s - loss: 1.4599
----- stdout -----
17408/32022 [===============>..............] - ETA: 8s - loss: 1.4604
----- stdout -----
18432/32022 [================>.............] - ETA: 7s - loss: 1.4616
----- stdout -----
19456/32022 [=================>............] - ETA: 7s - loss: 1.4589
----- stdout -----
20480/32022 [==================>...........] - ETA: 6s - loss: 1.4581
----- stdout -----
21504/32022 [===================>..........] - ETA: 6s - loss: 1.4572
----- stdout -----
22528/32022 [====================>.........] - ETA: 5s - loss: 1.4576
----- stdout -----
23552/32022 [=====================>........] - ETA: 4s - loss: 1.4572
----- stdout -----
24576/32022 [======================>.......] - ETA: 4s - loss: 1.4573
----- stdout -----
25600/32022 [======================>.......] - ETA: 3s - loss: 1.4569
----- stdout -----
26624/32022 [=======================>......] - ETA: 3s - loss: 1.4559
----- stdout -----
27648/32022 [========================>.....] - ETA: 2s - loss: 1.4553
----- stdout -----
28672/32022 [=========================>....] - ETA: 1s - loss: 1.4550
----- stdout -----
29696/32022 [==========================>...] - ETA: 1s - loss: 1.4551
----- stdout -----
30720/32022 [===========================>..] - ETA: 0s - loss: 1.4543
----- stdout -----
31744/32022 [============================>.] - ETA: 0s - loss: 1.4533
----- stdout -----
32022/32022 [==============================] - ETA: 0s - loss: 1.4536
----- stdout -----

Epoch 16: val_loss did not improve from 1.37697
----- stdout -----
32022/32022 [==============================] - 20s 615us/sample - loss: 1.4536 - val_loss: 1.3773
----- stdout -----
Epoch 17/50
----- stdout -----
 1024/32022 [..............................] - ETA: 17s - loss: 1.4380
----- stdout -----
 2048/32022 [>.............................] - ETA: 18s - loss: 1.4510
----- stdout -----
 3072/32022 [=>............................] - ETA: 17s - loss: 1.4463
----- stdout -----
 4096/32022 [==>...........................] - ETA: 16s - loss: 1.4551
----- stdout -----
 5120/32022 [===>..........................] - ETA: 15s - loss: 1.4585
----- stdout -----
 6144/32022 [====>.........................] - ETA: 15s - loss: 1.4574
----- stdout -----
 7168/32022 [=====>........................] - ETA: 14s - loss: 1.4597
----- stdout -----
 8192/32022 [======>.......................] - ETA: 13s - loss: 1.4565
----- stdout -----
 9216/32022 [=======>......................] - ETA: 13s - loss: 1.4557
----- stdout -----
10240/32022 [========>.....................] - ETA: 12s - loss: 1.4508
----- stdout -----
11264/32022 [=========>....................] - ETA: 12s - loss: 1.4502
----- stdout -----
12288/32022 [==========>...................] - ETA: 11s - loss: 1.4511
----- stdout -----
13312/32022 [===========>..................] - ETA: 10s - loss: 1.4552
----- stdout -----
14336/32022 [============>.................] - ETA: 10s - loss: 1.4529
----- stdout -----
15360/32022 [=============>................] - ETA: 9s - loss: 1.4525
----- stdout -----
16384/32022 [==============>...............] - ETA: 9s - loss: 1.4536
----- stdout -----
17408/32022 [===============>..............] - ETA: 8s - loss: 1.4520
----- stdout -----
18432/32022 [================>.............] - ETA: 7s - loss: 1.4496
----- stdout -----
19456/32022 [=================>............] - ETA: 7s - loss: 1.4487
----- stdout -----
20480/32022 [==================>...........] - ETA: 6s - loss: 1.4500
----- stdout -----
21504/32022 [===================>..........] - ETA: 6s - loss: 1.4505
----- stdout -----
22528/32022 [====================>.........] - ETA: 5s - loss: 1.4496
----- stdout -----
23552/32022 [=====================>........] - ETA: 4s - loss: 1.4503
----- stdout -----
24576/32022 [======================>.......] - ETA: 4s - loss: 1.4493
----- stdout -----
25600/32022 [======================>.......] - ETA: 3s - loss: 1.4482
----- stdout -----
26624/32022 [=======================>......] - ETA: 3s - loss: 1.4496
----- stdout -----
27648/32022 [========================>.....] - ETA: 2s - loss: 1.4499
----- stdout -----
28672/32022 [=========================>....] - ETA: 1s - loss: 1.4492
----- stdout -----
29696/32022 [==========================>...] - ETA: 1s - loss: 1.4488
----- stdout -----
30720/32022 [===========================>..] - ETA: 0s - loss: 1.4480
----- stdout -----
31744/32022 [============================>.] - ETA: 0s - loss: 1.4479
----- stdout -----
32022/32022 [==============================] - ETA: 0s - loss: 1.4475
----- stdout -----

Epoch 17: val_loss improved from 1.37697 to 1.37025, saving model to ./checkpoints/unknown_person_few_shot_baseline_p1_30.h5
----- stdout -----
32022/32022 [==============================] - 20s 632us/sample - loss: 1.4475 - val_loss: 1.3703
----- stdout -----
Epoch 18/50
----- stdout -----
 1024/32022 [..............................] - ETA: 17s - loss: 1.4492
----- stdout -----
 2048/32022 [>.............................] - ETA: 17s - loss: 1.4441
----- stdout -----
 3072/32022 [=>............................] - ETA: 16s - loss: 1.4608
----- stdout -----
 4096/32022 [==>...........................] - ETA: 15s - loss: 1.4533
----- stdout -----
 5120/32022 [===>..........................] - ETA: 15s - loss: 1.4541
----- stdout -----
 6144/32022 [====>.........................] - ETA: 14s - loss: 1.4533
----- stdout -----
 7168/32022 [=====>........................] - ETA: 14s - loss: 1.4465
----- stdout -----
 8192/32022 [======>.......................] - ETA: 13s - loss: 1.4431
----- stdout -----
 9216/32022 [=======>......................] - ETA: 13s - loss: 1.4454
----- stdout -----
10240/32022 [========>.....................] - ETA: 12s - loss: 1.4459
----- stdout -----
11264/32022 [=========>....................] - ETA: 12s - loss: 1.4441
----- stdout -----
12288/32022 [==========>...................] - ETA: 11s - loss: 1.4416
----- stdout -----
13312/32022 [===========>..................] - ETA: 10s - loss: 1.4427
----- stdout -----
14336/32022 [============>.................] - ETA: 10s - loss: 1.4400
----- stdout -----
15360/32022 [=============>................] - ETA: 9s - loss: 1.4419
----- stdout -----
16384/32022 [==============>...............] - ETA: 9s - loss: 1.4398
----- stdout -----
17408/32022 [===============>..............] - ETA: 8s - loss: 1.4408
----- stdout -----
18432/32022 [================>.............] - ETA: 7s - loss: 1.4421
----- stdout -----
19456/32022 [=================>............] - ETA: 7s - loss: 1.4420
----- stdout -----
20480/32022 [==================>...........] - ETA: 6s - loss: 1.4424
----- stdout -----
21504/32022 [===================>..........] - ETA: 6s - loss: 1.4431
----- stdout -----
22528/32022 [====================>.........] - ETA: 5s - loss: 1.4430
----- stdout -----
23552/32022 [=====================>........] - ETA: 4s - loss: 1.4429
----- stdout -----
24576/32022 [======================>.......] - ETA: 4s - loss: 1.4431
----- stdout -----
25600/32022 [======================>.......] - ETA: 3s - loss: 1.4419
----- stdout -----
26624/32022 [=======================>......] - ETA: 3s - loss: 1.4418
----- stdout -----
27648/32022 [========================>.....] - ETA: 2s - loss: 1.4437
----- stdout -----
28672/32022 [=========================>....] - ETA: 1s - loss: 1.4447
----- stdout -----
29696/32022 [==========================>...] - ETA: 1s - loss: 1.4440
----- stdout -----
30720/32022 [===========================>..] - ETA: 0s - loss: 1.4439
----- stdout -----
31744/32022 [============================>.] - ETA: 0s - loss: 1.4423
----- stdout -----
32022/32022 [==============================] - ETA: 0s - loss: 1.4422
----- stdout -----

Epoch 18: val_loss improved from 1.37025 to 1.36952, saving model to ./checkpoints/unknown_person_few_shot_baseline_p1_30.h5
----- stdout -----
32022/32022 [==============================] - 20s 617us/sample - loss: 1.4422 - val_loss: 1.3695
----- stdout -----
Epoch 19/50
----- stdout -----
 1024/32022 [..............................] - ETA: 17s - loss: 1.5290
----- stdout -----
 2048/32022 [>.............................] - ETA: 17s - loss: 1.4957
----- stdout -----
 3072/32022 [=>............................] - ETA: 16s - loss: 1.4986
----- stdout -----
 4096/32022 [==>...........................] - ETA: 16s - loss: 1.4874
----- stdout -----
 5120/32022 [===>..........................] - ETA: 15s - loss: 1.4818
----- stdout -----
 6144/32022 [====>.........................] - ETA: 14s - loss: 1.4738
----- stdout -----
 7168/32022 [=====>........................] - ETA: 14s - loss: 1.4717
----- stdout -----
 8192/32022 [======>.......................] - ETA: 13s - loss: 1.4677
----- stdout -----
 9216/32022 [=======>......................] - ETA: 13s - loss: 1.4628
----- stdout -----
10240/32022 [========>.....................] - ETA: 12s - loss: 1.4628
----- stdout -----
11264/32022 [=========>....................] - ETA: 11s - loss: 1.4576
----- stdout -----
12288/32022 [==========>...................] - ETA: 11s - loss: 1.4607
----- stdout -----
13312/32022 [===========>..................] - ETA: 10s - loss: 1.4632
----- stdout -----
14336/32022 [============>.................] - ETA: 10s - loss: 1.4564
----- stdout -----
15360/32022 [=============>................] - ETA: 9s - loss: 1.4536
----- stdout -----
16384/32022 [==============>...............] - ETA: 8s - loss: 1.4526
----- stdout -----
17408/32022 [===============>..............] - ETA: 8s - loss: 1.4519
----- stdout -----
18432/32022 [================>.............] - ETA: 7s - loss: 1.4510
----- stdout -----
19456/32022 [=================>............] - ETA: 7s - loss: 1.4506
----- stdout -----
20480/32022 [==================>...........] - ETA: 6s - loss: 1.4507
----- stdout -----
21504/32022 [===================>..........] - ETA: 6s - loss: 1.4514
----- stdout -----
22528/32022 [====================>.........] - ETA: 5s - loss: 1.4493
----- stdout -----
23552/32022 [=====================>........] - ETA: 4s - loss: 1.4483
----- stdout -----
24576/32022 [======================>.......] - ETA: 4s - loss: 1.4471
----- stdout -----
25600/32022 [======================>.......] - ETA: 3s - loss: 1.4477
----- stdout -----
26624/32022 [=======================>......] - ETA: 3s - loss: 1.4478
----- stdout -----
27648/32022 [========================>.....] - ETA: 2s - loss: 1.4482
----- stdout -----
28672/32022 [=========================>....] - ETA: 1s - loss: 1.4449
----- stdout -----
29696/32022 [==========================>...] - ETA: 1s - loss: 1.4446
----- stdout -----
30720/32022 [===========================>..] - ETA: 0s - loss: 1.4439
----- stdout -----
31744/32022 [============================>.] - ETA: 0s - loss: 1.4440
----- stdout -----
32022/32022 [==============================] - ETA: 0s - loss: 1.4435
----- stdout -----

Epoch 19: val_loss did not improve from 1.36952
----- stdout -----
32022/32022 [==============================] - 20s 612us/sample - loss: 1.4435 - val_loss: 1.3727
----- stdout -----
Epoch 20/50
----- stdout -----
 1024/32022 [..............................] - ETA: 17s - loss: 1.4520
----- stdout -----
 2048/32022 [>.............................] - ETA: 17s - loss: 1.4431
----- stdout -----
 3072/32022 [=>............................] - ETA: 16s - loss: 1.4395
----- stdout -----
 4096/32022 [==>...........................] - ETA: 15s - loss: 1.4336
----- stdout -----
 5120/32022 [===>..........................] - ETA: 15s - loss: 1.4438
----- stdout -----
 6144/32022 [====>.........................] - ETA: 14s - loss: 1.4524
----- stdout -----
 7168/32022 [=====>........................] - ETA: 14s - loss: 1.4536
----- stdout -----
 8192/32022 [======>.......................] - ETA: 13s - loss: 1.4611
----- stdout -----
 9216/32022 [=======>......................] - ETA: 13s - loss: 1.4545
----- stdout -----
10240/32022 [========>.....................] - ETA: 12s - loss: 1.4565
----- stdout -----
11264/32022 [=========>....................] - ETA: 11s - loss: 1.4525
----- stdout -----
12288/32022 [==========>...................] - ETA: 11s - loss: 1.4518
----- stdout -----
13312/32022 [===========>..................] - ETA: 10s - loss: 1.4459
----- stdout -----
14336/32022 [============>.................] - ETA: 10s - loss: 1.4453
----- stdout -----
15360/32022 [=============>................] - ETA: 9s - loss: 1.4425
----- stdout -----
16384/32022 [==============>...............] - ETA: 8s - loss: 1.4432
----- stdout -----
17408/32022 [===============>..............] - ETA: 8s - loss: 1.4407
----- stdout -----
18432/32022 [================>.............] - ETA: 7s - loss: 1.4421
----- stdout -----
19456/32022 [=================>............] - ETA: 7s - loss: 1.4412
----- stdout -----
20480/32022 [==================>...........] - ETA: 6s - loss: 1.4389
----- stdout -----
21504/32022 [===================>..........] - ETA: 6s - loss: 1.4394
----- stdout -----
22528/32022 [====================>.........] - ETA: 5s - loss: 1.4399
----- stdout -----
23552/32022 [=====================>........] - ETA: 4s - loss: 1.4417
----- stdout -----
24576/32022 [======================>.......] - ETA: 4s - loss: 1.4417
----- stdout -----
25600/32022 [======================>.......] - ETA: 3s - loss: 1.4420
----- stdout -----
26624/32022 [=======================>......] - ETA: 3s - loss: 1.4421
----- stdout -----
27648/32022 [========================>.....] - ETA: 2s - loss: 1.4417
----- stdout -----
28672/32022 [=========================>....] - ETA: 1s - loss: 1.4423
----- stdout -----
29696/32022 [==========================>...] - ETA: 1s - loss: 1.4431
----- stdout -----
30720/32022 [===========================>..] - ETA: 0s - loss: 1.4422
----- stdout -----
31744/32022 [============================>.] - ETA: 0s - loss: 1.4407
----- stdout -----
32022/32022 [==============================] - ETA: 0s - loss: 1.4412
----- stdout -----

Epoch 20: val_loss did not improve from 1.36952
----- stdout -----
32022/32022 [==============================] - 20s 615us/sample - loss: 1.4412 - val_loss: 1.3725
----- stdout -----
Epoch 21/50
----- stdout -----
 1024/32022 [..............................] - ETA: 17s - loss: 1.5023
----- stdout -----
 2048/32022 [>.............................] - ETA: 17s - loss: 1.4767
----- stdout -----
 3072/32022 [=>............................] - ETA: 16s - loss: 1.4618
----- stdout -----
 4096/32022 [==>...........................] - ETA: 16s - loss: 1.4483
----- stdout -----
 5120/32022 [===>..........................] - ETA: 15s - loss: 1.4619
----- stdout -----
 6144/32022 [====>.........................] - ETA: 15s - loss: 1.4598
----- stdout -----
 7168/32022 [=====>........................] - ETA: 14s - loss: 1.4606
----- stdout -----
 8192/32022 [======>.......................] - ETA: 13s - loss: 1.4623
----- stdout -----
 9216/32022 [=======>......................] - ETA: 13s - loss: 1.4657
----- stdout -----
10240/32022 [========>.....................] - ETA: 12s - loss: 1.4710
----- stdout -----
11264/32022 [=========>....................] - ETA: 11s - loss: 1.4714
----- stdout -----
12288/32022 [==========>...................] - ETA: 11s - loss: 1.4690
----- stdout -----
13312/32022 [===========>..................] - ETA: 10s - loss: 1.4666
----- stdout -----
14336/32022 [============>.................] - ETA: 10s - loss: 1.4623
----- stdout -----
15360/32022 [=============>................] - ETA: 9s - loss: 1.4627
----- stdout -----
16384/32022 [==============>...............] - ETA: 9s - loss: 1.4599
----- stdout -----
17408/32022 [===============>..............] - ETA: 8s - loss: 1.4589
----- stdout -----
18432/32022 [================>.............] - ETA: 7s - loss: 1.4592
----- stdout -----
19456/32022 [=================>............] - ETA: 7s - loss: 1.4590
----- stdout -----
20480/32022 [==================>...........] - ETA: 6s - loss: 1.4572
----- stdout -----
21504/32022 [===================>..........] - ETA: 6s - loss: 1.4554
----- stdout -----
22528/32022 [====================>.........] - ETA: 5s - loss: 1.4555
----- stdout -----
23552/32022 [=====================>........] - ETA: 4s - loss: 1.4552
----- stdout -----
24576/32022 [======================>.......] - ETA: 4s - loss: 1.4557
----- stdout -----
25600/32022 [======================>.......] - ETA: 3s - loss: 1.4545
----- stdout -----
26624/32022 [=======================>......] - ETA: 3s - loss: 1.4548
----- stdout -----
27648/32022 [========================>.....] - ETA: 2s - loss: 1.4542
----- stdout -----
28672/32022 [=========================>....] - ETA: 1s - loss: 1.4527
----- stdout -----
29696/32022 [==========================>...] - ETA: 1s - loss: 1.4528
----- stdout -----
30720/32022 [===========================>..] - ETA: 0s - loss: 1.4524
----- stdout -----
31744/32022 [============================>.] - ETA: 0s - loss: 1.4534
----- stdout -----
32022/32022 [==============================] - ETA: 0s - loss: 1.4526
----- stdout -----

Epoch 21: val_loss improved from 1.36952 to 1.36693, saving model to ./checkpoints/unknown_person_few_shot_baseline_p1_30.h5
----- stdout -----
32022/32022 [==============================] - 20s 618us/sample - loss: 1.4526 - val_loss: 1.3669
----- stdout -----
Epoch 22/50
----- stdout -----
 1024/32022 [..............................] - ETA: 17s - loss: 1.4562
----- stdout -----
 2048/32022 [>.............................] - ETA: 17s - loss: 1.4846
----- stdout -----
 3072/32022 [=>............................] - ETA: 16s - loss: 1.4656
----- stdout -----
 4096/32022 [==>...........................] - ETA: 15s - loss: 1.4594
----- stdout -----
 5120/32022 [===>..........................] - ETA: 15s - loss: 1.4526
----- stdout -----
 6144/32022 [====>.........................] - ETA: 14s - loss: 1.4551
----- stdout -----
 7168/32022 [=====>........................] - ETA: 14s - loss: 1.4560
----- stdout -----
 8192/32022 [======>.......................] - ETA: 13s - loss: 1.4493
----- stdout -----
 9216/32022 [=======>......................] - ETA: 13s - loss: 1.4537
----- stdout -----
10240/32022 [========>.....................] - ETA: 12s - loss: 1.4552
----- stdout -----
11264/32022 [=========>....................] - ETA: 11s - loss: 1.4579
----- stdout -----
12288/32022 [==========>...................] - ETA: 11s - loss: 1.4531
----- stdout -----
13312/32022 [===========>..................] - ETA: 10s - loss: 1.4536
----- stdout -----
14336/32022 [============>.................] - ETA: 10s - loss: 1.4518
----- stdout -----
15360/32022 [=============>................] - ETA: 9s - loss: 1.4511
----- stdout -----
16384/32022 [==============>...............] - ETA: 8s - loss: 1.4479
----- stdout -----
17408/32022 [===============>..............] - ETA: 8s - loss: 1.4473
----- stdout -----
18432/32022 [================>.............] - ETA: 7s - loss: 1.4479
----- stdout -----
19456/32022 [=================>............] - ETA: 7s - loss: 1.4483
----- stdout -----
20480/32022 [==================>...........] - ETA: 6s - loss: 1.4477
----- stdout -----
21504/32022 [===================>..........] - ETA: 6s - loss: 1.4486
----- stdout -----
22528/32022 [====================>.........] - ETA: 5s - loss: 1.4466
----- stdout -----
23552/32022 [=====================>........] - ETA: 4s - loss: 1.4458
----- stdout -----
24576/32022 [======================>.......] - ETA: 4s - loss: 1.4447
----- stdout -----
25600/32022 [======================>.......] - ETA: 3s - loss: 1.4445
----- stdout -----
26624/32022 [=======================>......] - ETA: 3s - loss: 1.4432
----- stdout -----
27648/32022 [========================>.....] - ETA: 2s - loss: 1.4429
----- stdout -----
28672/32022 [=========================>....] - ETA: 1s - loss: 1.4436
----- stdout -----
29696/32022 [==========================>...] - ETA: 1s - loss: 1.4419
----- stdout -----
30720/32022 [===========================>..] - ETA: 0s - loss: 1.4413
----- stdout -----
31744/32022 [============================>.] - ETA: 0s - loss: 1.4405
----- stdout -----
32022/32022 [==============================] - ETA: 0s - loss: 1.4402
----- stdout -----

Epoch 22: val_loss did not improve from 1.36693
----- stdout -----
32022/32022 [==============================] - 20s 615us/sample - loss: 1.4402 - val_loss: 1.3692
----- stdout -----
Epoch 23/50
----- stdout -----
 1024/32022 [..............................] - ETA: 17s - loss: 1.4743
----- stdout -----
 2048/32022 [>.............................] - ETA: 17s - loss: 1.4744
----- stdout -----
 3072/32022 [=>............................] - ETA: 16s - loss: 1.4644
----- stdout -----
 4096/32022 [==>...........................] - ETA: 16s - loss: 1.4600
----- stdout -----
 5120/32022 [===>..........................] - ETA: 15s - loss: 1.4519
----- stdout -----
 6144/32022 [====>.........................] - ETA: 15s - loss: 1.4443
----- stdout -----
 7168/32022 [=====>........................] - ETA: 14s - loss: 1.4483
----- stdout -----
 8192/32022 [======>.......................] - ETA: 13s - loss: 1.4515
----- stdout -----
 9216/32022 [=======>......................] - ETA: 13s - loss: 1.4519
----- stdout -----
10240/32022 [========>.....................] - ETA: 12s - loss: 1.4487
----- stdout -----
11264/32022 [=========>....................] - ETA: 12s - loss: 1.4525
----- stdout -----
12288/32022 [==========>...................] - ETA: 11s - loss: 1.4521
----- stdout -----
13312/32022 [===========>..................] - ETA: 10s - loss: 1.4526
----- stdout -----
14336/32022 [============>.................] - ETA: 10s - loss: 1.4481
----- stdout -----
15360/32022 [=============>................] - ETA: 9s - loss: 1.4463
----- stdout -----
16384/32022 [==============>...............] - ETA: 9s - loss: 1.4456
----- stdout -----
17408/32022 [===============>..............] - ETA: 8s - loss: 1.4472
----- stdout -----
18432/32022 [================>.............] - ETA: 7s - loss: 1.4436
----- stdout -----
19456/32022 [=================>............] - ETA: 7s - loss: 1.4433
----- stdout -----
20480/32022 [==================>...........] - ETA: 6s - loss: 1.4453
----- stdout -----
21504/32022 [===================>..........] - ETA: 6s - loss: 1.4460
----- stdout -----
22528/32022 [====================>.........] - ETA: 5s - loss: 1.4452
----- stdout -----
23552/32022 [=====================>........] - ETA: 4s - loss: 1.4453
----- stdout -----
24576/32022 [======================>.......] - ETA: 4s - loss: 1.4439
----- stdout -----
25600/32022 [======================>.......] - ETA: 3s - loss: 1.4440
----- stdout -----
26624/32022 [=======================>......] - ETA: 3s - loss: 1.4433
----- stdout -----
27648/32022 [========================>.....] - ETA: 2s - loss: 1.4426
----- stdout -----
28672/32022 [=========================>....] - ETA: 1s - loss: 1.4408
----- stdout -----
29696/32022 [==========================>...] - ETA: 1s - loss: 1.4409
----- stdout -----
30720/32022 [===========================>..] - ETA: 0s - loss: 1.4409
----- stdout -----
31744/32022 [============================>.] - ETA: 0s - loss: 1.4403
----- stdout -----
32022/32022 [==============================] - ETA: 0s - loss: 1.4402
----- stdout -----

Epoch 23: val_loss did not improve from 1.36693
----- stdout -----
32022/32022 [==============================] - 20s 615us/sample - loss: 1.4402 - val_loss: 1.3679
----- stdout -----
Epoch 24/50
----- stdout -----
 1024/32022 [..............................] - ETA: 17s - loss: 1.4267
----- stdout -----
 2048/32022 [>.............................] - ETA: 17s - loss: 1.4346
----- stdout -----
 3072/32022 [=>............................] - ETA: 16s - loss: 1.4427
----- stdout -----
 4096/32022 [==>...........................] - ETA: 16s - loss: 1.4425
----- stdout -----
 5120/32022 [===>..........................] - ETA: 15s - loss: 1.4437
----- stdout -----
 6144/32022 [====>.........................] - ETA: 15s - loss: 1.4384
----- stdout -----
 7168/32022 [=====>........................] - ETA: 14s - loss: 1.4382
----- stdout -----
 8192/32022 [======>.......................] - ETA: 13s - loss: 1.4370
----- stdout -----
 9216/32022 [=======>......................] - ETA: 13s - loss: 1.4367
----- stdout -----
10240/32022 [========>.....................] - ETA: 12s - loss: 1.4384
----- stdout -----
11264/32022 [=========>....................] - ETA: 12s - loss: 1.4425
----- stdout -----
12288/32022 [==========>...................] - ETA: 11s - loss: 1.4389
----- stdout -----
13312/32022 [===========>..................] - ETA: 10s - loss: 1.4352
----- stdout -----
14336/32022 [============>.................] - ETA: 10s - loss: 1.4335
----- stdout -----
15360/32022 [=============>................] - ETA: 9s - loss: 1.4329
----- stdout -----
16384/32022 [==============>...............] - ETA: 9s - loss: 1.4340
----- stdout -----
17408/32022 [===============>..............] - ETA: 8s - loss: 1.4358
----- stdout -----
18432/32022 [================>.............] - ETA: 7s - loss: 1.4347
----- stdout -----
19456/32022 [=================>............] - ETA: 7s - loss: 1.4327
----- stdout -----
20480/32022 [==================>...........] - ETA: 6s - loss: 1.4349
----- stdout -----
21504/32022 [===================>..........] - ETA: 6s - loss: 1.4338
----- stdout -----
22528/32022 [====================>.........] - ETA: 5s - loss: 1.4317
----- stdout -----
23552/32022 [=====================>........] - ETA: 4s - loss: 1.4303
----- stdout -----
24576/32022 [======================>.......] - ETA: 4s - loss: 1.4298
----- stdout -----
25600/32022 [======================>.......] - ETA: 3s - loss: 1.4300
----- stdout -----
26624/32022 [=======================>......] - ETA: 3s - loss: 1.4297
----- stdout -----
27648/32022 [========================>.....] - ETA: 2s - loss: 1.4288
----- stdout -----
28672/32022 [=========================>....] - ETA: 1s - loss: 1.4286
----- stdout -----
29696/32022 [==========================>...] - ETA: 1s - loss: 1.4307
----- stdout -----
30720/32022 [===========================>..] - ETA: 0s - loss: 1.4307
----- stdout -----
31744/32022 [============================>.] - ETA: 0s - loss: 1.4323
----- stdout -----
32022/32022 [==============================] - ETA: 0s - loss: 1.4320
----- stdout -----

Epoch 24: val_loss improved from 1.36693 to 1.36357, saving model to ./checkpoints/unknown_person_few_shot_baseline_p1_30.h5
----- stdout -----
32022/32022 [==============================] - 20s 621us/sample - loss: 1.4320 - val_loss: 1.3636
----- stdout -----
Epoch 25/50
----- stdout -----
 1024/32022 [..............................] - ETA: 18s - loss: 1.4604
----- stdout -----
 2048/32022 [>.............................] - ETA: 17s - loss: 1.4400
----- stdout -----
 3072/32022 [=>............................] - ETA: 16s - loss: 1.4294
----- stdout -----
 4096/32022 [==>...........................] - ETA: 16s - loss: 1.4246
----- stdout -----
 5120/32022 [===>..........................] - ETA: 15s - loss: 1.4239
----- stdout -----
 6144/32022 [====>.........................] - ETA: 14s - loss: 1.4329
----- stdout -----
 7168/32022 [=====>........................] - ETA: 14s - loss: 1.4353
----- stdout -----
 8192/32022 [======>.......................] - ETA: 13s - loss: 1.4356
----- stdout -----
 9216/32022 [=======>......................] - ETA: 13s - loss: 1.4327
----- stdout -----
10240/32022 [========>.....................] - ETA: 12s - loss: 1.4339
----- stdout -----
11264/32022 [=========>....................] - ETA: 12s - loss: 1.4343
----- stdout -----
12288/32022 [==========>...................] - ETA: 11s - loss: 1.4336
----- stdout -----
13312/32022 [===========>..................] - ETA: 10s - loss: 1.4328
----- stdout -----
14336/32022 [============>.................] - ETA: 10s - loss: 1.4320
----- stdout -----
15360/32022 [=============>................] - ETA: 9s - loss: 1.4297
----- stdout -----
16384/32022 [==============>...............] - ETA: 9s - loss: 1.4317
----- stdout -----
17408/32022 [===============>..............] - ETA: 8s - loss: 1.4317
----- stdout -----
18432/32022 [================>.............] - ETA: 7s - loss: 1.4312
----- stdout -----
19456/32022 [=================>............] - ETA: 7s - loss: 1.4312
----- stdout -----
20480/32022 [==================>...........] - ETA: 6s - loss: 1.4321
----- stdout -----
21504/32022 [===================>..........] - ETA: 6s - loss: 1.4300
----- stdout -----
22528/32022 [====================>.........] - ETA: 5s - loss: 1.4304
----- stdout -----
23552/32022 [=====================>........] - ETA: 4s - loss: 1.4304
----- stdout -----
24576/32022 [======================>.......] - ETA: 4s - loss: 1.4301
----- stdout -----
25600/32022 [======================>.......] - ETA: 3s - loss: 1.4297
----- stdout -----
26624/32022 [=======================>......] - ETA: 3s - loss: 1.4286
----- stdout -----
27648/32022 [========================>.....] - ETA: 2s - loss: 1.4271
----- stdout -----
28672/32022 [=========================>....] - ETA: 1s - loss: 1.4274
----- stdout -----
29696/32022 [==========================>...] - ETA: 1s - loss: 1.4284
----- stdout -----
30720/32022 [===========================>..] - ETA: 0s - loss: 1.4274
----- stdout -----
31744/32022 [============================>.] - ETA: 0s - loss: 1.4278
----- stdout -----
32022/32022 [==============================] - ETA: 0s - loss: 1.4281
----- stdout -----

Epoch 25: val_loss improved from 1.36357 to 1.36201, saving model to ./checkpoints/unknown_person_few_shot_baseline_p1_30.h5
----- stdout -----
32022/32022 [==============================] - 20s 621us/sample - loss: 1.4281 - val_loss: 1.3620
----- stdout -----
Epoch 26/50
----- stdout -----
 1024/32022 [..............................] - ETA: 17s - loss: 1.4264
----- stdout -----
 2048/32022 [>.............................] - ETA: 17s - loss: 1.4187
----- stdout -----
 3072/32022 [=>............................] - ETA: 16s - loss: 1.4267
----- stdout -----
 4096/32022 [==>...........................] - ETA: 16s - loss: 1.4253
----- stdout -----
 5120/32022 [===>..........................] - ETA: 15s - loss: 1.4257
----- stdout -----
 6144/32022 [====>.........................] - ETA: 14s - loss: 1.4275
----- stdout -----
 7168/32022 [=====>........................] - ETA: 14s - loss: 1.4293
----- stdout -----
 8192/32022 [======>.......................] - ETA: 13s - loss: 1.4337
----- stdout -----
 9216/32022 [=======>......................] - ETA: 13s - loss: 1.4305
----- stdout -----
10240/32022 [========>.....................] - ETA: 12s - loss: 1.4284
----- stdout -----
11264/32022 [=========>....................] - ETA: 12s - loss: 1.4284
----- stdout -----
12288/32022 [==========>...................] - ETA: 11s - loss: 1.4286
----- stdout -----
13312/32022 [===========>..................] - ETA: 10s - loss: 1.4251
----- stdout -----
14336/32022 [============>.................] - ETA: 10s - loss: 1.4222
----- stdout -----
15360/32022 [=============>................] - ETA: 9s - loss: 1.4225
----- stdout -----
16384/32022 [==============>...............] - ETA: 9s - loss: 1.4182
----- stdout -----
17408/32022 [===============>..............] - ETA: 8s - loss: 1.4189
----- stdout -----
18432/32022 [================>.............] - ETA: 8s - loss: 1.4179
----- stdout -----
19456/32022 [=================>............] - ETA: 7s - loss: 1.4189
----- stdout -----
20480/32022 [==================>...........] - ETA: 6s - loss: 1.4206
----- stdout -----
21504/32022 [===================>..........] - ETA: 6s - loss: 1.4224
----- stdout -----
22528/32022 [====================>.........] - ETA: 5s - loss: 1.4240
----- stdout -----
23552/32022 [=====================>........] - ETA: 5s - loss: 1.4240
----- stdout -----
24576/32022 [======================>.......] - ETA: 4s - loss: 1.4238
----- stdout -----
25600/32022 [======================>.......] - ETA: 3s - loss: 1.4243
----- stdout -----
26624/32022 [=======================>......] - ETA: 3s - loss: 1.4255
----- stdout -----
27648/32022 [========================>.....] - ETA: 2s - loss: 1.4249
----- stdout -----
28672/32022 [=========================>....] - ETA: 2s - loss: 1.4244
----- stdout -----
29696/32022 [==========================>...] - ETA: 1s - loss: 1.4245
----- stdout -----
30720/32022 [===========================>..] - ETA: 0s - loss: 1.4244
----- stdout -----
31744/32022 [============================>.] - ETA: 0s - loss: 1.4238
----- stdout -----
32022/32022 [==============================] - ETA: 0s - loss: 1.4238
----- stdout -----

Epoch 26: val_loss improved from 1.36201 to 1.35928, saving model to ./checkpoints/unknown_person_few_shot_baseline_p1_30.h5
----- stdout -----
32022/32022 [==============================] - 21s 667us/sample - loss: 1.4238 - val_loss: 1.3593
----- stdout -----
Epoch 27/50
----- stdout -----
 1024/32022 [..............................] - ETA: 18s - loss: 1.3980
----- stdout -----
 2048/32022 [>.............................] - ETA: 17s - loss: 1.4272
----- stdout -----
 3072/32022 [=>............................] - ETA: 16s - loss: 1.4119
----- stdout -----
 4096/32022 [==>...........................] - ETA: 16s - loss: 1.4150
----- stdout -----
 5120/32022 [===>..........................] - ETA: 15s - loss: 1.4248
----- stdout -----
 6144/32022 [====>.........................] - ETA: 15s - loss: 1.4314
----- stdout -----
 7168/32022 [=====>........................] - ETA: 14s - loss: 1.4219
----- stdout -----
 8192/32022 [======>.......................] - ETA: 13s - loss: 1.4248
----- stdout -----
 9216/32022 [=======>......................] - ETA: 13s - loss: 1.4279
----- stdout -----
10240/32022 [========>.....................] - ETA: 12s - loss: 1.4270
----- stdout -----
11264/32022 [=========>....................] - ETA: 12s - loss: 1.4250
----- stdout -----
12288/32022 [==========>...................] - ETA: 11s - loss: 1.4212
----- stdout -----
13312/32022 [===========>..................] - ETA: 10s - loss: 1.4237
----- stdout -----
14336/32022 [============>.................] - ETA: 10s - loss: 1.4216
----- stdout -----
15360/32022 [=============>................] - ETA: 9s - loss: 1.4216
----- stdout -----
16384/32022 [==============>...............] - ETA: 9s - loss: 1.4261
----- stdout -----
17408/32022 [===============>..............] - ETA: 8s - loss: 1.4233
----- stdout -----
18432/32022 [================>.............] - ETA: 8s - loss: 1.4242
----- stdout -----
19456/32022 [=================>............] - ETA: 7s - loss: 1.4248
----- stdout -----
20480/32022 [==================>...........] - ETA: 6s - loss: 1.4250
----- stdout -----
21504/32022 [===================>..........] - ETA: 6s - loss: 1.4235
----- stdout -----
22528/32022 [====================>.........] - ETA: 5s - loss: 1.4238
----- stdout -----
23552/32022 [=====================>........] - ETA: 5s - loss: 1.4238
----- stdout -----
24576/32022 [======================>.......] - ETA: 4s - loss: 1.4241
----- stdout -----
25600/32022 [======================>.......] - ETA: 3s - loss: 1.4245
----- stdout -----
26624/32022 [=======================>......] - ETA: 3s - loss: 1.4265
----- stdout -----
27648/32022 [========================>.....] - ETA: 2s - loss: 1.4243
----- stdout -----
28672/32022 [=========================>....] - ETA: 2s - loss: 1.4224
----- stdout -----
29696/32022 [==========================>...] - ETA: 1s - loss: 1.4222
----- stdout -----
30720/32022 [===========================>..] - ETA: 0s - loss: 1.4236
----- stdout -----
31744/32022 [============================>.] - ETA: 0s - loss: 1.4234
----- stdout -----
32022/32022 [==============================] - ETA: 0s - loss: 1.4231
----- stdout -----

Epoch 27: val_loss improved from 1.35928 to 1.35489, saving model to ./checkpoints/unknown_person_few_shot_baseline_p1_30.h5
----- stdout -----
32022/32022 [==============================] - 22s 677us/sample - loss: 1.4231 - val_loss: 1.3549
----- stdout -----
Epoch 28/50
----- stdout -----
 1024/32022 [..............................] - ETA: 18s - loss: 1.4282
----- stdout -----
 2048/32022 [>.............................] - ETA: 17s - loss: 1.4172
----- stdout -----
 3072/32022 [=>............................] - ETA: 16s - loss: 1.4086
----- stdout -----
 4096/32022 [==>...........................] - ETA: 16s - loss: 1.4119
----- stdout -----
 5120/32022 [===>..........................] - ETA: 15s - loss: 1.4155
----- stdout -----
 6144/32022 [====>.........................] - ETA: 15s - loss: 1.4062
----- stdout -----
 7168/32022 [=====>........................] - ETA: 14s - loss: 1.4162
----- stdout -----
 8192/32022 [======>.......................] - ETA: 13s - loss: 1.4198
----- stdout -----
 9216/32022 [=======>......................] - ETA: 13s - loss: 1.4198
----- stdout -----
10240/32022 [========>.....................] - ETA: 12s - loss: 1.4201
----- stdout -----
11264/32022 [=========>....................] - ETA: 12s - loss: 1.4192
----- stdout -----
12288/32022 [==========>...................] - ETA: 11s - loss: 1.4212
----- stdout -----
13312/32022 [===========>..................] - ETA: 10s - loss: 1.4200
----- stdout -----
14336/32022 [============>.................] - ETA: 10s - loss: 1.4212
----- stdout -----
15360/32022 [=============>................] - ETA: 9s - loss: 1.4199
----- stdout -----
16384/32022 [==============>...............] - ETA: 9s - loss: 1.4209
----- stdout -----
17408/32022 [===============>..............] - ETA: 8s - loss: 1.4207
----- stdout -----
18432/32022 [================>.............] - ETA: 7s - loss: 1.4202
----- stdout -----
19456/32022 [=================>............] - ETA: 7s - loss: 1.4206
----- stdout -----
20480/32022 [==================>...........] - ETA: 6s - loss: 1.4209
----- stdout -----
21504/32022 [===================>..........] - ETA: 6s - loss: 1.4204
----- stdout -----
22528/32022 [====================>.........] - ETA: 5s - loss: 1.4204
----- stdout -----
23552/32022 [=====================>........] - ETA: 4s - loss: 1.4187
----- stdout -----
24576/32022 [======================>.......] - ETA: 4s - loss: 1.4200
----- stdout -----
25600/32022 [======================>.......] - ETA: 3s - loss: 1.4195
----- stdout -----
26624/32022 [=======================>......] - ETA: 3s - loss: 1.4197
----- stdout -----
27648/32022 [========================>.....] - ETA: 2s - loss: 1.4184
----- stdout -----
28672/32022 [=========================>....] - ETA: 1s - loss: 1.4175
----- stdout -----
29696/32022 [==========================>...] - ETA: 1s - loss: 1.4162
----- stdout -----
30720/32022 [===========================>..] - ETA: 0s - loss: 1.4170
----- stdout -----
31744/32022 [============================>.] - ETA: 0s - loss: 1.4165
----- stdout -----
32022/32022 [==============================] - ETA: 0s - loss: 1.4161
----- stdout -----

Epoch 28: val_loss improved from 1.35489 to 1.35400, saving model to ./checkpoints/unknown_person_few_shot_baseline_p1_30.h5
----- stdout -----
32022/32022 [==============================] - 20s 623us/sample - loss: 1.4161 - val_loss: 1.3540
----- stdout -----
Epoch 29/50
----- stdout -----
 1024/32022 [..............................] - ETA: 17s - loss: 1.3822
----- stdout -----
 2048/32022 [>.............................] - ETA: 16s - loss: 1.4040
----- stdout -----
 3072/32022 [=>............................] - ETA: 16s - loss: 1.4085
----- stdout -----
 4096/32022 [==>...........................] - ETA: 15s - loss: 1.4128
----- stdout -----
 5120/32022 [===>..........................] - ETA: 15s - loss: 1.4087
----- stdout -----
 6144/32022 [====>.........................] - ETA: 14s - loss: 1.4111
----- stdout -----
 7168/32022 [=====>........................] - ETA: 14s - loss: 1.4159
----- stdout -----
 8192/32022 [======>.......................] - ETA: 13s - loss: 1.4212
----- stdout -----
 9216/32022 [=======>......................] - ETA: 12s - loss: 1.4153
----- stdout -----
10240/32022 [========>.....................] - ETA: 12s - loss: 1.4133
----- stdout -----
11264/32022 [=========>....................] - ETA: 11s - loss: 1.4116
----- stdout -----
12288/32022 [==========>...................] - ETA: 11s - loss: 1.4146
----- stdout -----
13312/32022 [===========>..................] - ETA: 10s - loss: 1.4115
----- stdout -----
14336/32022 [============>.................] - ETA: 10s - loss: 1.4120
----- stdout -----
15360/32022 [=============>................] - ETA: 9s - loss: 1.4128
----- stdout -----
16384/32022 [==============>...............] - ETA: 8s - loss: 1.4132
----- stdout -----
17408/32022 [===============>..............] - ETA: 8s - loss: 1.4131
----- stdout -----
18432/32022 [================>.............] - ETA: 7s - loss: 1.4132
----- stdout -----
19456/32022 [=================>............] - ETA: 7s - loss: 1.4155
----- stdout -----
20480/32022 [==================>...........] - ETA: 6s - loss: 1.4154
----- stdout -----
21504/32022 [===================>..........] - ETA: 6s - loss: 1.4168
----- stdout -----
22528/32022 [====================>.........] - ETA: 5s - loss: 1.4176
----- stdout -----
23552/32022 [=====================>........] - ETA: 4s - loss: 1.4162
----- stdout -----
24576/32022 [======================>.......] - ETA: 4s - loss: 1.4146
----- stdout -----
25600/32022 [======================>.......] - ETA: 3s - loss: 1.4151
----- stdout -----
26624/32022 [=======================>......] - ETA: 3s - loss: 1.4167
----- stdout -----
27648/32022 [========================>.....] - ETA: 2s - loss: 1.4163
----- stdout -----
28672/32022 [=========================>....] - ETA: 1s - loss: 1.4163
----- stdout -----
29696/32022 [==========================>...] - ETA: 1s - loss: 1.4155
----- stdout -----
30720/32022 [===========================>..] - ETA: 0s - loss: 1.4150
----- stdout -----
31744/32022 [============================>.] - ETA: 0s - loss: 1.4151
----- stdout -----
32022/32022 [==============================] - ETA: 0s - loss: 1.4159
----- stdout -----

Epoch 29: val_loss improved from 1.35400 to 1.35368, saving model to ./checkpoints/unknown_person_few_shot_baseline_p1_30.h5
----- stdout -----
32022/32022 [==============================] - 20s 617us/sample - loss: 1.4159 - val_loss: 1.3537
----- stdout -----
Epoch 30/50
----- stdout -----
 1024/32022 [..............................] - ETA: 17s - loss: 1.4361
----- stdout -----
 2048/32022 [>.............................] - ETA: 16s - loss: 1.4319
----- stdout -----
 3072/32022 [=>............................] - ETA: 16s - loss: 1.4326
----- stdout -----
 4096/32022 [==>...........................] - ETA: 15s - loss: 1.4273
----- stdout -----
 5120/32022 [===>..........................] - ETA: 15s - loss: 1.4249
----- stdout -----
 6144/32022 [====>.........................] - ETA: 14s - loss: 1.4221
----- stdout -----
 7168/32022 [=====>........................] - ETA: 14s - loss: 1.4187
----- stdout -----
 8192/32022 [======>.......................] - ETA: 13s - loss: 1.4247
----- stdout -----
 9216/32022 [=======>......................] - ETA: 12s - loss: 1.4225
----- stdout -----
10240/32022 [========>.....................] - ETA: 12s - loss: 1.4177
----- stdout -----
11264/32022 [=========>....................] - ETA: 11s - loss: 1.4147
----- stdout -----
12288/32022 [==========>...................] - ETA: 11s - loss: 1.4160
----- stdout -----
13312/32022 [===========>..................] - ETA: 10s - loss: 1.4159
----- stdout -----
14336/32022 [============>.................] - ETA: 10s - loss: 1.4172
----- stdout -----
15360/32022 [=============>................] - ETA: 9s - loss: 1.4166
----- stdout -----
16384/32022 [==============>...............] - ETA: 8s - loss: 1.4172
----- stdout -----
17408/32022 [===============>..............] - ETA: 8s - loss: 1.4165
----- stdout -----
18432/32022 [================>.............] - ETA: 7s - loss: 1.4178
----- stdout -----
19456/32022 [=================>............] - ETA: 7s - loss: 1.4165
----- stdout -----
20480/32022 [==================>...........] - ETA: 6s - loss: 1.4166
----- stdout -----
21504/32022 [===================>..........] - ETA: 6s - loss: 1.4175
----- stdout -----
22528/32022 [====================>.........] - ETA: 5s - loss: 1.4166
----- stdout -----
23552/32022 [=====================>........] - ETA: 4s - loss: 1.4160
----- stdout -----
24576/32022 [======================>.......] - ETA: 4s - loss: 1.4166
----- stdout -----
25600/32022 [======================>.......] - ETA: 3s - loss: 1.4130
----- stdout -----
26624/32022 [=======================>......] - ETA: 3s - loss: 1.4121
----- stdout -----
27648/32022 [========================>.....] - ETA: 2s - loss: 1.4138
----- stdout -----
28672/32022 [=========================>....] - ETA: 1s - loss: 1.4134
----- stdout -----
29696/32022 [==========================>...] - ETA: 1s - loss: 1.4137
----- stdout -----
30720/32022 [===========================>..] - ETA: 0s - loss: 1.4152
----- stdout -----
31744/32022 [============================>.] - ETA: 0s - loss: 1.4143
----- stdout -----
32022/32022 [==============================] - ETA: 0s - loss: 1.4144
----- stdout -----

Epoch 30: val_loss did not improve from 1.35368
----- stdout -----
32022/32022 [==============================] - 20s 614us/sample - loss: 1.4144 - val_loss: 1.3538
----- stdout -----
Epoch 31/50
----- stdout -----
 1024/32022 [..............................] - ETA: 17s - loss: 1.4555
----- stdout -----
 2048/32022 [>.............................] - ETA: 17s - loss: 1.4166
----- stdout -----
 3072/32022 [=>............................] - ETA: 16s - loss: 1.4113
----- stdout -----
 4096/32022 [==>...........................] - ETA: 16s - loss: 1.4222
----- stdout -----
 5120/32022 [===>..........................] - ETA: 15s - loss: 1.4274
----- stdout -----
 6144/32022 [====>.........................] - ETA: 14s - loss: 1.4310
----- stdout -----
 7168/32022 [=====>........................] - ETA: 14s - loss: 1.4240
----- stdout -----
 8192/32022 [======>.......................] - ETA: 14s - loss: 1.4132
----- stdout -----
 9216/32022 [=======>......................] - ETA: 13s - loss: 1.4097
----- stdout -----
10240/32022 [========>.....................] - ETA: 13s - loss: 1.4114
----- stdout -----
11264/32022 [=========>....................] - ETA: 12s - loss: 1.4112
----- stdout -----
12288/32022 [==========>...................] - ETA: 12s - loss: 1.4112
----- stdout -----
13312/32022 [===========>..................] - ETA: 11s - loss: 1.4112
----- stdout -----
14336/32022 [============>.................] - ETA: 11s - loss: 1.4106
----- stdout -----
15360/32022 [=============>................] - ETA: 10s - loss: 1.4119
----- stdout -----
16384/32022 [==============>...............] - ETA: 9s - loss: 1.4177
----- stdout -----
17408/32022 [===============>..............] - ETA: 9s - loss: 1.4171
----- stdout -----
18432/32022 [================>.............] - ETA: 8s - loss: 1.4175
----- stdout -----
19456/32022 [=================>............] - ETA: 8s - loss: 1.4154
----- stdout -----
20480/32022 [==================>...........] - ETA: 7s - loss: 1.4155
----- stdout -----
21504/32022 [===================>..........] - ETA: 6s - loss: 1.4146
----- stdout -----
22528/32022 [====================>.........] - ETA: 6s - loss: 1.4137
----- stdout -----
23552/32022 [=====================>........] - ETA: 5s - loss: 1.4136
----- stdout -----
24576/32022 [======================>.......] - ETA: 4s - loss: 1.4122
----- stdout -----
25600/32022 [======================>.......] - ETA: 4s - loss: 1.4132
----- stdout -----
26624/32022 [=======================>......] - ETA: 3s - loss: 1.4112
----- stdout -----
27648/32022 [========================>.....] - ETA: 2s - loss: 1.4125
----- stdout -----
28672/32022 [=========================>....] - ETA: 2s - loss: 1.4114
----- stdout -----
29696/32022 [==========================>...] - ETA: 1s - loss: 1.4126
----- stdout -----
30720/32022 [===========================>..] - ETA: 0s - loss: 1.4126
----- stdout -----
31744/32022 [============================>.] - ETA: 0s - loss: 1.4128
----- stdout -----
32022/32022 [==============================] - ETA: 0s - loss: 1.4131
----- stdout -----

Epoch 31: val_loss did not improve from 1.35368
----- stdout -----
32022/32022 [==============================] - 22s 697us/sample - loss: 1.4131 - val_loss: 1.3550
----- stdout -----
Epoch 32/50
----- stdout -----
 1024/32022 [..............................] - ETA: 20s - loss: 1.4374
----- stdout -----
 2048/32022 [>.............................] - ETA: 19s - loss: 1.4354
----- stdout -----
 3072/32022 [=>............................] - ETA: 19s - loss: 1.4355
----- stdout -----
 4096/32022 [==>...........................] - ETA: 18s - loss: 1.4341
----- stdout -----
 5120/32022 [===>..........................] - ETA: 17s - loss: 1.4358
----- stdout -----
 6144/32022 [====>.........................] - ETA: 17s - loss: 1.4309
----- stdout -----
 7168/32022 [=====>........................] - ETA: 16s - loss: 1.4255
----- stdout -----
 8192/32022 [======>.......................] - ETA: 15s - loss: 1.4245
----- stdout -----
 9216/32022 [=======>......................] - ETA: 15s - loss: 1.4224
----- stdout -----
10240/32022 [========>.....................] - ETA: 14s - loss: 1.4205
----- stdout -----
11264/32022 [=========>....................] - ETA: 13s - loss: 1.4214
----- stdout -----
12288/32022 [==========>...................] - ETA: 13s - loss: 1.4212
----- stdout -----
13312/32022 [===========>..................] - ETA: 12s - loss: 1.4168
----- stdout -----
14336/32022 [============>.................] - ETA: 11s - loss: 1.4187
----- stdout -----
15360/32022 [=============>................] - ETA: 11s - loss: 1.4179
----- stdout -----
16384/32022 [==============>...............] - ETA: 10s - loss: 1.4204
----- stdout -----
17408/32022 [===============>..............] - ETA: 9s - loss: 1.4203
----- stdout -----
18432/32022 [================>.............] - ETA: 9s - loss: 1.4190
----- stdout -----
19456/32022 [=================>............] - ETA: 8s - loss: 1.4185
----- stdout -----
20480/32022 [==================>...........] - ETA: 7s - loss: 1.4162
----- stdout -----
21504/32022 [===================>..........] - ETA: 6s - loss: 1.4159
----- stdout -----
22528/32022 [====================>.........] - ETA: 6s - loss: 1.4162
----- stdout -----
23552/32022 [=====================>........] - ETA: 5s - loss: 1.4172
----- stdout -----
24576/32022 [======================>.......] - ETA: 4s - loss: 1.4167
----- stdout -----
25600/32022 [======================>.......] - ETA: 4s - loss: 1.4158
----- stdout -----
26624/32022 [=======================>......] - ETA: 3s - loss: 1.4151
----- stdout -----
27648/32022 [========================>.....] - ETA: 2s - loss: 1.4139
----- stdout -----
28672/32022 [=========================>....] - ETA: 2s - loss: 1.4145
----- stdout -----
29696/32022 [==========================>...] - ETA: 1s - loss: 1.4149
----- stdout -----
30720/32022 [===========================>..] - ETA: 0s - loss: 1.4145
----- stdout -----
31744/32022 [============================>.] - ETA: 0s - loss: 1.4137
----- stdout -----
32022/32022 [==============================] - ETA: 0s - loss: 1.4128
----- stdout -----

Epoch 32: val_loss improved from 1.35368 to 1.34406, saving model to ./checkpoints/unknown_person_few_shot_baseline_p1_30.h5
----- stdout -----
32022/32022 [==============================] - 23s 715us/sample - loss: 1.4128 - val_loss: 1.3441
----- stdout -----
Epoch 33/50
----- stdout -----
 1024/32022 [..............................] - ETA: 20s - loss: 1.4410
----- stdout -----
 2048/32022 [>.............................] - ETA: 19s - loss: 1.4293
----- stdout -----
 3072/32022 [=>............................] - ETA: 19s - loss: 1.4468
----- stdout -----
 4096/32022 [==>...........................] - ETA: 18s - loss: 1.4377
----- stdout -----
 5120/32022 [===>..........................] - ETA: 17s - loss: 1.4279
----- stdout -----
 6144/32022 [====>.........................] - ETA: 17s - loss: 1.4211
----- stdout -----
 7168/32022 [=====>........................] - ETA: 16s - loss: 1.4176
----- stdout -----
 8192/32022 [======>.......................] - ETA: 15s - loss: 1.4181
----- stdout -----
 9216/32022 [=======>......................] - ETA: 15s - loss: 1.4171
----- stdout -----
10240/32022 [========>.....................] - ETA: 14s - loss: 1.4155
----- stdout -----
11264/32022 [=========>....................] - ETA: 13s - loss: 1.4161
----- stdout -----
12288/32022 [==========>...................] - ETA: 13s - loss: 1.4165
----- stdout -----
13312/32022 [===========>..................] - ETA: 12s - loss: 1.4129
----- stdout -----
14336/32022 [============>.................] - ETA: 11s - loss: 1.4159
----- stdout -----
15360/32022 [=============>................] - ETA: 11s - loss: 1.4150
----- stdout -----
16384/32022 [==============>...............] - ETA: 10s - loss: 1.4136
----- stdout -----
17408/32022 [===============>..............] - ETA: 9s - loss: 1.4110
----- stdout -----
18432/32022 [================>.............] - ETA: 9s - loss: 1.4116
----- stdout -----
19456/32022 [=================>............] - ETA: 8s - loss: 1.4134
----- stdout -----
20480/32022 [==================>...........] - ETA: 7s - loss: 1.4117
----- stdout -----
21504/32022 [===================>..........] - ETA: 7s - loss: 1.4122
----- stdout -----
22528/32022 [====================>.........] - ETA: 6s - loss: 1.4099
----- stdout -----
23552/32022 [=====================>........] - ETA: 5s - loss: 1.4101
----- stdout -----
24576/32022 [======================>.......] - ETA: 4s - loss: 1.4119
----- stdout -----
25600/32022 [======================>.......] - ETA: 4s - loss: 1.4106
----- stdout -----
26624/32022 [=======================>......] - ETA: 3s - loss: 1.4091
----- stdout -----
27648/32022 [========================>.....] - ETA: 2s - loss: 1.4093
----- stdout -----
28672/32022 [=========================>....] - ETA: 2s - loss: 1.4095
----- stdout -----
29696/32022 [==========================>...] - ETA: 1s - loss: 1.4111
----- stdout -----
30720/32022 [===========================>..] - ETA: 0s - loss: 1.4109
----- stdout -----
31744/32022 [============================>.] - ETA: 0s - loss: 1.4094
----- stdout -----
32022/32022 [==============================] - ETA: 0s - loss: 1.4101
----- stdout -----

Epoch 33: val_loss did not improve from 1.34406
----- stdout -----
32022/32022 [==============================] - 22s 680us/sample - loss: 1.4101 - val_loss: 1.3472
----- stdout -----
Epoch 34/50
----- stdout -----
 1024/32022 [..............................] - ETA: 17s - loss: 1.4025
----- stdout -----
 2048/32022 [>.............................] - ETA: 16s - loss: 1.3919
----- stdout -----
 3072/32022 [=>............................] - ETA: 16s - loss: 1.3907
----- stdout -----
 4096/32022 [==>...........................] - ETA: 15s - loss: 1.3966
----- stdout -----
 5120/32022 [===>..........................] - ETA: 15s - loss: 1.3952
----- stdout -----
 6144/32022 [====>.........................] - ETA: 15s - loss: 1.3952
----- stdout -----
 7168/32022 [=====>........................] - ETA: 15s - loss: 1.4010
----- stdout -----
 8192/32022 [======>.......................] - ETA: 14s - loss: 1.3990
----- stdout -----
 9216/32022 [=======>......................] - ETA: 14s - loss: 1.3965
----- stdout -----
10240/32022 [========>.....................] - ETA: 13s - loss: 1.3927
----- stdout -----
11264/32022 [=========>....................] - ETA: 13s - loss: 1.3985
----- stdout -----
12288/32022 [==========>...................] - ETA: 12s - loss: 1.3987
----- stdout -----
13312/32022 [===========>..................] - ETA: 11s - loss: 1.4019
----- stdout -----
14336/32022 [============>.................] - ETA: 11s - loss: 1.4015
----- stdout -----
15360/32022 [=============>................] - ETA: 10s - loss: 1.4020
----- stdout -----
16384/32022 [==============>...............] - ETA: 9s - loss: 1.4024
----- stdout -----
17408/32022 [===============>..............] - ETA: 9s - loss: 1.4023
----- stdout -----
18432/32022 [================>.............] - ETA: 8s - loss: 1.4012
----- stdout -----
19456/32022 [=================>............] - ETA: 7s - loss: 1.4050
----- stdout -----
20480/32022 [==================>...........] - ETA: 7s - loss: 1.4040
----- stdout -----
21504/32022 [===================>..........] - ETA: 6s - loss: 1.4041
----- stdout -----
22528/32022 [====================>.........] - ETA: 5s - loss: 1.4062
----- stdout -----
23552/32022 [=====================>........] - ETA: 5s - loss: 1.4037
----- stdout -----
24576/32022 [======================>.......] - ETA: 4s - loss: 1.4019
----- stdout -----
25600/32022 [======================>.......] - ETA: 3s - loss: 1.4030
----- stdout -----
26624/32022 [=======================>......] - ETA: 3s - loss: 1.4045
----- stdout -----
27648/32022 [========================>.....] - ETA: 2s - loss: 1.4055
----- stdout -----
28672/32022 [=========================>....] - ETA: 2s - loss: 1.4034
----- stdout -----
29696/32022 [==========================>...] - ETA: 1s - loss: 1.4050
----- stdout -----
30720/32022 [===========================>..] - ETA: 0s - loss: 1.4060
----- stdout -----
31744/32022 [============================>.] - ETA: 0s - loss: 1.4056
----- stdout -----
32022/32022 [==============================] - ETA: 0s - loss: 1.4062
----- stdout -----

Epoch 34: val_loss improved from 1.34406 to 1.34305, saving model to ./checkpoints/unknown_person_few_shot_baseline_p1_30.h5
----- stdout -----
32022/32022 [==============================] - 21s 641us/sample - loss: 1.4062 - val_loss: 1.3430
----- stdout -----
Epoch 35/50
----- stdout -----
 1024/32022 [..............................] - ETA: 17s - loss: 1.3890
----- stdout -----
 2048/32022 [>.............................] - ETA: 17s - loss: 1.4065
----- stdout -----
 3072/32022 [=>............................] - ETA: 16s - loss: 1.4124
----- stdout -----
 4096/32022 [==>...........................] - ETA: 15s - loss: 1.4104
----- stdout -----
 5120/32022 [===>..........................] - ETA: 15s - loss: 1.4005
----- stdout -----
 6144/32022 [====>.........................] - ETA: 14s - loss: 1.4091
----- stdout -----
 7168/32022 [=====>........................] - ETA: 14s - loss: 1.4164
----- stdout -----
 8192/32022 [======>.......................] - ETA: 13s - loss: 1.4157
----- stdout -----
 9216/32022 [=======>......................] - ETA: 12s - loss: 1.4148
----- stdout -----
10240/32022 [========>.....................] - ETA: 12s - loss: 1.4143
----- stdout -----
11264/32022 [=========>....................] - ETA: 11s - loss: 1.4137
----- stdout -----
12288/32022 [==========>...................] - ETA: 11s - loss: 1.4122
----- stdout -----
13312/32022 [===========>..................] - ETA: 10s - loss: 1.4109
----- stdout -----
14336/32022 [============>.................] - ETA: 10s - loss: 1.4119
----- stdout -----
15360/32022 [=============>................] - ETA: 9s - loss: 1.4117
----- stdout -----
16384/32022 [==============>...............] - ETA: 8s - loss: 1.4127
----- stdout -----
17408/32022 [===============>..............] - ETA: 8s - loss: 1.4105
----- stdout -----
18432/32022 [================>.............] - ETA: 7s - loss: 1.4073
----- stdout -----
19456/32022 [=================>............] - ETA: 7s - loss: 1.4094
----- stdout -----
20480/32022 [==================>...........] - ETA: 6s - loss: 1.4084
----- stdout -----
21504/32022 [===================>..........] - ETA: 6s - loss: 1.4078
----- stdout -----
22528/32022 [====================>.........] - ETA: 5s - loss: 1.4064
----- stdout -----
23552/32022 [=====================>........] - ETA: 5s - loss: 1.4054
----- stdout -----
24576/32022 [======================>.......] - ETA: 4s - loss: 1.4060
----- stdout -----
25600/32022 [======================>.......] - ETA: 3s - loss: 1.4051
----- stdout -----
26624/32022 [=======================>......] - ETA: 3s - loss: 1.4052
----- stdout -----
27648/32022 [========================>.....] - ETA: 2s - loss: 1.4047
----- stdout -----
28672/32022 [=========================>....] - ETA: 2s - loss: 1.4038
----- stdout -----
29696/32022 [==========================>...] - ETA: 1s - loss: 1.4030
----- stdout -----
30720/32022 [===========================>..] - ETA: 0s - loss: 1.4033
----- stdout -----
31744/32022 [============================>.] - ETA: 0s - loss: 1.4039
----- stdout -----
32022/32022 [==============================] - ETA: 0s - loss: 1.4041
----- stdout -----

Epoch 35: val_loss improved from 1.34305 to 1.33871, saving model to ./checkpoints/unknown_person_few_shot_baseline_p1_30.h5
----- stdout -----
32022/32022 [==============================] - 21s 666us/sample - loss: 1.4041 - val_loss: 1.3387
----- stdout -----
Epoch 36/50
----- stdout -----
 1024/32022 [..............................] - ETA: 20s - loss: 1.4082
----- stdout -----
 2048/32022 [>.............................] - ETA: 19s - loss: 1.3960
----- stdout -----
 3072/32022 [=>............................] - ETA: 19s - loss: 1.4057
----- stdout -----
 4096/32022 [==>...........................] - ETA: 18s - loss: 1.4098
----- stdout -----
 5120/32022 [===>..........................] - ETA: 17s - loss: 1.4096
----- stdout -----
 6144/32022 [====>.........................] - ETA: 17s - loss: 1.4127
----- stdout -----
 7168/32022 [=====>........................] - ETA: 16s - loss: 1.4082
----- stdout -----
 8192/32022 [======>.......................] - ETA: 15s - loss: 1.4069
----- stdout -----
 9216/32022 [=======>......................] - ETA: 15s - loss: 1.4037
----- stdout -----
10240/32022 [========>.....................] - ETA: 14s - loss: 1.3974
----- stdout -----
11264/32022 [=========>....................] - ETA: 13s - loss: 1.3996
----- stdout -----
12288/32022 [==========>...................] - ETA: 12s - loss: 1.4018
----- stdout -----
13312/32022 [===========>..................] - ETA: 12s - loss: 1.4026
----- stdout -----
14336/32022 [============>.................] - ETA: 11s - loss: 1.4038
----- stdout -----
15360/32022 [=============>................] - ETA: 10s - loss: 1.4036
----- stdout -----
16384/32022 [==============>...............] - ETA: 10s - loss: 1.4023
----- stdout -----
17408/32022 [===============>..............] - ETA: 9s - loss: 1.4018
----- stdout -----
18432/32022 [================>.............] - ETA: 8s - loss: 1.4005
----- stdout -----
19456/32022 [=================>............] - ETA: 8s - loss: 1.4033
----- stdout -----
20480/32022 [==================>...........] - ETA: 7s - loss: 1.4022
----- stdout -----
21504/32022 [===================>..........] - ETA: 6s - loss: 1.4002
----- stdout -----
22528/32022 [====================>.........] - ETA: 6s - loss: 1.4009
----- stdout -----
23552/32022 [=====================>........] - ETA: 5s - loss: 1.4034
----- stdout -----
24576/32022 [======================>.......] - ETA: 4s - loss: 1.4023
----- stdout -----
25600/32022 [======================>.......] - ETA: 4s - loss: 1.4041
----- stdout -----
26624/32022 [=======================>......] - ETA: 3s - loss: 1.4038
----- stdout -----
27648/32022 [========================>.....] - ETA: 2s - loss: 1.4038
----- stdout -----
28672/32022 [=========================>....] - ETA: 2s - loss: 1.4039
----- stdout -----
29696/32022 [==========================>...] - ETA: 1s - loss: 1.4052
----- stdout -----
30720/32022 [===========================>..] - ETA: 0s - loss: 1.4049
----- stdout -----
31744/32022 [============================>.] - ETA: 0s - loss: 1.4043
----- stdout -----
32022/32022 [==============================] - ETA: 0s - loss: 1.4038
----- stdout -----

Epoch 36: val_loss improved from 1.33871 to 1.33704, saving model to ./checkpoints/unknown_person_few_shot_baseline_p1_30.h5
----- stdout -----
32022/32022 [==============================] - 23s 714us/sample - loss: 1.4038 - val_loss: 1.3370
----- stdout -----
Epoch 37/50
----- stdout -----
 1024/32022 [..............................] - ETA: 20s - loss: 1.4131
----- stdout -----
 2048/32022 [>.............................] - ETA: 20s - loss: 1.4003
----- stdout -----
 3072/32022 [=>............................] - ETA: 19s - loss: 1.4006
----- stdout -----
 4096/32022 [==>...........................] - ETA: 18s - loss: 1.4028
----- stdout -----
 5120/32022 [===>..........................] - ETA: 18s - loss: 1.4077
----- stdout -----
 6144/32022 [====>.........................] - ETA: 17s - loss: 1.4085
----- stdout -----
 7168/32022 [=====>........................] - ETA: 16s - loss: 1.4041
----- stdout -----
 8192/32022 [======>.......................] - ETA: 16s - loss: 1.4047
----- stdout -----
 9216/32022 [=======>......................] - ETA: 15s - loss: 1.4049
----- stdout -----
10240/32022 [========>.....................] - ETA: 14s - loss: 1.4075
----- stdout -----
11264/32022 [=========>....................] - ETA: 13s - loss: 1.4090
----- stdout -----
12288/32022 [==========>...................] - ETA: 13s - loss: 1.4104
----- stdout -----
13312/32022 [===========>..................] - ETA: 12s - loss: 1.4076
----- stdout -----
14336/32022 [============>.................] - ETA: 11s - loss: 1.4065
----- stdout -----
15360/32022 [=============>................] - ETA: 11s - loss: 1.4055
----- stdout -----
16384/32022 [==============>...............] - ETA: 10s - loss: 1.4086
----- stdout -----
17408/32022 [===============>..............] - ETA: 9s - loss: 1.4082
----- stdout -----
18432/32022 [================>.............] - ETA: 9s - loss: 1.4106
----- stdout -----
19456/32022 [=================>............] - ETA: 8s - loss: 1.4075
----- stdout -----
20480/32022 [==================>...........] - ETA: 7s - loss: 1.4040
----- stdout -----
21504/32022 [===================>..........] - ETA: 7s - loss: 1.4037
----- stdout -----
22528/32022 [====================>.........] - ETA: 6s - loss: 1.4049
----- stdout -----
23552/32022 [=====================>........] - ETA: 5s - loss: 1.4056
----- stdout -----
24576/32022 [======================>.......] - ETA: 4s - loss: 1.4057
----- stdout -----
25600/32022 [======================>.......] - ETA: 4s - loss: 1.4055
----- stdout -----
26624/32022 [=======================>......] - ETA: 3s - loss: 1.4057
----- stdout -----
27648/32022 [========================>.....] - ETA: 2s - loss: 1.4050
----- stdout -----
28672/32022 [=========================>....] - ETA: 2s - loss: 1.4040
----- stdout -----
29696/32022 [==========================>...] - ETA: 1s - loss: 1.4038
----- stdout -----
30720/32022 [===========================>..] - ETA: 0s - loss: 1.4036
----- stdout -----
31744/32022 [============================>.] - ETA: 0s - loss: 1.4037
----- stdout -----
32022/32022 [==============================] - ETA: 0s - loss: 1.4041
----- stdout -----

Epoch 37: val_loss did not improve from 1.33704
----- stdout -----
32022/32022 [==============================] - 23s 712us/sample - loss: 1.4041 - val_loss: 1.3377
----- stdout -----
Epoch 38/50
----- stdout -----
 1024/32022 [..............................] - ETA: 20s - loss: 1.3883
----- stdout -----
 2048/32022 [>.............................] - ETA: 19s - loss: 1.3773
----- stdout -----
 3072/32022 [=>............................] - ETA: 19s - loss: 1.3852
----- stdout -----
 4096/32022 [==>...........................] - ETA: 18s - loss: 1.3961
----- stdout -----
 5120/32022 [===>..........................] - ETA: 17s - loss: 1.3985
----- stdout -----
 6144/32022 [====>.........................] - ETA: 17s - loss: 1.4035
----- stdout -----
 7168/32022 [=====>........................] - ETA: 16s - loss: 1.4035
----- stdout -----
 8192/32022 [======>.......................] - ETA: 15s - loss: 1.4020
----- stdout -----
 9216/32022 [=======>......................] - ETA: 14s - loss: 1.4027
----- stdout -----
10240/32022 [========>.....................] - ETA: 14s - loss: 1.4031
----- stdout -----
11264/32022 [=========>....................] - ETA: 13s - loss: 1.4045
----- stdout -----
12288/32022 [==========>...................] - ETA: 12s - loss: 1.4036
----- stdout -----
13312/32022 [===========>..................] - ETA: 12s - loss: 1.4042
----- stdout -----
14336/32022 [============>.................] - ETA: 11s - loss: 1.4050
----- stdout -----
15360/32022 [=============>................] - ETA: 10s - loss: 1.4041
----- stdout -----
16384/32022 [==============>...............] - ETA: 10s - loss: 1.4043
----- stdout -----
17408/32022 [===============>..............] - ETA: 9s - loss: 1.4075
----- stdout -----
18432/32022 [================>.............] - ETA: 8s - loss: 1.4070
----- stdout -----
19456/32022 [=================>............] - ETA: 8s - loss: 1.4054
----- stdout -----
20480/32022 [==================>...........] - ETA: 7s - loss: 1.4033
----- stdout -----
21504/32022 [===================>..........] - ETA: 6s - loss: 1.4020
----- stdout -----
22528/32022 [====================>.........] - ETA: 6s - loss: 1.4013
----- stdout -----
23552/32022 [=====================>........] - ETA: 5s - loss: 1.4016
----- stdout -----
24576/32022 [======================>.......] - ETA: 4s - loss: 1.4007
----- stdout -----
25600/32022 [======================>.......] - ETA: 4s - loss: 1.3996
----- stdout -----
26624/32022 [=======================>......] - ETA: 3s - loss: 1.3996
----- stdout -----
27648/32022 [========================>.....] - ETA: 2s - loss: 1.4012
----- stdout -----
28672/32022 [=========================>....] - ETA: 2s - loss: 1.4017
----- stdout -----
29696/32022 [==========================>...] - ETA: 1s - loss: 1.4023
----- stdout -----
30720/32022 [===========================>..] - ETA: 0s - loss: 1.4012
----- stdout -----
31744/32022 [============================>.] - ETA: 0s - loss: 1.4000
----- stdout -----
32022/32022 [==============================] - ETA: 0s - loss: 1.3997
----- stdout -----

Epoch 38: val_loss did not improve from 1.33704
----- stdout -----
32022/32022 [==============================] - 23s 708us/sample - loss: 1.3997 - val_loss: 1.3465
----- stdout -----
Epoch 39/50
----- stdout -----
 1024/32022 [..............................] - ETA: 22s - loss: 1.4206
----- stdout -----
 2048/32022 [>.............................] - ETA: 20s - loss: 1.4063
----- stdout -----
 3072/32022 [=>............................] - ETA: 19s - loss: 1.4103
----- stdout -----
 4096/32022 [==>...........................] - ETA: 18s - loss: 1.4103
----- stdout -----
 5120/32022 [===>..........................] - ETA: 18s - loss: 1.4058
----- stdout -----
 6144/32022 [====>.........................] - ETA: 17s - loss: 1.3995
----- stdout -----
 7168/32022 [=====>........................] - ETA: 16s - loss: 1.4018
----- stdout -----
 8192/32022 [======>.......................] - ETA: 15s - loss: 1.4008
----- stdout -----
 9216/32022 [=======>......................] - ETA: 15s - loss: 1.3971
----- stdout -----
10240/32022 [========>.....................] - ETA: 14s - loss: 1.3947
----- stdout -----
11264/32022 [=========>....................] - ETA: 13s - loss: 1.3937
----- stdout -----
12288/32022 [==========>...................] - ETA: 13s - loss: 1.3958
----- stdout -----
13312/32022 [===========>..................] - ETA: 12s - loss: 1.3931
----- stdout -----
14336/32022 [============>.................] - ETA: 11s - loss: 1.3941
----- stdout -----
15360/32022 [=============>................] - ETA: 11s - loss: 1.3992
----- stdout -----
16384/32022 [==============>...............] - ETA: 10s - loss: 1.3982
----- stdout -----
17408/32022 [===============>..............] - ETA: 9s - loss: 1.3980
----- stdout -----
18432/32022 [================>.............] - ETA: 9s - loss: 1.4003
----- stdout -----
19456/32022 [=================>............] - ETA: 8s - loss: 1.4015
----- stdout -----
20480/32022 [==================>...........] - ETA: 7s - loss: 1.4007
----- stdout -----
21504/32022 [===================>..........] - ETA: 7s - loss: 1.4005
----- stdout -----
22528/32022 [====================>.........] - ETA: 6s - loss: 1.3998
----- stdout -----
23552/32022 [=====================>........] - ETA: 5s - loss: 1.3989
----- stdout -----
24576/32022 [======================>.......] - ETA: 4s - loss: 1.3976
----- stdout -----
25600/32022 [======================>.......] - ETA: 4s - loss: 1.3995
----- stdout -----
26624/32022 [=======================>......] - ETA: 3s - loss: 1.4001
----- stdout -----
27648/32022 [========================>.....] - ETA: 2s - loss: 1.4000
----- stdout -----
28672/32022 [=========================>....] - ETA: 2s - loss: 1.4016
----- stdout -----
29696/32022 [==========================>...] - ETA: 1s - loss: 1.4009
----- stdout -----
30720/32022 [===========================>..] - ETA: 0s - loss: 1.4007
----- stdout -----
31744/32022 [============================>.] - ETA: 0s - loss: 1.3990
----- stdout -----
32022/32022 [==============================] - ETA: 0s - loss: 1.3993
----- stdout -----

Epoch 39: val_loss improved from 1.33704 to 1.33567, saving model to ./checkpoints/unknown_person_few_shot_baseline_p1_30.h5
----- stdout -----
32022/32022 [==============================] - 23s 718us/sample - loss: 1.3993 - val_loss: 1.3357
----- stdout -----
Epoch 40/50
----- stdout -----
 1024/32022 [..............................] - ETA: 20s - loss: 1.3933
----- stdout -----
 2048/32022 [>.............................] - ETA: 19s - loss: 1.4137
----- stdout -----
 3072/32022 [=>............................] - ETA: 18s - loss: 1.4199
----- stdout -----
 4096/32022 [==>...........................] - ETA: 18s - loss: 1.4198
----- stdout -----
 5120/32022 [===>..........................] - ETA: 17s - loss: 1.4158
----- stdout -----
 6144/32022 [====>.........................] - ETA: 16s - loss: 1.4092
----- stdout -----
 7168/32022 [=====>........................] - ETA: 16s - loss: 1.4054
----- stdout -----
 8192/32022 [======>.......................] - ETA: 15s - loss: 1.4043
----- stdout -----
 9216/32022 [=======>......................] - ETA: 15s - loss: 1.4038
----- stdout -----
10240/32022 [========>.....................] - ETA: 14s - loss: 1.4022
----- stdout -----
11264/32022 [=========>....................] - ETA: 13s - loss: 1.4032
----- stdout -----
12288/32022 [==========>...................] - ETA: 13s - loss: 1.4078
----- stdout -----
13312/32022 [===========>..................] - ETA: 12s - loss: 1.4085
----- stdout -----
14336/32022 [============>.................] - ETA: 11s - loss: 1.4071
----- stdout -----
15360/32022 [=============>................] - ETA: 11s - loss: 1.4066
----- stdout -----
16384/32022 [==============>...............] - ETA: 10s - loss: 1.4047
----- stdout -----
17408/32022 [===============>..............] - ETA: 9s - loss: 1.4043
----- stdout -----
18432/32022 [================>.............] - ETA: 8s - loss: 1.4042
----- stdout -----
19456/32022 [=================>............] - ETA: 8s - loss: 1.4027
----- stdout -----
20480/32022 [==================>...........] - ETA: 7s - loss: 1.4039
----- stdout -----
21504/32022 [===================>..........] - ETA: 6s - loss: 1.4036
----- stdout -----
22528/32022 [====================>.........] - ETA: 6s - loss: 1.4047
----- stdout -----
23552/32022 [=====================>........] - ETA: 5s - loss: 1.4046
----- stdout -----
24576/32022 [======================>.......] - ETA: 4s - loss: 1.4038
----- stdout -----
25600/32022 [======================>.......] - ETA: 4s - loss: 1.4028
----- stdout -----
26624/32022 [=======================>......] - ETA: 3s - loss: 1.4021
----- stdout -----
27648/32022 [========================>.....] - ETA: 2s - loss: 1.4008
----- stdout -----
28672/32022 [=========================>....] - ETA: 2s - loss: 1.4012
----- stdout -----
29696/32022 [==========================>...] - ETA: 1s - loss: 1.4013
----- stdout -----
30720/32022 [===========================>..] - ETA: 0s - loss: 1.4008
----- stdout -----
31744/32022 [============================>.] - ETA: 0s - loss: 1.3993
----- stdout -----
32022/32022 [==============================] - ETA: 0s - loss: 1.3996
----- stdout -----

Epoch 40: val_loss did not improve from 1.33567
----- stdout -----
32022/32022 [==============================] - 23s 710us/sample - loss: 1.3996 - val_loss: 1.3369
----- stdout -----
Epoch 41/50
----- stdout -----
 1024/32022 [..............................] - ETA: 20s - loss: 1.4269
----- stdout -----
 2048/32022 [>.............................] - ETA: 20s - loss: 1.4118
----- stdout -----
 3072/32022 [=>............................] - ETA: 19s - loss: 1.4049
----- stdout -----
 4096/32022 [==>...........................] - ETA: 18s - loss: 1.4013
----- stdout -----
 5120/32022 [===>..........................] - ETA: 17s - loss: 1.3965
----- stdout -----
 6144/32022 [====>.........................] - ETA: 17s - loss: 1.3979
----- stdout -----
 7168/32022 [=====>........................] - ETA: 16s - loss: 1.3937
----- stdout -----
 8192/32022 [======>.......................] - ETA: 15s - loss: 1.3982
----- stdout -----
 9216/32022 [=======>......................] - ETA: 15s - loss: 1.4022
----- stdout -----
10240/32022 [========>.....................] - ETA: 14s - loss: 1.4009
----- stdout -----
11264/32022 [=========>....................] - ETA: 13s - loss: 1.3983
----- stdout -----
12288/32022 [==========>...................] - ETA: 13s - loss: 1.3939
----- stdout -----
13312/32022 [===========>..................] - ETA: 12s - loss: 1.3909
----- stdout -----
14336/32022 [============>.................] - ETA: 11s - loss: 1.3912
----- stdout -----
15360/32022 [=============>................] - ETA: 11s - loss: 1.3936
----- stdout -----
16384/32022 [==============>...............] - ETA: 10s - loss: 1.3958
----- stdout -----
17408/32022 [===============>..............] - ETA: 9s - loss: 1.3958
----- stdout -----
18432/32022 [================>.............] - ETA: 9s - loss: 1.3945
----- stdout -----
19456/32022 [=================>............] - ETA: 8s - loss: 1.3955
----- stdout -----
20480/32022 [==================>...........] - ETA: 7s - loss: 1.3971
----- stdout -----
21504/32022 [===================>..........] - ETA: 7s - loss: 1.3967
----- stdout -----
22528/32022 [====================>.........] - ETA: 6s - loss: 1.3966
----- stdout -----
23552/32022 [=====================>........] - ETA: 5s - loss: 1.3961
----- stdout -----
24576/32022 [======================>.......] - ETA: 4s - loss: 1.3972
----- stdout -----
25600/32022 [======================>.......] - ETA: 4s - loss: 1.3980
----- stdout -----
26624/32022 [=======================>......] - ETA: 3s - loss: 1.3980
----- stdout -----
27648/32022 [========================>.....] - ETA: 2s - loss: 1.3964
----- stdout -----
28672/32022 [=========================>....] - ETA: 2s - loss: 1.3972
----- stdout -----
29696/32022 [==========================>...] - ETA: 1s - loss: 1.3975
----- stdout -----
30720/32022 [===========================>..] - ETA: 0s - loss: 1.3966
----- stdout -----
31744/32022 [============================>.] - ETA: 0s - loss: 1.3956
----- stdout -----
32022/32022 [==============================] - ETA: 0s - loss: 1.3952
----- stdout -----

Epoch 41: val_loss did not improve from 1.33567
----- stdout -----
32022/32022 [==============================] - 23s 715us/sample - loss: 1.3952 - val_loss: 1.3456
----- stdout -----
Epoch 42/50
----- stdout -----
 1024/32022 [..............................] - ETA: 20s - loss: 1.3823
----- stdout -----
 2048/32022 [>.............................] - ETA: 19s - loss: 1.3940
----- stdout -----
 3072/32022 [=>............................] - ETA: 18s - loss: 1.3901
----- stdout -----
 4096/32022 [==>...........................] - ETA: 18s - loss: 1.4016
----- stdout -----
 5120/32022 [===>..........................] - ETA: 17s - loss: 1.3955
----- stdout -----
 6144/32022 [====>.........................] - ETA: 17s - loss: 1.3945
----- stdout -----
 7168/32022 [=====>........................] - ETA: 16s - loss: 1.3954
----- stdout -----
 8192/32022 [======>.......................] - ETA: 15s - loss: 1.3945
----- stdout -----
 9216/32022 [=======>......................] - ETA: 15s - loss: 1.3937
----- stdout -----
10240/32022 [========>.....................] - ETA: 14s - loss: 1.3918
----- stdout -----
11264/32022 [=========>....................] - ETA: 13s - loss: 1.3930
----- stdout -----
12288/32022 [==========>...................] - ETA: 12s - loss: 1.3929
----- stdout -----
13312/32022 [===========>..................] - ETA: 11s - loss: 1.3948
----- stdout -----
14336/32022 [============>.................] - ETA: 11s - loss: 1.3969
----- stdout -----
15360/32022 [=============>................] - ETA: 10s - loss: 1.3992
----- stdout -----
16384/32022 [==============>...............] - ETA: 9s - loss: 1.3968
----- stdout -----
17408/32022 [===============>..............] - ETA: 9s - loss: 1.3954
----- stdout -----
18432/32022 [================>.............] - ETA: 8s - loss: 1.3973
----- stdout -----
19456/32022 [=================>............] - ETA: 7s - loss: 1.3947
----- stdout -----
20480/32022 [==================>...........] - ETA: 7s - loss: 1.3953
----- stdout -----
21504/32022 [===================>..........] - ETA: 6s - loss: 1.3952
----- stdout -----
22528/32022 [====================>.........] - ETA: 5s - loss: 1.3949
----- stdout -----
23552/32022 [=====================>........] - ETA: 5s - loss: 1.3959
----- stdout -----
24576/32022 [======================>.......] - ETA: 4s - loss: 1.3979
----- stdout -----
25600/32022 [======================>.......] - ETA: 3s - loss: 1.3971
----- stdout -----
26624/32022 [=======================>......] - ETA: 3s - loss: 1.3971
----- stdout -----
27648/32022 [========================>.....] - ETA: 2s - loss: 1.3945
----- stdout -----
28672/32022 [=========================>....] - ETA: 2s - loss: 1.3938
----- stdout -----
29696/32022 [==========================>...] - ETA: 1s - loss: 1.3937
----- stdout -----
30720/32022 [===========================>..] - ETA: 0s - loss: 1.3932
----- stdout -----
31744/32022 [============================>.] - ETA: 0s - loss: 1.3933
----- stdout -----
32022/32022 [==============================] - ETA: 0s - loss: 1.3935
----- stdout -----

Epoch 42: val_loss did not improve from 1.33567
----- stdout -----
32022/32022 [==============================] - 21s 659us/sample - loss: 1.3935 - val_loss: 1.3374
----- stdout -----
Epoch 43/50
----- stdout -----
 1024/32022 [..............................] - ETA: 18s - loss: 1.3811
----- stdout -----
 2048/32022 [>.............................] - ETA: 17s - loss: 1.3809
----- stdout -----
 3072/32022 [=>............................] - ETA: 17s - loss: 1.3876
----- stdout -----
 4096/32022 [==>...........................] - ETA: 17s - loss: 1.3938
----- stdout -----
 5120/32022 [===>..........................] - ETA: 16s - loss: 1.3951
----- stdout -----
 6144/32022 [====>.........................] - ETA: 15s - loss: 1.3914
----- stdout -----
 7168/32022 [=====>........................] - ETA: 14s - loss: 1.3874
----- stdout -----
 8192/32022 [======>.......................] - ETA: 14s - loss: 1.3846
----- stdout -----
 9216/32022 [=======>......................] - ETA: 13s - loss: 1.3853
----- stdout -----
10240/32022 [========>.....................] - ETA: 13s - loss: 1.3829
----- stdout -----
11264/32022 [=========>....................] - ETA: 12s - loss: 1.3891
----- stdout -----
12288/32022 [==========>...................] - ETA: 11s - loss: 1.3907
----- stdout -----
13312/32022 [===========>..................] - ETA: 11s - loss: 1.3931
----- stdout -----
14336/32022 [============>.................] - ETA: 10s - loss: 1.3949
----- stdout -----
15360/32022 [=============>................] - ETA: 10s - loss: 1.3946
----- stdout -----
16384/32022 [==============>...............] - ETA: 9s - loss: 1.3932
----- stdout -----
17408/32022 [===============>..............] - ETA: 8s - loss: 1.3923
----- stdout -----
18432/32022 [================>.............] - ETA: 8s - loss: 1.3913
----- stdout -----
19456/32022 [=================>............] - ETA: 7s - loss: 1.3921
----- stdout -----
20480/32022 [==================>...........] - ETA: 6s - loss: 1.3912
----- stdout -----
21504/32022 [===================>..........] - ETA: 6s - loss: 1.3909
----- stdout -----
22528/32022 [====================>.........] - ETA: 5s - loss: 1.3908
----- stdout -----
23552/32022 [=====================>........] - ETA: 5s - loss: 1.3906
----- stdout -----
24576/32022 [======================>.......] - ETA: 4s - loss: 1.3920
----- stdout -----
25600/32022 [======================>.......] - ETA: 3s - loss: 1.3932
----- stdout -----
26624/32022 [=======================>......] - ETA: 3s - loss: 1.3929
----- stdout -----
27648/32022 [========================>.....] - ETA: 2s - loss: 1.3939
----- stdout -----
28672/32022 [=========================>....] - ETA: 2s - loss: 1.3957
----- stdout -----
29696/32022 [==========================>...] - ETA: 1s - loss: 1.3951
----- stdout -----
30720/32022 [===========================>..] - ETA: 0s - loss: 1.3950
----- stdout -----
31744/32022 [============================>.] - ETA: 0s - loss: 1.3946
----- stdout -----
32022/32022 [==============================] - ETA: 0s - loss: 1.3950
----- stdout -----

Epoch 43: val_loss did not improve from 1.33567
----- stdout -----
32022/32022 [==============================] - 21s 653us/sample - loss: 1.3950 - val_loss: 1.3360
----- stdout -----
Epoch 44/50
----- stdout -----
 1024/32022 [..............................] - ETA: 20s - loss: 1.4122
----- stdout -----
 2048/32022 [>.............................] - ETA: 19s - loss: 1.3969
----- stdout -----
 3072/32022 [=>............................] - ETA: 18s - loss: 1.4072
----- stdout -----
 4096/32022 [==>...........................] - ETA: 17s - loss: 1.4177
----- stdout -----
 5120/32022 [===>..........................] - ETA: 16s - loss: 1.4121
----- stdout -----
 6144/32022 [====>.........................] - ETA: 16s - loss: 1.3994
----- stdout -----
 7168/32022 [=====>........................] - ETA: 15s - loss: 1.4008
----- stdout -----
 8192/32022 [======>.......................] - ETA: 14s - loss: 1.3959
----- stdout -----
 9216/32022 [=======>......................] - ETA: 13s - loss: 1.3945
----- stdout -----
10240/32022 [========>.....................] - ETA: 13s - loss: 1.3928
----- stdout -----
11264/32022 [=========>....................] - ETA: 12s - loss: 1.3946
----- stdout -----
12288/32022 [==========>...................] - ETA: 12s - loss: 1.3924
----- stdout -----
13312/32022 [===========>..................] - ETA: 11s - loss: 1.3950
----- stdout -----
14336/32022 [============>.................] - ETA: 10s - loss: 1.3958
----- stdout -----
15360/32022 [=============>................] - ETA: 10s - loss: 1.3932
----- stdout -----
16384/32022 [==============>...............] - ETA: 9s - loss: 1.3924
----- stdout -----
17408/32022 [===============>..............] - ETA: 8s - loss: 1.3930
----- stdout -----
18432/32022 [================>.............] - ETA: 8s - loss: 1.3917
----- stdout -----
19456/32022 [=================>............] - ETA: 7s - loss: 1.3901
----- stdout -----
20480/32022 [==================>...........] - ETA: 6s - loss: 1.3896
----- stdout -----
21504/32022 [===================>..........] - ETA: 6s - loss: 1.3891
----- stdout -----
22528/32022 [====================>.........] - ETA: 5s - loss: 1.3889
----- stdout -----
23552/32022 [=====================>........] - ETA: 5s - loss: 1.3895
----- stdout -----
24576/32022 [======================>.......] - ETA: 4s - loss: 1.3880
----- stdout -----
25600/32022 [======================>.......] - ETA: 3s - loss: 1.3886
----- stdout -----
26624/32022 [=======================>......] - ETA: 3s - loss: 1.3880
----- stdout -----
27648/32022 [========================>.....] - ETA: 2s - loss: 1.3889
----- stdout -----
28672/32022 [=========================>....] - ETA: 1s - loss: 1.3887
----- stdout -----
29696/32022 [==========================>...] - ETA: 1s - loss: 1.3885
----- stdout -----
30720/32022 [===========================>..] - ETA: 0s - loss: 1.3887
----- stdout -----
31744/32022 [============================>.] - ETA: 0s - loss: 1.3900
----- stdout -----
32022/32022 [==============================] - ETA: 0s - loss: 1.3898
----- stdout -----

Epoch 44: val_loss did not improve from 1.33567
----- stdout -----
32022/32022 [==============================] - 20s 634us/sample - loss: 1.3898 - val_loss: 1.3379
----- stdout -----
Epoch 45/50
----- stdout -----
 1024/32022 [..............................] - ETA: 17s - loss: 1.3995
----- stdout -----
 2048/32022 [>.............................] - ETA: 17s - loss: 1.3818
----- stdout -----
 3072/32022 [=>............................] - ETA: 16s - loss: 1.3727
----- stdout -----
 4096/32022 [==>...........................] - ETA: 16s - loss: 1.3809
----- stdout -----
 5120/32022 [===>..........................] - ETA: 15s - loss: 1.3800
----- stdout -----
 6144/32022 [====>.........................] - ETA: 15s - loss: 1.3779
----- stdout -----
 7168/32022 [=====>........................] - ETA: 14s - loss: 1.3812
----- stdout -----
 8192/32022 [======>.......................] - ETA: 13s - loss: 1.3841
----- stdout -----
 9216/32022 [=======>......................] - ETA: 13s - loss: 1.3884
----- stdout -----
10240/32022 [========>.....................] - ETA: 12s - loss: 1.3922
----- stdout -----
11264/32022 [=========>....................] - ETA: 12s - loss: 1.3901
----- stdout -----
12288/32022 [==========>...................] - ETA: 11s - loss: 1.3893
----- stdout -----
13312/32022 [===========>..................] - ETA: 10s - loss: 1.3904
----- stdout -----
14336/32022 [============>.................] - ETA: 10s - loss: 1.3899
----- stdout -----
15360/32022 [=============>................] - ETA: 9s - loss: 1.3867
----- stdout -----
16384/32022 [==============>...............] - ETA: 9s - loss: 1.3866
----- stdout -----
17408/32022 [===============>..............] - ETA: 8s - loss: 1.3869
----- stdout -----
18432/32022 [================>.............] - ETA: 7s - loss: 1.3899
----- stdout -----
19456/32022 [=================>............] - ETA: 7s - loss: 1.3890
----- stdout -----
20480/32022 [==================>...........] - ETA: 6s - loss: 1.3901
----- stdout -----
21504/32022 [===================>..........] - ETA: 6s - loss: 1.3916
----- stdout -----
22528/32022 [====================>.........] - ETA: 5s - loss: 1.3934
----- stdout -----
23552/32022 [=====================>........] - ETA: 4s - loss: 1.3934
----- stdout -----
24576/32022 [======================>.......] - ETA: 4s - loss: 1.3927
----- stdout -----
25600/32022 [======================>.......] - ETA: 3s - loss: 1.3930
----- stdout -----
26624/32022 [=======================>......] - ETA: 3s - loss: 1.3940
----- stdout -----
27648/32022 [========================>.....] - ETA: 2s - loss: 1.3942
----- stdout -----
28672/32022 [=========================>....] - ETA: 1s - loss: 1.3939
----- stdout -----
29696/32022 [==========================>...] - ETA: 1s - loss: 1.3919
----- stdout -----
30720/32022 [===========================>..] - ETA: 0s - loss: 1.3909
----- stdout -----
31744/32022 [============================>.] - ETA: 0s - loss: 1.3914
----- stdout -----
32022/32022 [==============================] - ETA: 0s - loss: 1.3910
----- stdout -----

Epoch 45: val_loss improved from 1.33567 to 1.33118, saving model to ./checkpoints/unknown_person_few_shot_baseline_p1_30.h5
----- stdout -----
32022/32022 [==============================] - 21s 646us/sample - loss: 1.3910 - val_loss: 1.3312
----- stdout -----
Epoch 46/50
----- stdout -----
 1024/32022 [..............................] - ETA: 20s - loss: 1.3197
----- stdout -----
 2048/32022 [>.............................] - ETA: 19s - loss: 1.3609
----- stdout -----
 3072/32022 [=>............................] - ETA: 19s - loss: 1.3675
----- stdout -----
 4096/32022 [==>...........................] - ETA: 18s - loss: 1.3649
----- stdout -----
 5120/32022 [===>..........................] - ETA: 17s - loss: 1.3683
----- stdout -----
 6144/32022 [====>.........................] - ETA: 17s - loss: 1.3725
----- stdout -----
 7168/32022 [=====>........................] - ETA: 16s - loss: 1.3719
----- stdout -----
 8192/32022 [======>.......................] - ETA: 15s - loss: 1.3772
----- stdout -----
 9216/32022 [=======>......................] - ETA: 14s - loss: 1.3836
----- stdout -----
10240/32022 [========>.....................] - ETA: 13s - loss: 1.3840
----- stdout -----
11264/32022 [=========>....................] - ETA: 13s - loss: 1.3818
----- stdout -----
12288/32022 [==========>...................] - ETA: 12s - loss: 1.3830
----- stdout -----
13312/32022 [===========>..................] - ETA: 11s - loss: 1.3819
----- stdout -----
14336/32022 [============>.................] - ETA: 11s - loss: 1.3837
----- stdout -----
15360/32022 [=============>................] - ETA: 10s - loss: 1.3848
----- stdout -----
16384/32022 [==============>...............] - ETA: 9s - loss: 1.3863
----- stdout -----
17408/32022 [===============>..............] - ETA: 8s - loss: 1.3867
----- stdout -----
18432/32022 [================>.............] - ETA: 8s - loss: 1.3900
----- stdout -----
19456/32022 [=================>............] - ETA: 7s - loss: 1.3911
----- stdout -----
20480/32022 [==================>...........] - ETA: 7s - loss: 1.3898
----- stdout -----
21504/32022 [===================>..........] - ETA: 6s - loss: 1.3899
----- stdout -----
22528/32022 [====================>.........] - ETA: 5s - loss: 1.3879
----- stdout -----
23552/32022 [=====================>........] - ETA: 5s - loss: 1.3863
----- stdout -----
24576/32022 [======================>.......] - ETA: 4s - loss: 1.3852
----- stdout -----
25600/32022 [======================>.......] - ETA: 3s - loss: 1.3852
----- stdout -----
26624/32022 [=======================>......] - ETA: 3s - loss: 1.3872
----- stdout -----
27648/32022 [========================>.....] - ETA: 2s - loss: 1.3869
----- stdout -----
28672/32022 [=========================>....] - ETA: 2s - loss: 1.3866
----- stdout -----
29696/32022 [==========================>...] - ETA: 1s - loss: 1.3888
----- stdout -----
30720/32022 [===========================>..] - ETA: 0s - loss: 1.3886
----- stdout -----
31744/32022 [============================>.] - ETA: 0s - loss: 1.3893
----- stdout -----
32022/32022 [==============================] - ETA: 0s - loss: 1.3889
----- stdout -----

Epoch 46: val_loss improved from 1.33118 to 1.33098, saving model to ./checkpoints/unknown_person_few_shot_baseline_p1_30.h5
----- stdout -----
32022/32022 [==============================] - 21s 645us/sample - loss: 1.3889 - val_loss: 1.3310
----- stdout -----
Epoch 47/50
----- stdout -----
 1024/32022 [..............................] - ETA: 18s - loss: 1.3491
----- stdout -----
 2048/32022 [>.............................] - ETA: 18s - loss: 1.3630
----- stdout -----
 3072/32022 [=>............................] - ETA: 18s - loss: 1.3647
----- stdout -----
 4096/32022 [==>...........................] - ETA: 17s - loss: 1.3693
----- stdout -----
 5120/32022 [===>..........................] - ETA: 16s - loss: 1.3721
----- stdout -----
 6144/32022 [====>.........................] - ETA: 15s - loss: 1.3730
----- stdout -----
 7168/32022 [=====>........................] - ETA: 15s - loss: 1.3748
----- stdout -----
 8192/32022 [======>.......................] - ETA: 14s - loss: 1.3744
----- stdout -----
 9216/32022 [=======>......................] - ETA: 13s - loss: 1.3710
----- stdout -----
10240/32022 [========>.....................] - ETA: 13s - loss: 1.3708
----- stdout -----
11264/32022 [=========>....................] - ETA: 12s - loss: 1.3742
----- stdout -----
12288/32022 [==========>...................] - ETA: 11s - loss: 1.3767
----- stdout -----
13312/32022 [===========>..................] - ETA: 11s - loss: 1.3754
----- stdout -----
14336/32022 [============>.................] - ETA: 10s - loss: 1.3774
----- stdout -----
15360/32022 [=============>................] - ETA: 9s - loss: 1.3767
----- stdout -----
16384/32022 [==============>...............] - ETA: 9s - loss: 1.3775
----- stdout -----
17408/32022 [===============>..............] - ETA: 8s - loss: 1.3774
----- stdout -----
18432/32022 [================>.............] - ETA: 7s - loss: 1.3792
----- stdout -----
19456/32022 [=================>............] - ETA: 7s - loss: 1.3818
----- stdout -----
20480/32022 [==================>...........] - ETA: 6s - loss: 1.3818
----- stdout -----
21504/32022 [===================>..........] - ETA: 6s - loss: 1.3840
----- stdout -----
22528/32022 [====================>.........] - ETA: 5s - loss: 1.3847
----- stdout -----
23552/32022 [=====================>........] - ETA: 4s - loss: 1.3859
----- stdout -----
24576/32022 [======================>.......] - ETA: 4s - loss: 1.3857
----- stdout -----
25600/32022 [======================>.......] - ETA: 3s - loss: 1.3853
----- stdout -----
26624/32022 [=======================>......] - ETA: 3s - loss: 1.3847
----- stdout -----
27648/32022 [========================>.....] - ETA: 2s - loss: 1.3852
----- stdout -----
28672/32022 [=========================>....] - ETA: 1s - loss: 1.3842
----- stdout -----
29696/32022 [==========================>...] - ETA: 1s - loss: 1.3848
----- stdout -----
30720/32022 [===========================>..] - ETA: 0s - loss: 1.3858
----- stdout -----
31744/32022 [============================>.] - ETA: 0s - loss: 1.3865
----- stdout -----
32022/32022 [==============================] - ETA: 0s - loss: 1.3862
----- stdout -----

Epoch 47: val_loss did not improve from 1.33098
----- stdout -----
32022/32022 [==============================] - 20s 619us/sample - loss: 1.3862 - val_loss: 1.3326
----- stdout -----
Epoch 48/50
----- stdout -----
 1024/32022 [..............................] - ETA: 17s - loss: 1.4116
----- stdout -----
 2048/32022 [>.............................] - ETA: 17s - loss: 1.4014
----- stdout -----
 3072/32022 [=>............................] - ETA: 16s - loss: 1.3974
----- stdout -----
 4096/32022 [==>...........................] - ETA: 15s - loss: 1.3965
----- stdout -----
 5120/32022 [===>..........................] - ETA: 15s - loss: 1.3970
----- stdout -----
 6144/32022 [====>.........................] - ETA: 14s - loss: 1.4003
----- stdout -----
 7168/32022 [=====>........................] - ETA: 14s - loss: 1.3954
----- stdout -----
 8192/32022 [======>.......................] - ETA: 13s - loss: 1.3949
----- stdout -----
 9216/32022 [=======>......................] - ETA: 13s - loss: 1.3943
----- stdout -----
10240/32022 [========>.....................] - ETA: 12s - loss: 1.3900
----- stdout -----
11264/32022 [=========>....................] - ETA: 11s - loss: 1.3930
----- stdout -----
12288/32022 [==========>...................] - ETA: 11s - loss: 1.3901
----- stdout -----
13312/32022 [===========>..................] - ETA: 10s - loss: 1.3849
----- stdout -----
14336/32022 [============>.................] - ETA: 10s - loss: 1.3832
----- stdout -----
15360/32022 [=============>................] - ETA: 9s - loss: 1.3829
----- stdout -----
16384/32022 [==============>...............] - ETA: 8s - loss: 1.3818
----- stdout -----
17408/32022 [===============>..............] - ETA: 8s - loss: 1.3810
----- stdout -----
18432/32022 [================>.............] - ETA: 7s - loss: 1.3838
----- stdout -----
19456/32022 [=================>............] - ETA: 7s - loss: 1.3826
----- stdout -----
20480/32022 [==================>...........] - ETA: 6s - loss: 1.3815
----- stdout -----
21504/32022 [===================>..........] - ETA: 6s - loss: 1.3826
----- stdout -----
22528/32022 [====================>.........] - ETA: 5s - loss: 1.3807
----- stdout -----
23552/32022 [=====================>........] - ETA: 4s - loss: 1.3811
----- stdout -----
24576/32022 [======================>.......] - ETA: 4s - loss: 1.3804
----- stdout -----
25600/32022 [======================>.......] - ETA: 3s - loss: 1.3834
----- stdout -----
26624/32022 [=======================>......] - ETA: 3s - loss: 1.3822
----- stdout -----
27648/32022 [========================>.....] - ETA: 2s - loss: 1.3817
----- stdout -----
28672/32022 [=========================>....] - ETA: 1s - loss: 1.3824
----- stdout -----
29696/32022 [==========================>...] - ETA: 1s - loss: 1.3842
----- stdout -----
30720/32022 [===========================>..] - ETA: 0s - loss: 1.3838
----- stdout -----
31744/32022 [============================>.] - ETA: 0s - loss: 1.3844
----- stdout -----
32022/32022 [==============================] - ETA: 0s - loss: 1.3841
----- stdout -----

Epoch 48: val_loss did not improve from 1.33098
----- stdout -----
32022/32022 [==============================] - 20s 611us/sample - loss: 1.3841 - val_loss: 1.3339
----- stdout -----
Epoch 49/50
----- stdout -----
 1024/32022 [..............................] - ETA: 17s - loss: 1.3828
----- stdout -----
 2048/32022 [>.............................] - ETA: 17s - loss: 1.3846
----- stdout -----
 3072/32022 [=>............................] - ETA: 16s - loss: 1.3768
----- stdout -----
 4096/32022 [==>...........................] - ETA: 16s - loss: 1.3751
----- stdout -----
 5120/32022 [===>..........................] - ETA: 15s - loss: 1.3775
----- stdout -----
 6144/32022 [====>.........................] - ETA: 14s - loss: 1.3757
----- stdout -----
 7168/32022 [=====>........................] - ETA: 14s - loss: 1.3750
----- stdout -----
 8192/32022 [======>.......................] - ETA: 13s - loss: 1.3794
----- stdout -----
 9216/32022 [=======>......................] - ETA: 13s - loss: 1.3851
----- stdout -----
10240/32022 [========>.....................] - ETA: 12s - loss: 1.3849
----- stdout -----
11264/32022 [=========>....................] - ETA: 11s - loss: 1.3890
----- stdout -----
12288/32022 [==========>...................] - ETA: 11s - loss: 1.3898
----- stdout -----
13312/32022 [===========>..................] - ETA: 10s - loss: 1.3870
----- stdout -----
14336/32022 [============>.................] - ETA: 10s - loss: 1.3870
----- stdout -----
15360/32022 [=============>................] - ETA: 9s - loss: 1.3884
----- stdout -----
16384/32022 [==============>...............] - ETA: 9s - loss: 1.3872
----- stdout -----
17408/32022 [===============>..............] - ETA: 8s - loss: 1.3873
----- stdout -----
18432/32022 [================>.............] - ETA: 7s - loss: 1.3867
----- stdout -----
19456/32022 [=================>............] - ETA: 7s - loss: 1.3839
----- stdout -----
20480/32022 [==================>...........] - ETA: 6s - loss: 1.3870
----- stdout -----
21504/32022 [===================>..........] - ETA: 6s - loss: 1.3876
----- stdout -----
22528/32022 [====================>.........] - ETA: 5s - loss: 1.3859
----- stdout -----
23552/32022 [=====================>........] - ETA: 4s - loss: 1.3859
----- stdout -----
24576/32022 [======================>.......] - ETA: 4s - loss: 1.3882
----- stdout -----
25600/32022 [======================>.......] - ETA: 3s - loss: 1.3863
----- stdout -----
26624/32022 [=======================>......] - ETA: 3s - loss: 1.3866
----- stdout -----
27648/32022 [========================>.....] - ETA: 2s - loss: 1.3852
----- stdout -----
28672/32022 [=========================>....] - ETA: 1s - loss: 1.3866
----- stdout -----
29696/32022 [==========================>...] - ETA: 1s - loss: 1.3868
----- stdout -----
30720/32022 [===========================>..] - ETA: 0s - loss: 1.3860
----- stdout -----
31744/32022 [============================>.] - ETA: 0s - loss: 1.3857
----- stdout -----
32022/32022 [==============================] - ETA: 0s - loss: 1.3853
----- stdout -----

Epoch 49: val_loss did not improve from 1.33098
----- stdout -----
32022/32022 [==============================] - 20s 617us/sample - loss: 1.3853 - val_loss: 1.3361
----- stdout -----
Epoch 50/50
----- stdout -----
 1024/32022 [..............................] - ETA: 18s - loss: 1.3698
----- stdout -----
 2048/32022 [>.............................] - ETA: 17s - loss: 1.3723
----- stdout -----
 3072/32022 [=>............................] - ETA: 16s - loss: 1.3765
----- stdout -----
 4096/32022 [==>...........................] - ETA: 16s - loss: 1.3768
----- stdout -----
 5120/32022 [===>..........................] - ETA: 15s - loss: 1.3752
----- stdout -----
 6144/32022 [====>.........................] - ETA: 15s - loss: 1.3652
----- stdout -----
 7168/32022 [=====>........................] - ETA: 14s - loss: 1.3725
----- stdout -----
 8192/32022 [======>.......................] - ETA: 13s - loss: 1.3710
----- stdout -----
 9216/32022 [=======>......................] - ETA: 13s - loss: 1.3735
----- stdout -----
10240/32022 [========>.....................] - ETA: 12s - loss: 1.3788
----- stdout -----
11264/32022 [=========>....................] - ETA: 12s - loss: 1.3791
----- stdout -----
12288/32022 [==========>...................] - ETA: 11s - loss: 1.3827
----- stdout -----
13312/32022 [===========>..................] - ETA: 10s - loss: 1.3851
----- stdout -----
14336/32022 [============>.................] - ETA: 10s - loss: 1.3860
----- stdout -----
15360/32022 [=============>................] - ETA: 9s - loss: 1.3843
----- stdout -----
16384/32022 [==============>...............] - ETA: 9s - loss: 1.3839
----- stdout -----
17408/32022 [===============>..............] - ETA: 8s - loss: 1.3838
----- stdout -----
18432/32022 [================>.............] - ETA: 7s - loss: 1.3838
----- stdout -----
19456/32022 [=================>............] - ETA: 7s - loss: 1.3848
----- stdout -----
20480/32022 [==================>...........] - ETA: 6s - loss: 1.3841
----- stdout -----
21504/32022 [===================>..........] - ETA: 6s - loss: 1.3856
----- stdout -----
22528/32022 [====================>.........] - ETA: 5s - loss: 1.3863
----- stdout -----
23552/32022 [=====================>........] - ETA: 4s - loss: 1.3863
----- stdout -----
24576/32022 [======================>.......] - ETA: 4s - loss: 1.3871
----- stdout -----
25600/32022 [======================>.......] - ETA: 3s - loss: 1.3865
----- stdout -----
26624/32022 [=======================>......] - ETA: 3s - loss: 1.3845
----- stdout -----
27648/32022 [========================>.....] - ETA: 2s - loss: 1.3831
----- stdout -----
28672/32022 [=========================>....] - ETA: 1s - loss: 1.3841
----- stdout -----
29696/32022 [==========================>...] - ETA: 1s - loss: 1.3850
----- stdout -----
30720/32022 [===========================>..] - ETA: 0s - loss: 1.3848
----- stdout -----
31744/32022 [============================>.] - ETA: 0s - loss: 1.3852
----- stdout -----
32022/32022 [==============================] - ETA: 0s - loss: 1.3854
----- stdout -----

Epoch 50: val_loss improved from 1.33098 to 1.33018, saving model to ./checkpoints/unknown_person_few_shot_baseline_p1_30.h5
----- stdout -----
32022/32022 [==============================] - 20s 623us/sample - loss: 1.3854 - val_loss: 1.3302
----- stdout -----
WARNING:tensorflow:From /usr/lib/python3.11/site-packages/tensorflow/python/ops/init_ops.py:94: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
----- stdout -----
WARNING:tensorflow:From /usr/lib/python3.11/site-packages/tensorflow/python/ops/init_ops.py:94: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
----- stdout -----
WARNING:tensorflow:From /usr/lib/python3.11/site-packages/tensorflow/python/ops/init_ops.py:94: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
----- stdout -----
WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_16 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_17 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_18 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_19 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_20 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_22 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_23 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_24 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_25 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_26 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_27 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_28 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_29 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_30 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_31 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_32 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_33 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_34 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_35 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stdout -----
WARNING:tensorflow:Layer lstm_36 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
----- stderr -----
2023-11-15 17:21:57.100633: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_27_1/lstm_cell_64/recurrent_kernel/Assign' id:22982 op device:{requested: '', assigned: ''} def:{{{node lstm_27_1/lstm_cell_64/recurrent_kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_27_1/lstm_cell_64/recurrent_kernel, lstm_27_1/lstm_cell_64/recurrent_kernel/Initializer/mul_1)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
----- stderr -----
2023-11-15 17:21:58.824861: W tensorflow/c/c_api.cc:304] Operation '{name:'lstm_30_1/lstm_cell_67/recurrent_kernel/v/Assign' id:26350 op device:{requested: '', assigned: ''} def:{{{node lstm_30_1/lstm_cell_67/recurrent_kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](lstm_30_1/lstm_cell_67/recurrent_kernel/v, lstm_30_1/lstm_cell_67/recurrent_kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
----- stderr -----
/usr/lib/python3.11/site-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-11-15 17:22:00.206675: W tensorflow/c/c_api.cc:304] Operation '{name:'dropout_4_1/cond/Merge' id:24591 op device:{requested: '', assigned: ''} def:{{{node dropout_4_1/cond/Merge}} = Merge[N=2, T=DT_FLOAT, _has_manual_control_dependencies=true](dropout_4_1/cond/Identity, dropout_4_1/cond/dropout/SelectV2)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
----- stdout -----
(1571, 1437)
----- stdout -----
(1632, 1437)
----- stdout -----
(1728, 1437)
----- stdout -----
(1872, 1437)
----- stdout -----
(1713, 1437)
(1345, 1437)
----- stdout -----
(1823, 1437)
----- stdout -----
(1606, 1437)
----- stdout -----
(1752, 1437)
----- stdout -----
(1502, 1437)
----- stdout -----
(1932, 1437)
----- stdout -----
(1728, 1437)
----- stdout -----
(1776, 1437)
----- stdout -----
(1848, 1437)
----- stdout -----
(1740, 1437)
----- stdout -----
(1812, 1437)
(946, 1437)
----- stdout -----
(1680, 1437)
----- stdout -----
(1872, 1437)
{2: 5.850236522216273, 4: 6.660091297861812, 5: 6.075589236150247, 6: 3.4328808179480443, 8: 8.652678454990497, 9: 4.747996400562146, 10: 6.9415721276275075, 11: 4.931413112542822, 12: 8.245398990189528, 13: 7.335455998642272, 17: 7.595654123904687, 19: 6.125015650578609, 21: 10.0, 22: 1.777977573013691, 25: 6.361430622969817, 26: 6.815937482022984, 27: 3.434353599216599, 28: 5.717145823826935, 29: 1.0}
----- stderr -----
/tmp/ipykernel_2886483/1329324107.py:304: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)
  w_train[i] = weights_dict[int(p_train[i])]
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mKeyError[0m                                  Traceback (most recent call last)
Cell [0;32mIn[2], line 304[0m
[1;32m    302[0m w_train [38;5;241m=[39m np[38;5;241m.[39mzeros_like(p_train)
[1;32m    303[0m [38;5;28;01mfor[39;00m i [38;5;129;01min[39;00m [38;5;28mrange[39m([38;5;28mlen[39m(w_train)):
[0;32m--> 304[0m     w_train[i] [38;5;241m=[39m [43mweights_dict[49m[43m[[49m[38;5;28;43mint[39;49m[43m([49m[43mp_train[49m[43m[[49m[43mi[49m[43m][49m[43m)[49m[43m][49m
[1;32m    306[0m w_train [38;5;241m=[39m np[38;5;241m.[39msqueeze(w_train)
[1;32m    308[0m [38;5;28;01mimport[39;00m [38;5;21;01msys[39;00m

[0;31mKeyError[0m: 1

Error converting unknown_baseline_p1_few_shot.ipynb. Exiting.
